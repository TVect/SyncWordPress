---
ID: 115
post_title: rnn + crf layer 理解
author: Chin
post_excerpt: ""
layout: post
permalink: http://www.tvect.cc/2018/01/29/115/
published: true
post_date: 2018-01-29 22:28:29
---
<h1>目的</h1>

一般的在rnn层之后可以直接加上softmax做分类,处理序列标注问题。但是直接这么做的话，并没有很好的考虑一些全局信息，比如label之间的转移概率约束。

通过在rnn层之上使用crf layer做序列标注，可以更好的学到一些全局性的约束。

<h1>loss function 及其计算</h1>

crf 层的损失函数包括两部分, Emission score和Transition score，或者叫unary_score和binary_score.

emission score可以直接由rnn层的输出计算得到，transition_score需要涉及到crf 层的参数transition_params.

称每一个可能的标注序列为一个Path, 每一个Path都对应着一个概率 $$ P_{i} =  e^{S_i}$$, 假定现在总共有 $$N$$ 种可能的Path, 真实的路径, 即训练数据中真实的标注序列为RealPath,  那么该训练样本对应的损失为:

$$ Loss Function = frac{P_{RealPath}}{P_1 + P_2 + … + P_N} $$

对上面的损失函数取-log之后有:

$$logLosFucntion = -logfrac{P_{RealPath}}{P_1 + P_2 + … + P_N}$$

$$ = -logP_{RealPath} + log(P_{1}+...+P_{N})$$

$$ = -log(exp(S_{RealPath})) + log(P_{1}+...+P_{N})$$

$$ = -S_{RealPath} + log(P_{1}+...+P_{N})$$

<h3>Example</h3>

$$ tags: tag_{1}, ..., tag_{m} $$

$$ TransParams: T $$

$$ WordSeq: w_{1}, w_{2}, w_{3}, ..., w_{n} $$

$$ RnnOutput: x_{w_{1}}, x_{w_{2}}, x_{w_{3}}, ..., x_{w_{n}} $$

$$ TagList: label_{w_{1}}, label_{w_{2}}, label_{w_{3}}, ..., label_{w_{n}} $$

<ol>
<li><strong>RealPath score 计算</strong></li>
</ol>

$$ P_{RealPath} = UnaryScore + BinaryScore $$

$$ UnaryScore = exp(x_{w_{1}}(label_{w_{1}}) + x_{w_{2}}(label_{w_{1}}) + ... + x_{w_{n}}(label_{w_{n}})) $$

$$ BinaryScore = exp(T(label_{w_{1}}, label_{w_{2}}) + T(label_{w_{2}}, label_{w_{3}}) + ... + T(label_{w_{n-1}}, label_{w_{n}})) $$

$$ logP_{RealPath} = x_{w_{1}}(label_{w_{1}}) + x_{w_{2}}(label_{w_{1}}) + ... + x_{w_{n}}(label_{w_{n}}) + T(label_{w_{1}}, label_{w_{2}}) + T(label_{w_{2}}, label_{w_{3}}) + ... + T(label_{w_{n-1}}, label_{w_{n}}) $$

<ol>
<li><strong>TotalPath score 计算</strong></li>
</ol>

- 计算 $$ TotalScore(label_{1}) $$

$$ log TotalScore(label_{1}=i) = log(exp(x_{w_{1}}(i))) $$

$$ TotalScore(label_{1}) $$

$$ = \\sum_{i \\in tags} TotalScore(label_{1}=i) $$

$$ = \\sum_{i \\in tags} {exp(logTotalScore(label_{1}=i))} $$

<ul>
<li>计算 $$ TotalScore(label_{1} \\rightarrow label_{2}) $$</li>
</ul>

$$ logTotalScore(*, label_{2} = i) $$

$$ = log(\\sum_{j \\in tags} TotalScore(label_{1}=j) * expT(j,i) * exp(x_{w_{2}(i)}))$$

$$ = x_{w_{2}}(i) + log (\\sum_{j \\in tags} TotalScore(label_{1}=j) * expT(j,i))$$

$$ = x_{w_{2}}(i) + log (\\sum_{j \\in tags} exp(logTotalScore(label_{1}=j)) * expT(j,i)) $$

$$ TotalScore(w_{1} \\rightarrow w_{2}) $$

$$ = \\sum_{i \\in tags} exp (logTotalScore(*, label_{2}=i))$$

根据以上推导, 既可以看到 $$ logTotalScore $$ 和 $$ TotalScore $$ 的递推表达式.

<strong>可以参考tensorflow中crf.py的实现, 其中使用了RNN来实现了上述递推公式.</strong>

<h1>解码过程</h1>

使用通常的viterbi解码算法即可.

具体可以参考tensorflow中crf.py的实现.

<h1>缺陷与改进</h1>

<h2>缺陷</h2>

上面提到的方法中, 使用RNN的输出表示了unary score, 使用transition_params表示binary_score.

某种程度上<strong>强加了在每个step的transition_params都相同的限制</strong>, 参考李航的书上,其实应该没有这种限制的.

更精确的来说，实际上我们希望建模的是：

$$ p(y|x) = \\frac{1}{Z}exp(\\sum f_{i}(x, y_{i-1}, y_{i}) + \\sum g_{i}(x, y_{i})) $$

其中对应的 binary score 为 $$ f_{i}(x, y_{i-1}, y_{i}) $$, 与整个句子有关，也与位置有关。

<h2>改进</h2>

以下改进来源于《Structured prediction models for RNN based sequence labeling in clinical text》

<h3>Bi-LSTM + CRF-pair</h3>

在传统的BiLSTM+CRF中, 计算binary_score时, 使用统一的转移矩阵, 而没有考虑当前的内容.

这里将binary_score通过一个独立于文本和单词的非线性网络建模, 通过1-D fileter size 2的一维CNN和非线性层tanh, 在每一个标签位置，$$ [\\Omega (x) _{t}, \\Omega (x) _{t+1}] $$作为输入, 得到一个与当前上下文相关的转移矩阵.

<h3>Skip-Chain CRF</h3>

显式的建模了长时依赖的关系

<h1>参考文献</h1>

<a href="https://github.com/createmomo/CRF-Layer-on-the-Top-of-BiLSTM">crf layer流程案例</a>

<a href="https://zhuanlan.zhihu.com/p/27662562">Structured prediction models for RNN based sequence labeling in clinical text阅读笔记</a>

论文: Structured prediction models for RNN based sequence labeling in clinical text