---
ID: 196
post_title: CopyNet
author: chin340823
post_excerpt: ""
layout: post
permalink: http://www.tvect.cc/2018/05/04/copynet/
published: true
post_date: 2018-05-04 15:35:51
---
本文的模型通过借鉴人类在处理难理解的文字时采用的死记硬背的方法，提出了COPYNET。将拷贝模式融入到了Seq2Seq模型中，将传统的生成模式和拷贝模式混合起来构建了新的模型，非常好地解决了OOV问题。

解决问题的思路与 Pointer Net 很类似。

<h1>概述</h1>

本文探索了一种人类语言交流中的"拷贝机制"，它定位了输入序列中的特定片段，并把这一片段插入到输出序列中。

通常的 encoder-decoder 模型及其 attention 变种，都非常依赖于语义表示，这可能会非常的不准确，如果我们的系统涉及到输入中的日期，实体名等子序列。对比之下，拷贝机制更接近于人类在语言处理中的死记硬背（rote memorization）。在 seq2seq 的任务中，模型如果可以同时有 understanding 和 rote memorization 的能力，可能会更有优势。

本文提出的 CopyNet，不仅可以像普通的seq2seq模型一样生成词，也可以从输入序列中拷贝适当的片段。CopyNet 可以端到端的训练。本文的试验结果也证明了 CopyNet 的有效性。

<h1>模型</h1>

<h2>Encoder 部分</h2>

和普通的 seq2seq 的 encoder 部分一样，采用了 bi-directional RNN 对输入序列编码，得到了一个隐藏层表示的矩阵 M，后续会作为 decoder 部分的输入。

<h2>Decoder 部分</h2>

decoder 部分同样是一个 RNN，读取 encoder 部分产生的隐藏层表示的矩阵 M，用以产生目标序列。
与普通的 RNN-decoder 类型，但是有一下几点重要的差别：

<ul>
<li>Prediction
预测一个词的概率会基于 generate-mode 和 copy-mode，后者表示的直接从输入序列中选择 word。</p></li>
<li><p>State Update
CopyNet 更新 state 的时候，不仅会依赖于 last predicted word 的 word embedding, 也会依赖 last predicted word 的 corresponding location-specfic hidden state in M (if any)</p></li>
<li><p>Reading M
读取 M 的时候，会有 attentive read 和 selective read，从而有力的混合了 content-based addressing 和 location-based addressing</p></li>
</ul>

<h3>Prediction with Copying and Generation</h3>

<p>构造一个词汇表（V）,用在 Generate mode 中产生一个word。
构造一个在输入序列中出现过一次的词表, 这部分的词用来支持 CopyNet。
另外还有一个 UNK, 表示集外词（OOV），最终输入序列的词汇表是三者的并集。

这些词的解码概率可以用如下图表示:

<img src="http://www.tvect.cc/wp-content/uploads/2018/05/CopyNet-Prob.png" alt="" />

具体的解码概率计算公式如下:

<img src="http://www.tvect.cc/wp-content/uploads/2018/05/CopyNet-Prob2.png" alt="" />

<h3>State Update</h3>

基本更新公式如下：

<img src="http://www.tvect.cc/wp-content/uploads/2018/05/CopyNet-stateupdate.png" alt="" />

<strong>上面公式中的 p 是怎么计算的？？？如果和前一节的p(y, c|)一样，那p在不同的位置不是一样的吗？？</strong>

上面公式中的 zeta 代表了 <strong>Selective Read</strong>, 为 CopyNet 专门设计的 Selective Read，自然的考虑了 y 在输入序列中的位置信息，如果 y 在输入序列中没有出现，那么 zeta=0 。
C 代表了 <strong>Attentive Read</strong>。
归一化因子 K 考虑了在输入序列中可能会有多个位置上出现特定的 y。

这种设计能够让 copy mode 覆盖输入序列中一个连续的序列。

<h3>Hybrid Addressing of M</h3>

CopyNet 既有 Content-based Addressing, 又有 Location-based Addressing.

后续试验说明了 CopyNet 的 attentive read 由语义和语言模型驱动，因此可以在 M 中自由的跨越很长的一段距离。另一方面，当 CopyNet 进入了 copy-mode 之后，selective read 由位置信息来控制，导致 selective read 会倾向于覆盖一段输入序列中连续的words。

<h1>实验</h1>

<h1>参考资料</h1>

<a href="https://arxiv.org/abs/1603.06393" title="Incorporating Copying Mechanism in Sequence-to-Sequence Learning">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a>

<a href="https://zhuanlan.zhihu.com/p/30810609" title="论文笔记 - Copy or Generate">论文笔记 - Copy or Generate</a>

<a href="https://zhuanlan.zhihu.com/p/21421396" title="知乎CopyNet笔记">知乎CopyNet笔记</a>