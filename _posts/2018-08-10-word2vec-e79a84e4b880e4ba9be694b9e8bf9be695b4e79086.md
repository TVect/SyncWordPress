---
ID: 59
post_title: Word2Vec 的一些改进整理
author: chin340823
post_excerpt: ""
layout: post
permalink: https://blog.tvect.cn/?p=59
published: true
post_date: 2018-08-10 19:50:57
---
[toc]

下面是关于文献中对 Word2Vec 改进点的一些简单总结。

这些方法中，很多细节都没有再做仔细的整理。之后列出的一些文献可以作为参考。

<!--more-->

<h1>Word2Vec 概述</h1>

略...

<hr />

<h1>改进点</h1>

<h2>从优化的目标函数角度</h2>

$$
&#92;begin{aligned}
J &amp;= -&#92;sum &#92;limits_{i=1}^{V} &#92;sum &#92;limits_{j=1}^{V}X_{i,j} logQ_{i,j} &#92;&#92;
&amp;= -&#92;sum &#92;limits_{i=1}^{V} &#92;sum &#92;limits_{j=1}^{V} P_{i,j}X_{i}logQ_{i,j} &#92;&#92;
&amp;= -&#92;sum &#92;limits_{i=1}^{V} X_{i} &#92;sum &#92;limits_{j=1}^{V} P_{i,j} log Q_{i,j} &#92;&#92;
&amp;= -&#92;sum &#92;limits_{i=1}^{V} X_{i} H(P_{i}, Q_{i}) &#92;&#92;
where &#92;quad &amp; Q_{i,j} = &#92;frac{exp(w_{i}&#92;tilde{w_j})}{&#92;sum exp(w_{i}&#92;tilde {w_k})} , &#92;quad P_{i,j} = &#92;frac{X_{i,j}}{X_{i}} &#92;&#92;
&#92;end{aligned}
$$

其中的 $$ H(P_{i}, Q_{i}) $$ 是两个分布的交叉熵, 可以将 $$ X_{i} $$ 视为权重, 那么优化的目标函数即是一个加权的交叉熵.

<h3>Glove</h3>

对 Skip-gram 的损失函数做进一步的改变.

<ul>
<li>替换 error metric 为 $$ (w_i &#92;tilde{w_j} + b_i + &#92;tilde{b_k} - log X_{i,j})^{2} $$</p></li>
<li><p>替换权重因子为 更广义的 $$f(X_{i,j})$$, 其满足一下约束:</p></li>
</ul>

<p>最后即得到 GloVe 的优化目标为:

$$
J = &#92;sum &#92;limits_{i,j=1}^{V} f(X_{i,j})(w_{i}^{T} &#92;tilde{w_j} + b_{i} + &#92;tilde{b_j} - log X_{i,j})^{2}
$$

注意, 文章中提到, 引入偏置项 $$b_{i}, &#92;tilde{b_k}$$, 是为了保证一定的对称性. 而且从公式中可以看到, 如果不加偏置项, 最后得到的词向量 $$w_{i}, &#92;tilde{w_j}$$ 中的值会偏向于很大???

具体的，关于为什么会使用这样子的优化目标函数，可以参看原始论文 <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a>

<h3>WordRank</h3>

<h4>概述</h4>

给定一个 word w， 期望输出一个上下文词的有序列表 (c1, c2, ...)，使得和 w 频繁共现的词出现在列表的前面.

记所有 word 的集合为 $$W$$, 所有上下文 word 的集合为 $$C$$，相应的 embedding 参数分别为: $$ U := {u_w}<em>{w &#92;in W}, V := {v_c}</em>{c &#92;in C} $$.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/08/word-rank.png" alt="" />

其中的，$$ &#92;rho(.) $$ 是一个 monotonically increasing and concave functions.
之所以需要是 concave 的，是因为 concave 可以保证 $$ &#92;rho(.) $$ 的导数是非增的，从而 ranking loss 对有序列表的 top 更为敏感。
具体的，$$ &#92;rho(.) $$ 可以有如下的选择：

<img src="http://blog.tvect.cc/wp-content/uploads/2018/08/word-rank-rho.png" alt="" />

<h4>优点</h4>

相比之下，WordRank 的方法有以下优点：

<ol>
<li>是判别式的而不是生成式的，因为它直接建模了词的相对顺序</p></li>
<li><p>将 embedding 的学习当做一个 ranking 的问题，可以使我们能设计一些对噪声 Robust 的模型. 例如，上面的 $$&#92;rho(.)$$ 的选择.</p></li>
</ol>

<p>模型的 Stochastic Optimization 细节，以及其他的一些详情可以参数<a href="https://arxiv.org/abs/1506.02761">原始论文</a>

<hr />

<h2>矩阵分解角度</h2>

可以从矩阵分解的角度去看 Skip-gram with Negative Sampling.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/08/sgns.png" alt="" />

所以, 可以将Skip-gram with Negative Sampling 视为是对 Shifted PMI Matrix 做矩阵分解.

但是, 考虑到 PMI 矩阵有一些弱点, 比如:

<ul>
<li>对于在语料中未出现的 (w, c) pair, PMI(w, c) = log 0, 这导致矩阵是 ill-defined and dense.</p></li>
<li><p>简单的让未出现的 (w, c) pair 的 PMI 为0, 会导致一些不一致性.(考虑到有些很少共同出现的pair的PMI是负的)</p></li>
</ul>

<p>可以使用一些对上述弱点进行改进了的矩阵，比如 Shifted Positive PMI:

$$ SPPMI_{k}(w, c) = max (PMI(w, c) - log k, 0)$$

具体的细节，可以参考原始论文 <a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Neural Word Embedding as Implicit Matrix Factorization</a>

<hr />

<h2>其他改进</h2>

<h3>FastText</h3>

在原始的 skipgram model with negative sampling 方法中忽略了 word 的内部结构，这里的 fastText 方法考虑了 subword units, 用 sum of character n-grams 来表示一个 word.

具体的，在这个方法中，先为要考虑的 word 加上前缀 &lt; 和后缀 > , 接下来抽取 character n-grams 和 整个word序列 <word>, 将这些 n-grams 的向量表示求和，作为 word 的向量表示. 原始文章中的 n 取得是 $$ 3 &#92;leq n &#92;leq 6$$.

fastText 词向量表示的代码实现见 <a href="https://github.com/facebookresearch/fastText">facebookresearch/fastText</a>

<hr />

<h1>参考资料</h1>

<ul>
<li><p><a href="https://blog.csdn.net/qjf42/article/details/79671230">关于词向量的一些理解</a></p></li>
<li><p><a href="https://blog.csdn.net/daiyongya/article/details/81018307">Embedding算法之矩阵分解</a></p></li>
<li><p><a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Neural Word Embedding as Implicit Matrix Factorization</a></p></li>
<li><p><a href="https://arxiv.org/abs/1506.02761">WordRank: Learning Word Embeddings via Robust Ranking</a></p></li>
<li><p><a href="https://arxiv.org/abs/1607.04606">Enriching Word Vectors with Subword Information</a></p></li>
<li><p><a href="https://arxiv.org/abs/1607.01759">Bag of Tricks for Efficient Text Classification</a></p></li>
<li><p><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a></p></li>
</ul>