---
ID: 631
post_title: Word2Vec 的一些改进整理
author: Chin
post_excerpt: ""
layout: post
permalink: 'http://blog.tvect.cc/2018/08/10/word2vec-%e7%9a%84%e4%b8%80%e4%ba%9b%e6%94%b9%e8%bf%9b%e6%95%b4%e7%90%86/'
published: true
post_date: 2018-08-10 19:50:57
---
[toc]

下面是关于文献中对 Word2Vec 改进点的一些总结

<h1>Word2Vec 概述</h1>

<h1>改进点</h1>

<h2>从优化的目标函数角度</h2>

$$
\\begin{aligned}
J &= -\\sum \\limits_{i=1}^{V} \\sum \\limits_{j=1}^{V}X_{i,j} logQ_{i,j} \\\\
\\\\
&= -\\sum \\limits_{i=1}^{V} \\sum \\limits_{j=1}^{V} P_{i,j}X_{i}logQ_{i,j} \\\\
\\\\
&= -\\sum \\limits_{i=1}^{V} X_{i} \\sum \\limits_{j=1}^{V} P_{i,j} log Q_{i,j} \\\\
\\\\
&= -\\sum \\limits_{i=1}^{V} X_{i} H(P_{i}, Q_{i}) \\\\
\\\\
where \\quad & Q_{i,j} = \\frac{exp(w_{i}\\tilde{w}_{j})}{\\sum exp(w_{i}\\tilde {w}_{k})} , \\quad P_{i,j} = \\frac{X_{i,j}}{X_{i}} \\\\
\\end{aligned}
$$

其中的 $$ H(P_{i}, Q_{i}) $$ 是两个分布的交叉熵, 可以将 $$ X_{i} $$ 视为权重, 那么优化的目标函数即是一个加权的交叉熵.

<h3>Glove</h3>

对 Skip-gram 的损失函数做进一步的改变.

<ul>
<li>替换 error metric 为 $$ (w_{i}\\tilde{w}_{j} + b_{i} + \\tilde{b}_{k} - log X_{i,j})^{2} $$</p></li>
<li><p>替换权重因子为 更广义的 $$f(X_{i,j})$$, 其满足一下约束:</p></li>
</ul>

<p>最后即得到 GloVe 的优化目标为:

$$
J = \\sum \\limits_{i,j=1}^{V} f(X_{i,j})(w_{i}^{T} \\tilde{w}_{j} + b_{i} + \\tilde{b}_{j} - log X_{i,j})^{2}
$$

注意, 文章中提到, 引入偏置项 $$b_{i}, \\tilde{b}_{k}$$, 是为了保证一定的对称性. 而且从公式中可以看到, 如果不加偏置项, 最后得到的词向量 $$w_{i}, \\tilde{w}_{j}$$ 中的值会偏向于很大???

具体的，关于为什么会使用这样子的优化目标函数，可以参看原始论文 <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a>

<h3>WordRank (待补充...)</h3>

<h2>矩阵分解角度</h2>

可以从矩阵分解的角度去看 Skip-gram with Negative Sampling.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/08/sgns.png" alt="" />

所以, 可以将Skip-gram with Negative Sampling 视为是对 Shifted PMI Matrix 做矩阵分解.

但是, 考虑到 PMI 矩阵有一些弱点, 比如:

<ul>
<li>对于在语料中未出现的 (w, c) pair, PMI(w, c) = log 0, 这导致矩阵是 ill-defined and dense.</p></li>
<li><p>简单的让未出现的 (w, c) pair 的 PMI 为0, 会导致一些不一致性.(考虑到有些很少共同出现的pair的PMI是负的)</p></li>
</ul>

<p>可以使用一些对上述弱点进行改进了的矩阵，比如 Shifted Positive PMI:

$$ SPPMI_{k}(w, c) = max (PMI(w, c) - log k, 0)$$

具体的细节，可以参考原始论文 <a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Neural Word Embedding as Implicit Matrix Factorization</a>

<h2>其他改进</h2>

<h3>FastText (待补充...)</h3>

<h1>参考资料</h1>

<ul>
<li><p><a href="https://blog.csdn.net/qjf42/article/details/79671230">关于词向量的一些理解</a></p></li>
<li><p><a href="https://blog.csdn.net/daiyongya/article/details/81018307">Embedding算法之矩阵分解</a></p></li>
<li><p><a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Neural Word Embedding as Implicit Matrix Factorization</a></p></li>
<li><p><a href="https://arxiv.org/abs/1506.02761">WordRank: Learning Word Embeddings via Robust Ranking</a></p></li>
<li><p><a href="https://arxiv.org/abs/1607.04606">Enriching Word Vectors with Subword Information</a></p></li>
<li><p><a href="https://arxiv.org/abs/1607.01759">Bag of Tricks for Efficient Text Classification</a></p></li>
<li><p><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a></p></li>
</ul>