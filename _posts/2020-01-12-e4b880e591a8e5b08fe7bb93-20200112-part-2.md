---
ID: 693
post_title: 一周小结-20200112-Part 2
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/693
published: true
post_date: 2020-01-12 21:12:58
---
本周的阅读笔记的第二部分，来源于 Jordan Book：An Introduction to PGMs，Chapter06. Linear Regression and the LMS algorithm。

这一章主要介绍了从 constraint satisfation problem 的角度来看待 linear regression 问题，同时将这种观点和概率模型的观点做了关联。

<!--more-->

<h1>The LMS algorithm</h1>

Suppose that we have a pair of observed variables $x_n$ and $y_n$ that we assume are related linearly, LMS learning algorithm has the following form:

$$ \theta^{t+1} = \theta^{t} + \rho (y_n - (\theta^{t})^{T}x_n) x_n $$

where $\rho$ is a free parameter known as the "step size".

<h2>Multiple data points</h2>

寻找参数 $\theta$, 使得 error 最小（这里的 error 定义为平方误差的求和）.

问题等价于：

choosing a vetor $\hat{y}$ in a vector subspace (column space of the matrix X, whose n-th row is the row vector $(x^n)^T$) that best represents a vector $y$ outside of the subspace.

<strong>normal equations</strong>: $ X^{T} X \theta^{\star} = X^T y$

<strong>Steepest descent</strong>: $ \theta^{t+1} = \theta^{t} + \rho \sum_{n=1}^N (y_n - (\theta^{t})^{T}x_n) x_n$

<hr />

<h3>求解方法：</h3>

<strong>batch method</strong>:

<ol>
<li>directed method: 直接求解 normal equations（涉及到矩阵求逆）</p></li>
<li><p>iterative method: steepest descent</p></li>
</ol>

<p><strong>on-line method</strong>: LMS or sgd

<h1>Weighted least squares</h1>

给每个数据点 $(x_n, y_n)$, 关联一个权重 $w_n$, 此时的 error 为：

$$ J(\theta) = \frac{1}{2} \sum_{n=1}^N w_n(y_n - \theta^T x_n)^2 = \frac{1}{2} (y-X\theta)^T W (y-X\theta)$$

其中, $W$ 是一个对角矩阵, $ W = diag(w_1, w_2, ..., w_N) $.

<strong>normal equations for weighted least squares</strong>:  $X^T W X \theta^{\star} = X^T W y$

<h1>Probabilistic interpretations</h1>

a Gaussian distribution and IID sampling imply - within a maximum likelihood framework - the minimization of the least-squares cost function.

<strong>概率解释中的高斯假设和IID假设, 与 least square cost function 中的 error function 定义 和 误差项组合方式 是相互对应的</strong>

<blockquote>
  in the constraint satisfaction formulation, we need to decide how to measure the magnitudes of the errors and how to combine these magnitudes. These decisions have correspondences in the probabilistic formulation, in particular the Gaussian assumption effectively determines the metric by which we measure the errors, and the IID assumption determines the way in which the errors are combined.
</blockquote>

<h1>参考资料</h1>

<ul>
<li><a href="">Jordan: An Introduction to PGMs Chapter 6</a></li>
</ul>