---
ID: 728
post_title: 小结-20200120
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/728
published: true
post_date: 2020-01-20 20:11:46
---
[toc]

这部分内容整理来源于 Jordan Book：An Introduction to PGMs，Chapter09. Completely Observed Graphical Models

前面几章 Linear Regression，Linear Classification，包括 exponential family，都主要是在关注 single-node (with its parents) graphical model. 这一章讨论了在一般的 Completely Observed Graphical Models 中的 MLE 参数估计问题.

<!--more-->

<h1>The basic idea</h1>

<h1>Directed models</h1>

对于完全可观测的有向图模型参数估计问题来说，可以把问题解耦为单独地在每一个节点上做 MLE. 所以相对于之前章节中介绍的 single-node (with its parents) graphical model，这里并不会有太多额外的工作.

<blockquote>
  In general, in the case of completely observed data, any solution to the problem of estimating parameters at a single node (conditional on its parents) applies immediately to general directed graphs.
</blockquote>

<h1>Undirected models</h1>

与有向图不同，对于一般的无向图模型来说，参数估计问题无法简单的解耦合。原因是因为其概率表示中会出现 normalization factor $Z$.

<h2>Discrete models</h2>

$$
\begin{aligned}
l(\theta; D) &amp;= \log p(D | \theta) &#92;
&amp;= \sum_n \sum_{x_\nu} \delta(x_\nu, x_{\nu, n}) \log p(x_\nu | \theta) &#92;
&amp;= \sum_\nu m(x_\nu) \log p(x_\nu | \theta) &#92;
&amp;= \sum_\nu m(x_\nu) \log (\frac{1}{Z} \prod_C \psi_C(x_C)) &#92;
&amp;= \sum_\nu m(x_\nu) \sum_C \log \psi_C(x_C) - \sum_\nu m(x_\nu) \log Z &#92;
&amp;= \sum_\nu \sum_C m_\nu \log \psi_C(x_C) - N \log Z
\end{aligned}
$$

<blockquote>
  We see that the marginal counts $m(x_C)$, for $C \in C$, are the sufficient statistics for our model. This is reminiscent of the directed case, where the cliques $C$ were the families $\phi_v$.
</blockquote>

<h2>Maximum likelihood estimation</h2>

$$ \frac{\partial l}{\partial \psi_C(x_C)} = \frac{m(x_C)}{\psi_C(x_C)} - N \frac{p(x_C)}{\psi_C(x_C)}$$

从而有: $ \hat{p}_{ML}(x_C) = \frac{m(x_C)}{N} = \tilde{p}(x)$. 其中 $\tilde{p}(x)$ 为经验分布.

<blockquote>
  Thus we have the following important characterization of maximum likelihood estimates -- for each clique $C \in C$, the model marginals must be equal to the empirical marginals.
</blockquote>

<h2>decomposable models</h2>

A graph is said to be <strong>decomposable</strong> if it can be recursively subdivided into disjoint sets A, B and S, where S separates A and B, and where S is complete.

We can find maximum likelihood estimates for decomposable graphs by inspection, but only if the potentials are defined on maximal cliques. That is, our parameterization must be such that the set $C$ ranges over the maximal cliques in the graph. Given this constraint, the recipe is the following:

<ul>
<li>for every clique C, set the clique potential to the empirical marginal for that clique,</p></li>
<li><p>for every non-empty intersection between cliques, associate an empirical marginal with that intersection, and divide that empirical marginal into the potential of one of the two cliques that form the intersection.</p></li>
</ul>

<h2>Iterative proportional fitting</h2>

<blockquote>
  <p>We now turn to an alternative algorithm for finding maximum likelihood estimates in all undirected graphs, whether decomposable and nondecomposable. In the decomposable case the algorithm turns out to converge in a finite number of iterations, updating each potential once. Indeed, in this case, the algorithm essentially implements the recipe for decomposable graphs that we described above. Thus the algorithm can be viewed as a general solution to the maximum likelihood estimation problem that takes advantage of the decomposable structure in the problem if it is present.
</blockquote>

IPF, however, does converge, and does behave well -- the log likelihood is guaranteed to increase or remain the same after each IPF update. The facts about IPF follow from the fact that it is not only a fixed-point algorithm, but that it is also a coordinate ascent algorithm.

IPF 迭代更新公式如下：(其中 $\tilde{p}(x)$ 为经验分布: $\tilde{p}(x) = \frac{m(x_C)}{N}$)

$$ \psi_C^{(t+1)} = \psi_C^{(t)} \frac{\tilde{p}(x_C)}{p^{(t)}(x_C)}$$

<strong>Properties of the IPF update equation</strong>

The IPF update equation has two interesting properties:

<ol>
<li>the marginal $p^{(t+1)}(x_C)$ is equal to the empirical marginal $\hat{p}(x_C)$, and</p></li>
<li><p>the normalization factor Z remains constant across IPF updates.</p></li>
</ol>

<p>IPF works toward the goal of equal model and empirical marginals, by equating a single model marginal and empirical marginal at a time.

从前面的 IPF 迭代公式 和 迭代公式的性质, 容易得到 IPF 的另一种形式（使用 joint probability）

$$ p^{(t+1)}(x_\nu) = p^{(t)}(x_\nu) \frac{\tilde{p}(x_C)}{p^{(t)}(x_C)} $$

$$ p^{(t+1)}(x_\nu) = p^{(t)}(x_\nu | x_C) \tilde{p}(x_C) $$

上面的公式给出了 IPF 的另一种解释：

<blockquote>
  retaining the "old" conditional probability $p^{(t)}(x_\nu | x_C)$, while replacing the "old" marginal probability $p^{(t)}(x_C)$ with the "new" marginal $\tilde{p}(x_C)$.
</blockquote>

<strong>IPF as coordinate ascent</strong>

<strong>View from the KL divergence</strong>

<h2>Gradient ascent</h2>

<h1>Latent variables</h1>

<h1>Summary</h1>

<h1>参考资料</h1>

<ul>
<li><a href="">Jordan book: PGM chapter 08</a></li>
</ul>