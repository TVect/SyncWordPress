---
ID: 738
post_title: 小结-20200122
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/738
published: true
post_date: 2020-01-22 22:22:26
---
[toc]

这部分内容整理来源于 Jordan Book：An Introduction to PGMs，Chapter10. Mixtures and conditional mixtures 和 Chapter 11. The EM algorithm.

第10章主要是考虑的模型带有隐变量且隐变量是离散变量的情况（且隐变量之前无依赖关系），并以启发式的方式引入了 EM algorithm，第 11 章给出了 EM algorithm 的严肃的推导.

<!--more-->

<h1>Mixtures and conditional mixtures</h1>

In this chapter we discuss two kinds of models based on discrete latent variables--unconditional mixture models and conditional mixture models. Roughly speaking, unconditional mixture models are used to solve density estimation problems, whereas conditional mixture models are used to solve regression and classification problems.

<h2>Unconditional mixture models</h2>

<h3>Gaussian mixture models</h3>

<strong>EM算法的好处：</strong>

Its important virtue in the graphical model setting is that it allows us to take full advantage of the graphical structure underlying the likelihood; in particular, we will be able to exploit the inference algorithms discussed in Chapter 3 and Chapter 17. By relating the problem of parameter estimation and the problem of efficient inference, the EM algorithm brings together two of our major themes.

<h3>The K-means algorithm</h3>

<strong>K-means算法的收敛性</strong>：

It is easily seen that the K -means algorithm must converge after a finite number of iterations, since there are only a finite number of possible assignments for the set of discrete variables $z_i^n$ and for each such assignment there is a unique value for the $\mu_i$.

每一轮迭代，$J$ 都是单调递减的，也即是每一轮的 $\mu_1, ... \mu_k$ 是不重复的，而且只有有限种样本点分配方式(有限种 $\mu_1, ... \mu_k$)，所以必定会在有限步之后收敛.

<h3>The EM algorithm</h3>

<blockquote>
  Note that the EM algorithm takes many more iterations to reach (approximate)  convergence compared with the K-means algorithm, and that each cycle requires significantly more computation. It is therefore common to run the K-means algorithm in order to find a suitable initialization for a Gaussian mixture model which is subsequently adapted using EM.
  
  The covariance matrices can conveniently be initialized to the sample covariances of the clusters found by the K-means algorithm, and the mixing proportions can be set to the fractions of data points assigned to the respective clusters.
</blockquote>

<h3>Necessary conditions</h3>

可以直接对 GMM 求倒数，得到迭代更新公式，也即为 EM 的更新公式. 这也说明了 EM 迭代公式的合理性.

<h3>The expected complete log likelihood</h3>

<h2>Conditional mixture models</h2>

$$ p(y | x, \theta) = \sum_i p(Z^i = 1 | x; \xi) p(y | Z^i=1, x; \theta_i)$$

<h3>Examples</h3>

<strong>Mixtures of linear regressions</strong>

<strong>Mixtures of logistic regressions</strong>

<h3>Parameter estimation via the EM algorithm</h3>

<h3>An on-line algorithm</h3>

<hr />

<h1>The EM algorithm</h1>

<h2>Latent variables and parameter estimation</h2>

<h2>The general setting</h2>

<strong>E-step</strong>: $ q^{(t+1)} = \arg \max_q L(q, \theta^{(t)}) $

<strong>M-step</strong>: $ \theta^{(t+1)} = \arg \max_{\theta} L(q^{(t+1)}, \theta)$

<blockquote>
  In summary, we have shown that the EM algorithm is a hill-climbing algorithm in the log likelihood $l(\theta; x)$. The algorithm achieves this hill-climbing behavior indirectly, by coordinate ascent in the auxiliary function $L(q; \theta)$. The advantage of working with the latter function is that it involves maximization of the expected complete log likelihood rather than the log likelihood itself, and, as we have seen in examples, this is often a substantial simplication.
</blockquote>

<h2>EM and alternating minimization</h2>

<h1>参考资料</h1>

<ul>
<li><p><a href="">Jordan book: PGM chapter 10. Mixtures and conditional mixtures</a></p></li>
<li><p><a href="">Jordan book: PGM chapter 11. The EM algorithm</a></p></li>
</ul>