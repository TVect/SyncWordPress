---
ID: 211
post_title: ConvS2S 笔记
author: Chin
post_excerpt: ""
layout: post
permalink: http://blog.tvect.cc/2018/05/15/convs2s/
published: true
post_date: 2018-05-15 11:36:19
---
<strong>下面是 Convolutional Sequence to Sequence Learning 的一些笔记。</strong>

之前流行的 seq2seq learning 是使用 RNN。文章中引入了一种完全基于 CNN 的结构。与 RNN 相比，CNN 在训练期间可以在所有元素上并行，有效的利用了 GPU 硬件。另外，在 CNN 中，执行的非线性变换的数量与序列长度无关，可以更容易的进行优化。

<h1>概述</h1>

<h2>RNN vs. CNN</h2>

<ul>
<li>和 RNN 相比，CNN 很少用于序列建模。
虽然卷积只能可以获取固定窗口上下文的表示，但可以通过堆叠多层的 CNN 可以有效的增加 context size。这种堆叠的操作可以精确的控制能够建模的最大依赖长度。</p></li>
<li><p>CNN 可以很好的在序列元素上并行化。
CNN 计算中不依赖于前一个 time step 的计算结果，因此可以在序列元素维度上并行化。而 RNN 需要先计算出整个前面时间步的 hidden state，这限制了 RNN 在序列内部的并行化。</p></li>
<li><p>CNN 的层次化表示可以使用 shorter path to capture long-range dependencies.
CNN 可以在输入序列上构造层次化的表示，相近的元素会在 lower layer 上关联，较远的元素会在 higher layer 上关联。
Hierarchical structure provides a shorter path to capture long-range dependencies
compared to the chain structure modeled by recurrent networks.
例如，要得到一个特征表示，捕获窗口为n的relationship, Hierarchical structure 需要执行 $$ O(n/k) $$ 次宽度为 $$k$$ 的卷积操作，而在 RNN 中需要执行 $$ O(n) $$ 次操作。</p></li>
<li><p>CNN 可以让学习更容易。
对于输入序列中的每一个元素，CNN所进行的非线性运算次数都是恒定的；RNN（单向RNN）则不然，序列中的第一个元素会进行n次非线性运算，而最后一个元素则仅需进行1次非线性运算。</p></li>
</ul>

<h2>本文的贡献</h2>

<p>本文提出了一个完全基于卷积网络的序列学习模型，而且模型中加入了GLU与残差连接。另外，每个解码器层加入了注意力机制，并且证明每个注意力层只增加了很小甚至可以被忽略的开销。

The combination of these choices enables us to tackle large scale problems

<h1>模型</h1>

<h2>模型结构</h2>

<img src="http://www.tvect.cc/wp-content/uploads/2018/05/convs2s.png" alt="" />

<h2>模型输入 + position embedding</h2>

<strong>encoder部分</strong>的输入序列为 $$ e = (e_{1}, e_{2}, ... , e_{n}) $$, 其中 $$e_{i} = w_{i} + p_{i}$$，$$w_{i}$$ 表示词 $$x_{i}$$的word embedding, $$p_{i}$$ 表示词 $$x_{i}$$的position embedding.

<strong>decoder部分</strong>在每一个时刻的输入序列 $$g = (g_{1},...,g_{m}) $$, 同样由两部分组成：上一时刻decoder输出词的word embedding以及对应的position embedding.

<h2>Convolutional Block Structure</h2>

encoder 和 decoder 部分同样都使用了 convolution block structure, 基于固定数目的输入元素去计算中间状态。
记 Decoder 中第 l 个 block (或 layer) 的输出为 $$ h^{l} =  (h_{1}^{l},...,h_{n}^{l}). Encoder 中第 l 个 block (或 layer) 的输出 $$ z^{l}=(z_{1}^{l},...,z_{m}^{l}).

在每个conv block中包含了一个卷积操作，以及随后的一个非线性变换。通过堆叠conv layer，可以让每个输出元素依赖更多的输入元素。

<ul>
<li><strong>卷积计算：</strong>
卷积核的大小为 $$W^{kd*2d}$$，其中 $$d$$ 为词向量长度，$$k$$ 为卷积窗口大小，每次卷积生成两列 $$d$$ 维向量 $$ Y =[A,B] \\in R^{2d} $$。</p></li>
<li><p><strong>非线性计算：</strong>
非线性部分采用的是门控结构 gated linear units（GLM）。计算公式如下：
$$
v([A,B]) = A \\otimes \\delta(B)
$$
其中，$$\\delta (B)$$是门控函数，控制着网络中的信息流，即哪些能够传递到下一个神经元中。</p></li>
<li><p><strong>残差连接：</strong>
把输入与输出相加，输入到下一层网络中。
$$
h^l_i =v(W^l[h^{l-1}_{i-k/2} ,...,h^{l-1}_{i+k/2} ]+b^l )+h^{l-1}_i
$$</p></li>
<li><p><strong>输出：</strong>
decoder的最后一层卷积层的最后一个单元输出经过softmax得到下一个目标词的概率。
$$
p(y_{i+1}|y_1, . . . , y_i, x) = softmax(W_o h^L_i + b_o) 
$$</p></li>
</ul>

<h2>Multi-step Attention</h2>

<p><img src="http://www.tvect.cc/wp-content/uploads/2018/05/convs2s_att.png" alt="" />
与普通的RNN attention 不同的是，在计算每个$$c^{l}$$时，除了使用encoder 的 output，还要使用 encoder 输入的 embedding。

最终得到$$c_i$$和$$h_i$$相加组成新的$$h_i$$。如此，在每一个卷积层都会进行 attention 的操作，得到的结果输入到下一层卷积层，这就是多跳注意机制multi-hop attention。

<h2>图示</h2>

<img src="https://raw.githubusercontent.com/facebookresearch/fairseq/master/fairseq.gif" alt="" />

<h1>参考资料</h1>

<ul>
<li><p><a href="https://arxiv.org/abs/1705.03122">原始论文：Convolutional Sequence to Sequence Learning</a></p></li>
<li><p><a href="https://zhuanlan.zhihu.com/p/27464080">从《Convolutional Sequence to Sequence Learning》到《Attention Is All You Need》</a></p></li>
<li><p><a href="https://zhuanlan.zhihu.com/p/26918935">《Convolutional Sequence to Sequence Learning》阅读笔记</a></p></li>
<li><p><a href="https://github.com/facebookresearch/fairseq">github代码</a></p></li>
</ul>