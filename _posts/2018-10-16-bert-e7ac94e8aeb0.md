---
ID: 799
post_title: BERT 笔记
author: Chin
post_excerpt: ""
layout: post
permalink: http://blog.tvect.cc/archives/799
published: true
post_date: 2018-10-16 11:15:32
---
[toc]

文中提出了一种新的语言表示模型 BERT(Bidirectional Encoder Representations from Transformers).

它同时基于左侧和右侧的上下文预训练了一个 deep bidirectional representations, 这个预训练的 BERT representations，加上一个附加的输出层之后，做fine-tuned，就可以对一系列的任务产生state-of-the-art的结果.

<!--more-->

<h1>概述</h1>

常见的 pre-training general language representations 的方法有：

<ol>
<li><strong>Feature-based Approaches</strong></li>
</ol>

比如 ELMo. ELMo 通过 language model 得到上下文敏感的特征。再把这些上下文敏感的特征结合到任务特定的模型结构中，可以得到了很好的结果。

ELMo相关内容可以参考笔记<a href="http://blog.tvect.cc/archives/608">ELMo: Deep contextualized word representations 阅读笔记</a>

<ol start="2">
<li><strong>Fine-tuning Approaches</strong></li>
</ol>

A recent trend in transfer learning from language models (LMs) is to pre-train some model architecture on a LM objective before fine-tuning that same model for a supervised downstream task. The advantage of these approaches is that few parameters need to be learned from scratch.

比如 OpenAI GPT. 后面会对 OpenAI GPT 做一些笔记.

文章中提到这些方法有些缺陷，比如训练的语言模型是 unidirectional. 而提出的 BERT 中<strong>使用了 Masked LM 来处理 unidirectional 的问题，另外引入了 "next sentence prediction" task, 刻画了 sentence pair 之间的关系</strong>。

<h1>BERT</h1>

<h2>模型结构</h2>

使用的是 multi-layer bidirectional Transformer encoder, 也就是 <strong>[Attention is all you need]</strong> 中结构的 encoder 部分.

考虑对比了两种 model sizes:

<ul>
<li><strong>BERT_BASE</strong>: L=12, H=768, A=12, Total Parameters=110M</p></li>
<li><p><strong>BERT_LARGE</strong>: L=24, H=1024, A=16, Total Parameters=340M</p></li>
</ul>

<p>BERT_BASE 和 OpenAI GPT 的 model size 是一样的，但是 OpenAI GPT 中用的结构是 Transformer decoder，即每个token只能 attend 到它的左边部分的上下文.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/10/architectures-1024x389.png" alt="" />

Transformer结构的解释见 <a href="http://blog.tvect.cc/archives/228">Attention is all you need 笔记笔记</a>, 代码实现见 <a href="https://github.com/tensorflow/tensor2tensor">tensor2tensor</a>.

<h2>Input Representation</h2>

<img src="http://blog.tvect.cc/wp-content/uploads/2018/10/input-1024x360.png" alt="" />

<ul>
<li><strong>Token Embeddings</strong>
使用了 WordPiece embeddings 有 30,000 tokens.
用 ## 表示 split word pieces，另外在每个 sentence / sentence pair 前面加上一个 special classification embedding([CLS]). [cls] 最后的表示可以用来做分类任务.</p></li>
<li><p><strong>positional embeddings</strong>
使用了一个学到的 positional  embedding，最长可以支持 512 tokens.</p></li>
<li><p><strong>Segment Embeddings</strong>
Sentence pairs 放在一起弄成一个 sequence, 用a special token ([SEP]) 分隔两个句子. 为第一个句子加上学到的 sentence embedding A, 第二个句子加上学到的 sentence embedding B.</p></li>
</ul>

<h2>Pre-training Tasks</h2>

<h3>Masked LM</h3>

<p>随机隐掉 input tokens 中一定比例的 tokens, 然后训练模型以预测这些隐掉的 tokens. 和 denosing auto-encoders 对比的话，这里只预测 masked tokens，并不是要重构整个 input.

<strong>?????? 没有太明白这样实作的意图 ??????</strong>
考虑到 mismatch between pre-training and finetuning, since the [MASK] token is never seen during fine-tuning. 文中提到的实际的做法如下：

in the sentence <em>my dog is hairy</em> it chooses <em>hairy</em>. It then performs the following procedure:
- Rather than always replacing the chosen words with [MASK], the data generator will do the following:

<ul>
<li>80% of the time: Replace the word with the [MASK] token, e.g., <em>my dog is hairy</em> -> <em>my dog is [MASK]</em></p></li>
<li><p>10% of the time: Replace the word with a random word, e.g., <em>my dog is hairy</em> -> <em>my dog is apple</em></p></li>
<li><p>10% of the time: Keep the word unchanged, e.g., <em>my dog is hairy</em> -> <em>my dog is hairy</em>.
The purpose of this is to bias the representation towards the actual observed word.</p></li>
</ul>

<h3>Next Sentence Prediction</h3>

<p>为了刻画句子之间的关系，这里做了一个二分类预测，判断 sentence B 是不是 sentence A 的后续.

<h2>Fine-tuning Procedure</h2>

对于 sentence-level classification task, 用 [CLS] 最后的表示做 softmax 分类, 会 fine-tune softmax的参数和 BERT 的参数.

对于 token-level prediction, 比如 NER, 会用每个 token 最后的表示做 softmax 分类.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/10/task.png" alt="" />

<h1>番外 - OpenAI GPT</h1>

<ul>
<li><strong>Unsupervised pre-training</strong></li>
</ul>

一个 multi-layer Transformer decoder， 训练 a standard language modeling.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/10/pre-train-558x300.png" alt="" />

<ul>
<li><strong>Supervised fine-tuning</strong></li>
</ul>

以分类任务为例，使用 final transformer block’s activation 作为输入到 added linear output layer，做分类预测。

另外，加上了一个 language modeling as auxiliary objective to the fine-tuning，作者说这可以提升监督模型的泛化能力，同时加速收敛。

另外，对于输入是 sentence pair 的情况，会在两个句子之间加上  a delimiter token ($)，剩余的和前面的分类任务一致。

<img src="http://blog.tvect.cc/wp-content/uploads/2018/10/fine-tuning-e1539661014446.png" alt="" />

<ul>
<li><strong>Task-specific input transformations</strong></li>
</ul>

<img src="http://blog.tvect.cc/wp-content/uploads/2018/10/task-specific-1024x507.png" alt="" />

<h1>参考资料</h1>

<ul>
<li><p><a href="https://arxiv.org/abs/1810.04805">原始论文 BERT: Pre-training of Deep Bidirectional Transformers for LU</a></p></li>
<li><p><a href="http://blog.tvect.cc/archives/608">ELMo 笔记</a></p></li>
<li><p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">论文: Improving Language Understanding by Generative Pre-Training</a></p></li>
<li><p><a href="https://www.zhihu.com/question/298203515">知乎: 如何评价 BERT 模型？</a></p></li>
</ul>