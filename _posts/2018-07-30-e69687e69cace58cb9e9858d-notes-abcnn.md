---
ID: 597
post_title: '文本匹配 notes &#8211; ABCNN'
author: Chin
post_excerpt: ""
layout: post
permalink: 'http://blog.tvect.cc/2018/07/30/%e6%96%87%e6%9c%ac%e5%8c%b9%e9%85%8d-notes-abcnn/'
published: true
post_date: 2018-07-30 18:28:46
---
[toc]

之前的笔记中已经对文本匹配的相关paper做了一些梳理，可以参看：<a href="http://blog.tvect.cc/2018/07/05/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D-notes/">文本匹配 notes</a>

这里是一些补充.

<h1>概述</h1>

这篇文章中使用了 Attention Based Convolutional Neural Network (ABCNN) 去处理文本匹配类的任务，其提出了三种 attention 机制来在 CNN 中加入句子间的交互影响。

<blockquote>
  CNNs benefit from incorporating attention into representations of local phrases detected by filters; in contrast, LSTMs encode the whole context to form attention-based word representations
</blockquote>

<h1>模型</h1>

<h2>BCNN: Basic Bi-CNN</h2>

模型基本结构示意图如下：

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/bcnn.png" alt="" />

<ul>
<li><strong>Input layer</strong>
普通的 word embedding 作为输入</p></li>
<li><p><strong>Convlution ayer</strong>
类似于普通的卷积，但是做conv之后，序列长度会增加为 s+w-1</p></li>
<li><p><strong>Average pooling layer</strong>
对最后一个 conv layer 输出的 feature map, 执行的是 all-ap, 即 column-wise averaging over all columns, 得到两个 sentence 的向量表示.
对于前面的 conv layer 输出的 feature map, 执行的是 w-ap, 即 column-wise averaging over windows of w consecutive columns, 使序列长度由 s+w-1 重新变为 s.</p></li>
<li><p><strong>Output layer</strong>
依据不同的任务做 softmax 或者 logistic regression 等.</p></li>
</ul>

<h2>ABCNN: Attention-Based BCNN</h2>

<p>ABCNN 在 BCNN 的基础上增加了 attention 操作，刻画了句子之间的交互影响。

文章中描述了基于BCNN的三种结构：ABCNN-1, the ABCNN-2 和 ABCNN-3.

<h3>ABCNN-1</h3>

ABCNN-1 是在 conv layer 的输入表示上做 attention.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/abcnn-1.png" alt="" />

<ul>
<li><strong>计算 attention matrix A</strong>
A 是由对左右两个句子中每个 unit 算匹配得分得到的.
attention matrix A 中每一行，可以视为左边句子每个 unit 的向量表示. 相应的, A 的每一列可以视为右边句子每个 unit 的向量表示. 因此, 可以将 A 视为一个 feature map. 很自然的想法是要将 A 和 原始的 representation feature maps, 一起作为 conv layer 的输入.

$$
	A_{i, j} = MatchScore(F_{0,r}[:, i], F_{1,r}[:, j])
	$$
其中, $$F_{k,r} \\in R^{d \\times  x}$$ 表示句子 k 的 representation feature map.
MatchScore 可以使用 $$1/(1+|x-y|)$$.</p></li>
<li><p><strong>计算 attention feature map</strong>
attention feature map $$ F_{0,a}, F_{1, a} $$ 的计算公式如下:

$$
	\\begin{aligned}
	F_{0,a} &= W_{0} A^{T} \\\\
	F_{1,a} &= W_{1} A \\\\
	\\end{aligned}
	$$
其中, $$W_{i} \\in R^{d \\times s}$$ 是要学习的参数.</p></li>
<li><p><strong>做 convolution</strong>
将 representation feature maps 和 attention feature map 作为两个channel, 当做 conv 的输入. 卷积之后的句子长度为 s+w-1</p></li>
</ul>

<h3>ABCNN-2</h3>

<p>ABCNN-2 是在 conv layer 的输出上做 attention.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/abcnn-2.png" alt="" />

<ul>
<li><strong>计算 attention matrix A</strong>
attention matrix A compares 比较了两个句子中每个单元.
<strong>类似前面的 ABCNN-1 中 attention 的计算 ??? 是直接在原始的feature map上计算的, 还是在conv layer 的输出上做计算的 ???</strong></p></li>
<li><p><strong>attention weighted w-ap</strong>
对于 attention matrix $$ A \\in R_{x \\times x} $$, 分别按行和按列求和, 得到第一个句子中每个 unit 的权重为 $$a_{0, j} = \\sum A[j, :]$$, 第二个句子中每个 unit 的权重为 $$ a_{1, j} = \\sum A[:, j]$$.
attention weighted w-ap 操作的计算公式如下：

$$
	F^{p}_{i, r}[:, j] = \\sum_{k=j: j+w} a_{i, k} F^{c}_{i, r}[:, k]
	$$
其中, $$ F^{c}_{i, r} \\in R^{d \\times (s+w-1)} $$ 是句子 s 做 conv 之后的输出, $$ F^{p}_{i, r} \\in R^{d \\times s} $$.</p></li>
</ul>

<p>显然，可以将 conv-pooling 结构堆叠很多层.

<strong>vs. ABCNN-1</strong>
ABCNN-1 需要有两个参数矩阵 W0, W1, 将 attention matrix 转化为 feature map, 另外因为现在是2 channel, 做卷积操作的参数也会更多。总之与 ABCNN-2 相比，ABCNN-1 需要的参数.

<h3>ABCNN-3</h3>

因为 ABCNN-1 和 ABCNN-2 分别在 convolution 前后做 attention, 可以视为是在不同的语言粒度上的attention. ABCNN-3 尝试结合了 ABCNN-1 和 ABCNN-2.

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/abcnn-3.png" alt="" />

<h1>参考资料</h1>

<ul>
<li><a href="https://arxiv.org/abs/1512.05193">论文：ABCNN for Modeling Sentence Pairs</a></li>
</ul>