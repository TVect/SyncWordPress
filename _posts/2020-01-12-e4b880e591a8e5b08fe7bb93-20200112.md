---
ID: 682
post_title: 一周小结-20200112-Part 1
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/682
published: true
post_date: 2020-01-12 13:26:48
---
本周的阅读笔记，来源于 Jordan Book：An Introduction to PGMs，Chapter05. Statistic Concepts

<!--more-->

<h1>Bayesian and frequentist statistics</h1>

<strong>ML</strong>: $ \hat{\theta} = \arg max_{\theta} p(x|\theta) $

<strong>MAP</strong>: $ \hat{\theta} = \arg max_{\theta} p(\theta | x) =  \arg max_{\theta} p(x | \theta) p(\theta)$

<strong>Bayesian Approach</strong>: $ p(\theta | x) = \frac {p(x|\theta)p(\theta)} {p(x)} $

<h1>Statistical problems</h1>

<h2>Density estimation</h2>

<strong>Univariate Gaussian density estimation</strong>

<strong>Bayesian univariate Gaussian density estimation</strong>

<strong>Density estimation for discrete data</strong>: multinomial probability distribution

<strong>Bayesian density estimation for discrete data</strong>

<strong>Mixture models</strong>

$ p(x | \theta) = \sum_{k=1}^K \alpha_k f_k(x | \theta_k) $, where $ \sum_{k=1}^K \alpha_k = 1 $, and $f_k$ are component desities.

<strong>Nonparameteric density estimation</strong>

<ul>
<li>kernel density estimator</li>
</ul>

$ \hat{p}(x) = \frac{1}{N} \sum_{i=1}^N k(x, x_n, \lambda)$ where $ k(x, x_n, \lambda) $ is a kernel function.

<blockquote>
  A variety of different kernel functions are be used in practice. Simple (e.g., piecewise polynomial) functions are often preferred, partly for computational reasons. Gaussian functions are sometimes used, in which case $x_n$ play the role of the mean and $\lambda$ play the role of the standard deviation.
</blockquote>

<strong>parameteric models &amp; nonparameteric models 的一些对比</strong>

下面的对比主要针对的是 parameteric density estimation &amp; nonparameteric density estimation， 更具体的是 gaussian mixture &amp; kde.

<ol>
<li>how a given model would change if the number of data points $N$ went to increase.

for parametric models, the basic structure of the model remains fixed as $N$ increases.

in the nonparametric case, on the other hand, the class of the densities increases as $N$ increases. (in particular, with $N+1$ data points, it is possible to obtain densities with $N+1$ modes, this is not possible with $N$ data points)</p></li>
<li><p>view the locations of the kernels as "parameters", the number of such "parameters" increases with the number of data points in nonparametric case.

we can view nonparametric models as parametric, but with an unbounded, data-dependent, number of parameters.

parametric models: "finite-dimensional models"

nonparametric models: "infinite-dimensional models"</p></li>
<li><p>Caveats:

in the mixture model setting, we may not know the number $K$ of mixture components in practice and we wish to estimate $K$ from the data.

mixture models can also be used nonparametrically.</p></li>
<li><p>tradeoff between flexibility and statistical efficiency

if the undelying "true" density is a Gaussian:

<ul>
<li>kde: will eventually converge to the true density, but it may require very many data points.</p></li>
<li><p>gaussian mixture: will converge to the true density more rapidly.</p></li>
</ul>

<p>if the undelying density is not a Gaussian:

<ul>
<li>kde: eventually converge to the true density</p></li>
<li><p>gaussian mixture: still converge, but to the wrong density.</p></li>
</ul>

<p>In sum, if we are willing to make more assumptions then we get faster convergence, but with the possibility of poor performance if reality does not match our assumptions. Nonparametric estimators allow us to get away with fewer assumptions, while require more data points for comparable levels of performance.</p></li>
</ol>

<h2>Regression</h2>

<p><strong>Typical linear regression</strong>

$$ p(y|x) \sim N(\beta^T \phi(x), \sigma^2) $$

<strong>Conditional mixture models</strong>

$$ p(y|x) = \sum_{k=1}^K p(z^k=1 | x; \theta) p(y | z^k=1, x; \theta) $$

<ul>
<li>case 1: $z$ is denpendent of $x$</p></li>
<li><p>case 2: $z$ is indenpendent of $x$</p></li>
</ul>

<p><strong>Nonparametric regression</strong>

kernel regression:

$$ \hat{f}(x) = \frac{\sum_{n=1}^N k(x, x_n, \lambda) y_n}{\sum_{n=1}^N k(x, x_n, \lambda)} $$

where, $\hat{f}(x)$ is a estimator of the conditional mean function, $ k(x, x_n, \lambda) $ is a kernel function centered around the data point $x_n$.

<h2>Classification</h2>

<strong>Typical Classification Example</strong>

two classes with equal class priors, Gaussian class-conditional densities -> posterior probability corresponds to a logistic function.

<ul>
<li>generative approach:

estimating both the means and covariances of the Gaussian class-conditional densities, as well as the class priors.</p></li>
<li><p>discriminative approach:

logisitc function is the center object of analysis (and we are not restricted to the logistic function, or to any other function that is derived from a generative model.)</p></li>
</ul>

<p><strong>Remarks</strong>

<ol>
<li>mixture models:

in the context of generative approach, we can use mixture models as class-conditional densities.

in the context of discriminative approach, we can use mixture models to represent the posterior probability $p(q|x)$.</p></li>
<li><p>nonparametric methods:

in the context of generative approach, we can use nonparametric methods to estimate the class-conditional densities.

in the context of discriminative approach, we can use nonparametric methods to estimate the posterior probability.</p></li>
<li><p>bayesian approaches</p></li>
</ol>

<h1>Model selection and model averaging</h1>

<h2>Bayesian approaches</h2>

<p>$$
\begin{aligned}
p(m | x) &amp;= \frac{p(x|m)p(m)}{p(x)} &#92;
p(x | m) &amp;= \int p(x, \theta | m) d\theta &#92;
&amp;= \int p(x, \theta, m) p(\theta | m) d\theta &#92;
p(x_{new} | x) &amp;=  \int \int p(x_{new}, \theta, m | x) d\theta dm &#92;
&amp;= \int \int p(x_{new} | \theta, m) p(\theta, m | x) d\theta dm &#92;
&amp;= \int \int p(x_{new} | \theta, m) p(\theta | m, x) p(m | x) d\theta dm
\end{aligned}
$$

<h2>Frequentist approaches</h2>

consider various model selection procedures, and evaluate these procedures in terms of various frequentist criteria.

<strong>Tips</strong>: Maximum likelihood itself cannot be use as model selection procedure, because maximum likelihood is unable to address the "overfitting"  phenomenon.

<strong>approaches to frequentist model selection</strong>: AIC, cross validation

<h1>参考资料</h1>

<ul>
<li><a href="">Jordan: An Introduction to PGMs Chapter 5</a></li>
</ul>