---
ID: 476
post_title: BERT 改进系列文献整理(二)
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/476
published: true
post_date: 2019-07-15 22:15:07
---
从 BERT 提出之后, 后续出现了很多篇文章基于 BERT 做了各式各样的改进. 这里会尝试对自己看到了关于BERT改进的文献做一些整理。

<h1>MT-DNN (multi-task + pre_train)</h1>

结合了 multi-task learning 和 language model pre-training 来做语言的表示学习.

<strong>效果</strong>:

<ul>
<li>MT-DNN obtains new state-of-the-art results on eight out of nine NLU tasks used in the General Language Understanding Evaluation (GLUE) benchmark</p></li>
<li><p>The representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations.</p></li>
</ul>

<p>总体上说, MT-DNN 也分为预训练和 fine-tuning 两步.

第一步和 Bert 类似, 但是加上了 Multi-task learning. 具体来说, 是使用 all GLUE tasks 做 multi-task learning.

第二步会对每个 GLUE task, 使用 task-specific data 做 fine-tuning.

<h2>模型结构</h2>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/mt-dnn-1024x735.png" width=900 align=center>

<h3>Shared layers</h3>

包含<strong>Lexicon Encoder (l1)</strong> 和 <strong>Transformer Encoder (l2)</strong>, 这部分与之前的 Bert 类似.

<h3>Task specific layers</h3>

<ul>
<li><strong>Single-Sentence Classification Output</strong></li>
</ul>

记输入句子为 X, 用 $\bold{x}$ 表示 token <code>[CLS]</code> 对应的 contextual embedding (l2), 则: $ P_r(c|X) = softmax(W^T_{SST} \bold{x}) $

<ul>
<li><strong>Text Similarity Output</strong></li>
</ul>

记输入句子为 $X_1, X_2$, 用 $\bold{x}$ 表示 token <code>[CLS]</code> 对应的 contextual embedding (l2), 则: $ Sim(X_1, X_2) = w^T_{STS} \bold{x}) $

<ul>
<li><strong>Pairwise Text Classification Output</strong></li>
</ul>

构造 premise P 的 working memory: concatenating the contextual embeddings of the words in P, 得到 $M^p \in \mathbb{R}^{d \times m}$

构造 hypothesis H 的 working memory: concatenating the contextual embeddings of the words in H, 得到 $M^h \in \mathbb{R}^{d \times n}$

执行 K-step reasoning on the memory:

$$
\begin{aligned}
s^k &amp;= GRU(s^{k-1}, x^k) &#92;
s^0 &amp;= \sum_j \alpha_j M_j^h , \quad \alpha_j = \frac{exp(w_1^T M_j^h)}{\sum_i exp(w_1^T M_i^h)} &#92;
x^k &amp;= \sum_j \beta_j M_j^p , \quad \beta_j = softmax(s^{k-1} W_2^T M^p) &#92;
P_r^k &amp;= softmax(W_3^T[s^k; x^k; |s^k-x^k|; s^k \dot x^k]) &#92;
P_r &amp;= avg ([P_r^0, ..., P_r^{k-1}]) &#92;
\end{aligned}
$$

<ul>
<li><strong>Relevance Ranking Output</strong></li>
</ul>

用 $\bold{x}$ 表示 token <code>[CLS]</code> 对应的 contextual embedding (l2), 则: $ Rel(Q, A) = g(w^T_{QNLI} \bold{x}) $.

对于一个给定的 Question, 基于上面计算的 $Rel(Q, A)$ 排序候选 Answers.

但是 MT-DNN 的训练阶段包含两步, 一步是和 Bert 中的 pretraining 一致, 另一步是要做 multi-task learning

<h2>The Training Procedure</h2>

MT-DNN 的训练包含两个步骤, 一个步骤是做 pretraining, 这与 Bert 一致, 另一个步骤是做 multi-task learning.

对于分类任务, 目标函数为: $ - \sum_c 1(X, c) \log P_r(c | X) $

对于回归任务, 目标函数为: $ (y - Sim(X_1, X_2))^2 $

对于 relevance ranking 任务, 目标函数为: $ - \sum_(Q, A^+) P_r(A^+ | Q) \quad P_r(A^+ | Q) = \frac{exp(\gamma Rel(Q, A^+))}{\sum_{A^{\prime} \in A}exp(\gamma Rel(Q, A^{\prime}))}$

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/mt-dnn-alg.png" width=400 align=center>

<h1>XLNet (Autoregressive LM + Autoencoder LM)</h1>

<h1>参考资料</h1>

<ul>
<li><a href="https://arxiv.org/abs/1901.11504">Multi-Task Deep Neural Networks for Natural Language Understanding</a></li>
</ul>