---
ID: 304
post_title: VAE 笔记
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/304
published: true
post_date: 2019-04-20 19:21:38
---
这里主要基于文章Auto-Encoding Variational Bayes做了一些读书笔记.

<h1>概述</h1>

<ul>
<li>a novel estimator of the variational lower bound, Stochastic Gradient VB
(SGVB), for efficient approximate inference with continuous latent variables.</li>
</ul>

<blockquote>
  First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. 
  Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.
</blockquote>

<h1>Method</h1>

<h2>Problem scenario</h2>

<h2>The variational bound</h2>

$$ \log p_\theta(x^1; · · · ; x^N) = \sum_{i=1}^N \log p_{\theta}(x^i) $$

$$
\begin{aligned}
\log p_{\theta}(x^i) &amp;= \int q_{\phi}(z^i | x^i)\log p_{\theta}(x^i) dz_i &#92;

&amp;= \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i, z^i) q_{\phi}(z^i | x^i)}{q_{\phi}(z^i | x^i) p_{\theta}(z^i | x^i)} dz^i &#92;

&amp;= KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i | x^i)) + L(\theta, \phi, x^i)
\end{aligned}
$$

其中 $L(\theta, \phi, x^i) = \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i, z^i)}{q_{\phi}(z^i | x^i)} dz^i = \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} (\log p_{\theta}(x^i, z^i) - \log q_{\phi}(z^i | x^i) )$.

$L$ can also be written as $L = \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i | z^i) p_{\theta}(z^i)}{q_{\phi}(z^i | x^i)} dz^i = -KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i)) + \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} \log p_{\theta}(x^i | z^i)$

为了做 MLE, 参照 EM algorithm, 需要完成下面两步操作:

<ul>
<li>E-step
优化 $\phi$, 使得 $q_{\phi}(z^i | x^i)$ 尽量逼近 $p_{\theta}(z^i | x^i))$, 也即 $ \arg\min_{\phi} KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i | x^i)) = \arg\max_{\phi} L(\theta, \phi, x^i)$</p></li>
<li><p>M-step
优化 $\theta$, 也即 $ \arg\max_{\theta} L(\theta, \phi, x^i)$</p></li>
</ul>

<p>整个问题也就等价于优化 ELBO: $\arg\max_{\phi, \theta} L(\theta, \phi, X)$

下面分别考虑 $L(\theta, \phi, x^i)$ 对于 $\theta, \phi$ 的导数.

<ul>
<li>对于$\theta$的导数</li>
</ul>

$$
\begin{aligned}
\triangledown_{\theta} L(\theta, \phi, x^i) &amp;= \triangledown_{\theta} \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i, z^i)}{q_{\phi}(z^i | x^i)} dz^i &#92;

&amp;= \triangledown_{\theta} \int q_{\phi}(z^i | x^i)\log p_{\theta}(x^i, z^i) dz^i &#92;

&amp;= \int q_{\phi}(z^i | x^i) \triangledown_{\theta}\log p_{\theta}(x^i, z^i) dz^i &#92;

&amp;\simeq \frac{1}{L} \sum_{l=1}^L\log p_{\theta}(x^i, z^{i, l}) &#92;

where &amp;\quad z^{i, l} \sim q_{\phi}(z^i | x^i)
\end{aligned}
$$

<ul>
<li>对于$\phi$的导数</li>
</ul>

$$
\begin{aligned}
\triangledown_{\theta} L(\theta, \phi, x^i) &amp;= \triangledown_{\phi} \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} f(\phi, z^i) = \triangledown_{\phi} \int q_{\phi}(z^i)f(\phi, z^i) dz^i &#92;

&amp;= \int f(\phi, z^i) \triangledown_{\phi} q_{\phi}(z^i) + q_{\phi}(z^i) \triangledown_{\phi} f(\phi, z^i)dz^i &#92;

&amp;=\int f(\phi, z^i) q_{\phi}(z^i) \triangledown_{\phi} \log q_{\phi}(z^i) + q_{\phi}(z^i) \triangledown_{\phi} f(\phi, z^i)dz^i &#92;

&amp;=\int q_{\phi}(z^i) [f(\phi, z^i)  \triangledown_{\phi} \log q_{\phi}(z^i) + \triangledown_{\phi} f(\phi, z^i)] dz^i &#92;

&amp; \simeq \frac{1}{L} \sum_{l=1}^L f(\phi, z^{i,l})  \triangledown_{\phi} \log q_{\phi}(z^{i,l}) + \triangledown_{\phi} f(\phi, z^{i,l}) &#92;

where &amp; \quad z^{i, l} \sim q_{\phi}(z^i | x^i)

\end{aligned}
$$

但是这种对 $\triangledown_{\theta} L(\theta, \phi, x^i) $ 估计的方式, 会导致非常不稳定的随机梯度.

<blockquote>
  This gradient estimator exhibits exhibits very high variance.
</blockquote>

下面会用 <strong>Reparameterization trick</strong> 解决这个问题.

<h3>Reparameterization trick</h3>

reparameterize the random variable $z \sim q_{\phi}(z | x)$ using a differentiable transformation $g_{\phi}(\epsilon; x)$ of an (auxiliary) noise variable $\epsilon$:

$$z = g_{\phi}(\epsilon; x) \quad with \quad \epsilon ∼ p(\epsilon)$$

此时, $\mathbb{E}<em>{q</em>{\phi}(z | x)} f(\phi, z) = \mathbb{E}<em>{p(\epsilon)} f(\phi, g</em>{\phi}(\epsilon; x))$, 所以 $L$ 关于 $\phi$ 的导数可以写为如下形式:

$$
\begin{aligned}
\triangledown_{\phi} L &amp;= \triangledown_{\phi} \mathbb{E}<em>{p(\epsilon)} f(\phi, g</em>{\phi}(\epsilon; x)) = \triangledown_{\phi} \int p(\epsilon) f(\phi, g_{\phi}(\epsilon; x)) d\epsilon &#92;

&amp;= \int p(\epsilon) \triangledown_{\phi} f(\phi, g_{\phi}(\epsilon; x)) d\epsilon &#92;

&amp;\simeq \frac{1}{L} \sum_{l=1}^L \triangledown_{\phi} f(\phi, g_{\phi}(\epsilon^{l}; x)) &#92;

where &amp;\quad \epsilon^{l} \sim p(\epsilon)
\end{aligned}
$$

<h2>The SGVB estimator and AEVB algorithm</h2>

<h3>SGVB estimator</h3>

将 Reparameterization trick 技巧用到前面推导出的变分下界的两种不同表达式上, 可以相应得到两种 Stochastic Gradient Variational Bayes (SGVB) estimator.

<ul>
<li><strong>first version of the SGVB estimator</strong></li>
</ul>

将 Reparameterization trick 用于表达式: $L(\theta, \phi, x^i) = \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} (\log p_{\theta}(x^i, z^i) - \log q_{\phi}(z^i | x^i) ) $, 得到的 SGVB estimator 如下:

$\tilde{L}^A(\theta, \phi, x^i) = \frac{1}{L} \sum_{l} [\log p_{\theta}(x^i, z^{i,l}) - \log q_{\phi}(z^{i, l} | x^i)]$

其中, $z^{i, l} = g_{\phi}(\epsilon^{i, l}; x^i) \quad with \quad \epsilon^{i,l} ∼ p(\epsilon)$

<ul>
<li><strong>second version of the SGVB estimator</strong></li>
</ul>

将 Reparameterization trick 用于表达式: $L = -KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i)) + \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} \log p_{\theta}(x^i | z^i)$, 得到的 SGVB estimator 如下:

$\tilde{L}^B(\theta, \phi, x^i) = -KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i)) + \frac{1}{L} \sum_{l} \log p_{\theta}(x^i | z^{i, l})]$

其中, $z^{i, l} = g_{\phi}(\epsilon^{i, l}; x^i) \quad with \quad \epsilon^{i,l} ∼ p(\epsilon)$

<h2>Variational Auto-Encoder</h2>

<h1>Tips</h1>

<ul>
<li>PCA &amp; VAE</p></li>
<li><p><strong>Monte Carlo EM &amp; gradient-based MCMC</strong></p></li>
</ul>

<p>不太清楚这一块的知识.

<ul>
<li>CVAE</li>
</ul>

<h1>参考资料</h1>

<ul>
<li><p><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></p></li>
<li><p><a href="https://drive.google.com/file/d/1NqtMy7uMti9Xrsck9WIqvv8o3PWP1jS4/view">Scalable Bayesian methods</a></p></li>
<li><p><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></p></li>
</ul>