---
ID: 304
post_title: VAE 笔记
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/304
published: true
post_date: 2019-04-20 19:21:38
---
这里主要基于文章Auto-Encoding Variational Bayes做了一些读书笔记.

<h1>概述</h1>

<ul>
<li>introduce a novel estimator of the variational lower bound, <strong>Stochastic Gradient VB (SGVB)</strong>, for efficient approximate inference with continuous latent variables.</p></li>
<li><p>For the case of i.i.d. datasets and continuous latent variables per datapoint, introduce an efficient algorithm for efficient inference and learning, <strong>Auto-Encoding VB (AEVB)</strong>, that learns an approximate inference model using the SGVB estimator.</p></li>
</ul>

<h1>Method</h1>

<h2>The variational bound</h2>

<p>$$ \log p_\theta(x^1; · · · ; x^N) = \sum_{i=1}^N \log p_{\theta}(x^i) $$

$$
\begin{aligned}
\log p_{\theta}(x^i) &amp;= \int q_{\phi}(z^i | x^i)\log p_{\theta}(x^i) dz_i &#92;

&amp;= \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i, z^i) q_{\phi}(z^i | x^i)}{q_{\phi}(z^i | x^i) p_{\theta}(z^i | x^i)} dz^i &#92;

&amp;= KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i | x^i)) + L(\theta, \phi, x^i)
\end{aligned}
$$

其中 $L(\theta, \phi, x^i) = \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i, z^i)}{q_{\phi}(z^i | x^i)} dz^i = \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} (\log p_{\theta}(x^i, z^i) - \log q_{\phi}(z^i | x^i) )$.

另外, $L$ 也可以写为 $L = \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i | z^i) p_{\theta}(z^i)}{q_{\phi}(z^i | x^i)} dz^i = -KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i)) + \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} \log p_{\theta}(x^i | z^i)$. 这种写法在等号右边第一项的KL divergence 可以直接求出的情况下会很有用.

为了做 MLE, 参照 EM algorithm, 需要完成下面两步操作:

<ul>
<li>E-step
优化 $\phi$, 使得 $q_{\phi}(z^i | x^i)$ 尽量逼近 $p_{\theta}(z^i | x^i))$, 也即 $ \arg\min_{\phi} KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i | x^i)) = \arg\max_{\phi} L(\theta, \phi, x^i)$</p></li>
<li><p>M-step
优化 $\theta$, 也即 $ \arg\max_{\theta} L(\theta, \phi, x^i)$</p></li>
</ul>

<p>整个问题也就等价于优化 ELBO: $\arg\max_{\phi, \theta} L(\theta, \phi, X)$

下面分别考虑 $L(\theta, \phi, x^i)$ 对于 $\theta, \phi$ 的导数.

<ul>
<li>对于$\theta$的导数</li>
</ul>

$$
\begin{aligned}
\triangledown_{\theta} L(\theta, \phi, x^i) &amp;= \triangledown_{\theta} \int q_{\phi}(z^i | x^i)\log \frac{p_{\theta}(x^i, z^i)}{q_{\phi}(z^i | x^i)} dz^i &#92;

&amp;= \triangledown_{\theta} \int q_{\phi}(z^i | x^i)\log p_{\theta}(x^i, z^i) dz^i &#92;

&amp;= \int q_{\phi}(z^i | x^i) \triangledown_{\theta}\log p_{\theta}(x^i, z^i) dz^i &#92;

&amp;\simeq \frac{1}{L} \sum_{l=1}^L\log p_{\theta}(x^i, z^{i, l}) &#92;

where &amp;\quad z^{i, l} \sim q_{\phi}(z^i | x^i)
\end{aligned}
$$

<ul>
<li>对于$\phi$的导数</li>
</ul>

$$
\begin{aligned}
\triangledown_{\theta} L(\theta, \phi, x^i) &amp;= \triangledown_{\phi} \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} f(\phi, z^i) = \triangledown_{\phi} \int q_{\phi}(z^i)f(\phi, z^i) dz^i &#92;

&amp;= \int f(\phi, z^i) \triangledown_{\phi} q_{\phi}(z^i) + q_{\phi}(z^i) \triangledown_{\phi} f(\phi, z^i)dz^i &#92;

&amp;=\int f(\phi, z^i) q_{\phi}(z^i) \triangledown_{\phi} \log q_{\phi}(z^i) + q_{\phi}(z^i) \triangledown_{\phi} f(\phi, z^i)dz^i &#92;

&amp;=\int q_{\phi}(z^i) [f(\phi, z^i)  \triangledown_{\phi} \log q_{\phi}(z^i) + \triangledown_{\phi} f(\phi, z^i)] dz^i &#92;

&amp; \simeq \frac{1}{L} \sum_{l=1}^L f(\phi, z^{i,l})  \triangledown_{\phi} \log q_{\phi}(z^{i,l}) + \triangledown_{\phi} f(\phi, z^{i,l}) &#92;

where &amp; \quad z^{i, l} \sim q_{\phi}(z^i | x^i)

\end{aligned}
$$

但是这种对 $\triangledown_{\theta} L(\theta, \phi, x^i) $ 估计的方式, 会导致非常不稳定的随机梯度.

<blockquote>
  This gradient estimator exhibits exhibits very high variance.
</blockquote>

下面会用 <strong>Reparameterization trick</strong> 解决这个问题.

<h3>Reparameterization trick</h3>

reparameterize the random variable $z \sim q_{\phi}(z | x)$ using a differentiable transformation $g_{\phi}(\epsilon; x)$ of an (auxiliary) noise variable $\epsilon$:

$$z = g_{\phi}(\epsilon; x) \quad with \quad \epsilon ∼ p(\epsilon)$$

此时, $\mathbb{E}<em>{q</em>{\phi}(z | x)} f(\phi, z) = \mathbb{E}<em>{p(\epsilon)} f(\phi, g</em>{\phi}(\epsilon; x))$, 所以 $L$ 关于 $\phi$ 的导数可以写为如下形式:

$$
\begin{aligned}
\triangledown_{\phi} L &amp;= \triangledown_{\phi} \mathbb{E}<em>{p(\epsilon)} f(\phi, g</em>{\phi}(\epsilon; x)) = \triangledown_{\phi} \int p(\epsilon) f(\phi, g_{\phi}(\epsilon; x)) d\epsilon &#92;

&amp;= \int p(\epsilon) \triangledown_{\phi} f(\phi, g_{\phi}(\epsilon; x)) d\epsilon &#92;

&amp;\simeq \frac{1}{L} \sum_{l=1}^L \triangledown_{\phi} f(\phi, g_{\phi}(\epsilon^{l}; x)) &#92;

where &amp;\quad \epsilon^{l} \sim p(\epsilon)
\end{aligned}
$$

<h2>The SGVB estimator and AEVB algorithm</h2>

<h3>SGVB estimator</h3>

将 Reparameterization trick 技巧用到前面推导出的变分下界的两种不同表达式上, 可以相应得到两种 Stochastic Gradient Variational Bayes (SGVB) estimator.

<ul>
<li><strong>first version of the SGVB estimator</strong>
将 Reparameterization trick 用于表达式: $L(\theta, \phi, x^i) = \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} (\log p_{\theta}(x^i, z^i) - \log q_{\phi}(z^i | x^i) ) $, 得到的 SGVB estimator 如下:
$$\tilde{L}^A(\theta, \phi, x^i) = \frac{1}{L} \sum_{l} [\log p_{\theta}(x^i, z^{i,l}) - \log q_{\phi}(z^{i, l} | x^i)]$$
其中, $z^{i, l} = g_{\phi}(\epsilon^{i, l}; x^i) \quad with \quad \epsilon^{i,l} ∼ p(\epsilon)$</p></li>
<li><p><strong>second version of the SGVB estimator</strong>
将 Reparameterization trick 用于表达式: $L = -KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i)) + \mathbb{E}<em>{q</em>{\phi}(z^i | x^i)} \log p_{\theta}(x^i | z^i)$, 得到的 SGVB estimator 如下:
$$\tilde{L}^B(\theta, \phi, x^i) = -KL(q_{\phi}(z^i | x^i), p_{\theta}(z^i)) + \frac{1}{L} \sum_{l} \log p_{\theta}(x^i | z^{i, l})$$
其中, $z^{i, l} = g_{\phi}(\epsilon^{i, l}; x^i) \quad with \quad \epsilon^{i,l} ∼ p(\epsilon)$</p></li>
</ul>

<p><strong>整个数据集上的SGVB estimator</strong>

现在考虑包含 $N$ 个数据点的整个数据集 $X$, 可以构造如下的基于mini-batch 的整个数据集的边缘似然下界估计:

$ L(\theta, \phi, X) \simeq \tilde{L}^M(\theta, \phi, X^M) = \frac{N}{M} \sum_{i=1}^{M} \tilde{L}(\theta, \phi, x^i)$

其中 minibatch $X^M = (x^1, ..., x^M)$ 是从整个数据集 $X$ 中随机抽取的 $M$ 个数据点.

文章中提到在 $M$ 足够大时 (e.g. M=100) 时，可以将每个 datapoint 对应的采样隐变量数设置为 1.

<h3>AEVB algorithm</h3>

<img src="https://www.tvect.cn/wp-content/uploads/2019/04/c80d2080ea76d0aad5fc5bc03408bd8f.png" alt="" />

<h2>Example: Variational Auto-Encoder</h2>

取隐变量上的先验为: $p_{\theta}(z) = N(z; 0, I)$

取 $p_{\theta}(x|z)$ 为高斯分布(连续的情况下),或 Bernoulli 分布(binary data情况下). 分布的参数通过神经网络得到.

取 $q_{\phi}(z|x)$ 为一个协方差阵为对角阵的高斯, 即 $q_{\phi}(z|x^i) = N(z; u^i, \sigma^i)$, 其中 $u^i, \sigma^{2i}$ 通过神经网络得到.

使用 Reparameterization trick 技巧之后有: $z^{i, l} = g_{\phi}(\epsilon^{i, l}; x^i) = u^i + \sigma^i \epsilon^{i, l} \quad with \quad \epsilon^{i,l} ∼ N(0, I)$

此时 second version of the SGVB estimator 中的第一项 KL divergence 可以显式求出:

$$-KL(q_{\phi}(z^i), p_{\theta}(z^i)) = \int q_{\phi}(z) \log p_{\theta}(z) dz - \int q_{\phi} \log q_{\phi}(z) dz = \frac{1}{2}\sum_{j=1}^J (1 + \log \sigma_j^2 - u_j^2 - \sigma_j^2)$$

最终的 estimator 形如：

$$ L(\theta, \phi, x^i) = \frac{1}{2}\sum_{j=1}^J (1 + \log \sigma_j^{i 2} - u_j^{i 2} - \sigma_j^{i 2}) + \frac{1}{L} \sum_{l} \log p_{\theta}(x^i | z^{i, l})$$

<h1>其他</h1>

<ul>
<li><p>PPCA &amp; VAE
VAE 可以看做 deep generalization of probabilistic PCA.
之前的 PPCA 中限制 $p(x | z)$ 为 linear Gaussian 的形式, 这里将其扩展到均值方差通过神经网络计算得到的 Gaussian. 在做了这种扩展之后, EM 算法中 $p(z | x, \theta)$ 无法显式求出表达式. 所以需要对其做变分近似, VAE 中使用了另一个均值方差通过神经网络计算得到的 Gaussian 来近似 $p(z | x, \theta)$ .
PPCA的内容可以参考 PRML Chapter 12.2, 从 PPCA 到 VAE 的改进流程可以参考 <a href="https://drive.google.com/file/d/1NqtMy7uMti9Xrsck9WIqvv8o3PWP1jS4/view">Scalable Bayesian methods</a></p></li>
<li><p><strong>Monte Carlo EM &amp; gradient-based MCMC</strong>
文章 Appendix 中提到的 gradient-based MCMC (e.g. HMC), 可以参考 PRML Chapter 11 和 <a href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">Blog: Hamiltonian Monte Carlo explained</a></p></li>
<li><p>CVAE
可以参考 <a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></p></li>
</ul>

<h1>参考资料</h1>

<ul>
<li><p><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></p></li>
<li><p><a href="https://drive.google.com/file/d/1NqtMy7uMti9Xrsck9WIqvv8o3PWP1jS4/view">Scalable Bayesian methods</a></p></li>
<li><p><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></p></li>
</ul>