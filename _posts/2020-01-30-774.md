---
ID: 774
post_title: 小结-20200130
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/774
published: true
post_date: 2020-01-30 11:52:35
---
这部分内容整理来源于 Jordan Book：An Introduction to PGMs，Chapter19. Features, maximum entropy, and duality

第19章介绍了使用 collections of "features" 来表示 potential functions, 并说明了最大熵问题和最大似然问题是拉格朗日对偶的.

第20章重新审视回顾了 decompoable models 中参数学习方法 和 IPF 算法, 同时介绍了 GIS 算法作为 IPF 算法的完全并行化推广.

<!--more-->

<h1>Chapter 19. Features, maximum entropy, and duality</h1>

展示了如何使用 collections of "features" 来表示 potential functions. 最后说明了最大熵问题和最大似然问题是拉格朗日对偶的.

<h2>19.1 Features</h2>

In the general case, we parameterize a clique $C$ as follows.

Define a set of features $f_i(x_{C_i})$, where $C_i \subseteq C$ is the subset of variables that feature $f_i$ references. Associated with each feature there is a scalar parameter $\theta_i$.

Given the features and the parameters we define the clique potential $\psi_C(x_C)$ as follows:

$$
\begin{aligned}
\psi_C(x_C) &amp;= \prod_{i \in I_C} \exp (\theta_i f_i(x_{C_i})) &#92;
&amp;= \exp(\sum_{i \in I_C} \theta_i f_i(x_{C_i}))
\end{aligned}
$$

Taking the product over clique potentials and normalizing, we obtain the usual joint probability distribution associated with the graph:

$$
\begin{aligned}
p(x|\theta) &amp;= \frac{1}{Z(\theta)} \prod_{C} \psi_C(x_C) &#92;
&amp;= \frac{1}{Z(\theta)} \prod_{C} \exp(\sum_{i \in I_C} \theta_i f_i(x_{C_i})) &#92;
&amp;= \frac{1}{Z(\theta)} \exp(\sum_C \sum_{i \in I_C} \theta_i f_i(x_{C_i})) 
\end{aligned}
$$

In many cases, we can simplify our representation by simply summing over all features associated with a given graph, irrespective of their association with maximal cliques:

$$ p(x| \theta) = \frac{1}{Z(\theta)} \exp(\sum_{i \in I} \theta_i f_i(x_{C_i}))$$

We see that the featural representation is nothing but the exponential family, and the "features" are the sufficient statistics.

注意，这种特征表示方法可以视为(离散变量?)无向图模型的一种通用表示，因为只要取 features 为 indicator functions that pick out the cells in the maximal cliques，即可以可以之前的无向图模型的分解表示.

<h3>19.1.1 Graph first or features first?</h3>

<blockquote>
  The relationship between exponential family models and graphical models is a very close one.If we use an exponential representation for the contribution of each individual feature, then the product of potential functions leads to an exponential family representation for the joint distribution associated with the graphical model. Alternatively, we can also represent an arbitrary exponential family model as a graphical model by connecting nodes that appear together as arguments to the features.
</blockquote>

<h2>19.2 Maximum entropy and maximum likelihood</h2>

下面的推导都是基于离散变量的情况，但是结果可以推广到连续变量的情况.

<h3>19.2.1 Maximum likelihood</h3>

$$
\begin{aligned}
l(\theta; D) &amp;= \sum_x m(x) \log p(x|\theta) &#92;
&amp;= \sum_x (\sum_i \theta_i f_i(x) - \log Z(\theta)) &#92;
&amp;= \sum_x m(x) \sum_i \theta_i f_i(x) - N \log Z(\theta)
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial l}{\partial \theta_i} &amp;= \sum_x m(x) f_i(x) - N \frac{\partial}{\partial \theta_i} \log Z(\theta) &#92;
&amp;= \sum_x m(x) f_i(x) - N \sum_x p(x | \theta) f_i(x)
\end{aligned}
$$

This yields the following characterization of maximum likelihood estimates:

$$ \sum_x p(x | \theta) f_i(x) = \sum_x \tilde{p}(x) f_i(x)$$

where $\tilde{p}(x) = \frac{m(x)}{N}$ is the empirical distribution. We see that the marginals of the sufficient statistics (the "features") must be equal to the empirical marginals.

注意：如果取 features 为 indicator functions that pick out the cells in the maximal cliques, 从上面的marginal of sufficient statistics 相等的公式，即可以得到 clique marginal 相等的公式 (即为 之前 Chapter 09 中，推导 无向图模型 MLE 参数估计的结果).

<h3>19.2.2 Maximum entropy</h3>

<strong>general form 的最大熵问题</strong>

$$
\begin{aligned}
\min&amp; \quad D(p || h) = \sum_x p(x) \log \frac{p(x)}{h(x)} &#92;
st.&amp; \quad \sum_x p(x)f_i(x) = \alpha_i &#92;
&amp; \quad \sum_x p(x) = 1
\end{aligned}
$$

注意:

<ol>
<li>When $h(x)$ is the uniform distribution we obtain the maximum entropy problem.</p></li>
<li><p>generanl form 当中无法保证能找到 $p(x)$ 满足 $\sum_x p(x)f_i(x) = \alpha_i$.</p></li>
</ol>

<p>使用 拉格朗日乘子法 求解上述优化问题：

$$ L = \sum_x p(x) \log \frac{p(x)}{h(x)} - \sum_i \theta_i (\sum_x p(x)f_i(x) - \alpha_i) - \mu (\sum_x p(x) - 1) $$

$$ \frac{\partial L}{\partial p(x)} = 1 + \log p(x) - \log h(x) - \sum_i \theta_i f_i(x) - \mu $$

$$ p(x) = \frac{1}{Z(\theta)} h(x) \exp(\sum_i \theta_i f_i(x)) $$

We have derived the exponential family distribution as an expression of the maximum entropy principle. The features $f_i(x)$ are the sufficient statistics, and the Lagrange multipliers $\theta_i$ are the canonical parameters.

<strong>特定形式的最大熵问题</strong>

$$
\begin{aligned}
\min&amp; \quad D(p || h) = \sum_x p(x) \log \frac{p(x)}{h(x)} &#92;
st.&amp; \quad \sum_x p(x)f_i(x) = \tilde{p}(x) f_i(x) &#92;
&amp; \quad \sum_x p(x) = 1
\end{aligned}
$$

<ol>
<li>可以看到可行解是一定存在的

empirical distribution 就是一个满足约束条件的可行解</p></li>
<li><p>Moreover, that solution must be unique.

The KL divergence is strictly convex in the variables $p(x)$. The problem of minimizing a strictly convex function with respect to linear constraints has at most a single solution.</p></li>
</ol>

<h3>19.2.3 Duality</h3>

<blockquote>
  <p>In one case (maximum likelihood) we assume the exponential family distribution and show that the model expectations must equal the empirical expectations. In the other case (maximum entropy) we assume that the model expectations must equal the empirical expectations and show that we must use an exponential family distribution.
  
  The relationship turns out to be one of duality: the maximum likelihood problem and the maximum entropy problem are Lagrangian duals.
</blockquote>

实际上, 对于上述提到的最大熵问题，求解其对偶问题，容易得到最大似然表达式.

<h4>Geometric interpretation</h4>

构造下面两种概率分布的集合：

<ul>
<li>the subset of all exponential family distributions based on a given set of features $f_i(x)$ and a given base measure h(x):</li>
</ul>

$$ \mathcal{E} = &#123; p(x): p(x) = \frac{1}{Z(\theta)} h(x) \exp(\sum_i \theta_i f_i(x)) &#125; $$

<ul>
<li>subset of all distributions that meet the moment constraints:
$$ \mathcal{M} = &#123; p(x): \sum_x p(x)f_i(x) = \sum_x \tilde{p}(x) f_i(x) &#125; $$
in general the distributions in $\mathcal{M}$ are not exponential family distributions</li>
</ul>

our results in the previous section have shown that there is a single distribution that is in $\mathcal{M}$ and is also in $\mathcal{E}$. We denote this distribution, the maximum entropy or maximum likelihood distribution, as $p(x | \theta_M )$, or $p_M$ for short.

<strong>Pythagorean theorem</strong>

there is an interesting "orthogonality" relationship between these subsets. Let $p$ be any other point in $\mathcal{E}$ (an exponential family distribution with parameters $\theta$), and let $q$ be any other point in $\mathcal{M}$. We now prove the following fact:

$$ D(q || p) = D(q || p_M ) + D(pM || p) $$

which can be viewed as a generalized "Pythagorean theorem".

可以有如下图的几何解释：

<img src="https://www.tvect.cn/wp-content/uploads/2020/01/pythagorean.png" alt="" />

<!--nextpage-->

<h1>Chapter 20. Iterative scaling algorithms</h1>

重新审视回顾了 decompoable models 中参数学习方法 和 IPF 算法, 同时介绍了 GIS 算法作为 IPF 算法的完全并行化推广.

<h2>20.1 Complete observations</h2>

<h3>20.1.1 Decomposable models</h3>

<strong>Algorithm</strong> Learning of an unconstrained decomposable Markov network using maximum likelihood.

<ol>
<li>Form a junction tree from the cliques.</p></li>
<li><p>Initialise each clique $\psi_C(x_C)$ to $\tilde{p}(x_C)$ and each separator $\phi_S(x_S)$ to $\tilde{p}(x_S)$.</p></li>
<li><p>Choose a root clique on the junction tree and orient edges consistently away from this root.</p></li>
<li><p>For this oriented junction tree, divide each clique by its parent separator. (or child?)</p></li>
<li><p>Return the new potentials on each clique as the maximum likelihood solution.</p></li>
</ol>

<h3>20.1.2 Iterative proportional fitting</h3>

<p>IPF 算法的更新公式为:

$$ \psi^{(t+1)}_C(x_C) =  \psi^{(t)}_C(x_C) \frac{\tilde{p}(x_C)}{p^{(t)}(x_C)} $$

或者可以用联合概率的形式表示为：

$$ p^{(t+1)}(x_V) = p^{(t)}(x_V) \frac{\tilde{p}(x_C)}{p^{(t)}(x_C)} $$

which implies $ p^{(t+1)}(x_V) = p^{(t)}(x_{V \setminus C} | x_C) \tilde{p}(x_C) $

<h4>IPF as a sequence of I-procjetions</h4>

考虑如下的最大熵问题：

$$
\begin{aligned}
\min&amp; \quad D(p(x) || p^{(t)}(x)) = \sum_x p(x) \log \frac{p(x)}{p^{(t)}(x)} &#92;
st.&amp; \quad p(x_C) = \tilde{p}(x_C) &#92;
&amp; \quad \sum_x p(x) = 1
\end{aligned}
$$

其中，$C$ 是一个固定的 clique, 所以上述限制条件对应的是 a set of constraints for all configurations in the clique $C$.

根据前一章的推导，可知 $ p(x) = \frac{1}{Z} p^{(t)}(x) \exp(\psi_C(x_C))$.

对上式两边在 $ V \setminus C $ 上求和，可以得到: $ p(x) = p^{(t)}(x) \frac{\tilde{p}(x_C)}{p^{(t)}(x_C)} $, 即为 IPF 更新公式.

所以，An IPF step is an I-projection of $p^{(t)}(x)$ on the set defined by a single marginal constraint.

<strong>几何解释</strong>

Let $\mathcal{M}<em>C$ denote the set corresponding to the marginal constraint $p(x_C) = \tilde{p}(x_C)$. The overall solution $p_M$ is a distribution in the set of distributions which satisfy all of these marginal constraints.
We can write this set as an intersection: $\mathcal{M} = \cup</em>{C} \mathcal{M}_C $.

The basic setting is the exponential family $\mathcal{E}$; all IPF iterates lie in this set.

At each step IPF projects onto one of the subsets $\mathcal{M}_C$ . Each of these projections is an I-projection. This sequence of projections converges to $p_M$ in the limit.

图示如下：

<img src="https://www.tvect.cn/wp-content/uploads/2020/01/IPF-geometry.png" alt="" />

<h3>20.1.3 Iterative scaling</h3>

考虑 potential function 使用一系列特征函数进行表示的情况：

$$ p(x | \theta) = \frac{1}{Z(\theta)} \exp (\sum_i \theta_i f_i(x)) $$

The <strong>generalized iterative scaling (GIS) algorithm</strong> is an iterative algorithm for finding maximum likelihood parameter estimates. GIS applies to arbitrary collections of features, subject to two constraints: the features must be nonnegative and they must sum to one:

$$ f_i(x) \ge 0 , \quad \sum_i f_i(x) = 1 $$

GIS 算法更新公式如下：

$$ \theta_i^{(t+1)} = \theta_i^{(t)} + \log (\frac{\sum_x \tilde{p}(x) f_i(x)}{\sum_x p^{(t)}(x) f_i(x)}) $$

即有:

$$ p^{(t+1)}(x) = p^{(t)}(x) \prod_i (\frac{\sum_x \tilde{p}(x) f_i(x)}{\sum_x p^{(t)}(x) f_i(x)})^{f_i(x)} $$

<strong>关于 IPF &amp; GIS 的一些对比</strong>

<ol>
<li>As in case of the IPF algorithm, GIS involves a ratio of expectations, in which the numerators are expectations with respect to the empirical distribution and the denominators are expectations with respect to the current model. These ratios are "update factors" that multiply the current distribution.</p></li>
<li><p>GIS is a fully "parallel" algorithm -- the product in the algorithm is a product over all features.

the IPF algorithm is parallel at the level of a single clique</p></li>
<li><p>If we parameterize the clique potential $\psi_C (x_C )$ using a set of indicator features $f_i(x_C)$, ie. for any given x, only one of these features are equal to one and the others are equal to zero.

这种情况下使用 GIS，得到的更新公式和 IPF 是一致的. 因此：</p></li>
</ol>

<blockquote>
  <p>Thus, each step of IPF -- which is parallel across the features corresponding to a single clique -- is a GIS update. On the other hand, a sequence of IPF iterations corresponds to a sequence of GIS iterations with a changing set of features at each iteration.
</blockquote>

<h1>参考资料</h1>

<ul>
<li><p><a href="">Jordan book: PGM Chapter 19. Features, maximum entropy, and duality</a></p></li>
<li><p><a href="">Jordan book: PGM Chapter 20. Iterative scaling algorithms</a></p></li>
</ul>