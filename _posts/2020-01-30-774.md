---
ID: 774
post_title: 小结-20200130
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/774
published: true
post_date: 2020-01-30 11:52:35
---
这部分内容整理来源于 Jordan Book：An Introduction to PGMs，Chapter19. Features, maximum entropy, and duality

<h1>Chapter 19. Features, maximum entropy, and duality</h1>

展示了如何使用 collections of "features" 来表示 potential functions. 最后说明了最大熵问题和最大似然问题是拉格朗日对偶的.

<h2>19.1 Features</h2>

In the general case, we parameterize a clique $C$ as follows.

Define a set of features $f_i(x_{C_i})$, where $C_i \subseteq C$ is the subset of variables that feature $f_i$ references. Associated with each feature there is a scalar parameter $\theta_i$.

Given the features and the parameters we define the clique potential $\psi_C(x_C)$ as follows:

$$
\begin{aligned}
\psi_C(x_C) &amp;= \prod_{i \in I_C} \exp (\theta_i f_i(x_{C_i})) &#92;
&amp;= \exp(\sum_{i \in I_C} \theta_i f_i(x_{C_i}))
\end{aligned}
$$

Taking the product over clique potentials and normalizing, we obtain the usual joint probability distribution associated with the graph:

$$
\begin{aligned}
p(x|\theta) &amp;= \frac{1}{Z(\theta)} \prod_{C} \psi_C(x_C) &#92;
&amp;= \frac{1}{Z(\theta)} \prod_{C} \exp(\sum_{i \in I_C} \theta_i f_i(x_{C_i})) &#92;
&amp;= \frac{1}{Z(\theta)} \exp(\sum_C \sum_{i \in I_C} \theta_i f_i(x_{C_i})) 
\end{aligned}
$$

In many cases, we can simplify our representation by simply summing over all features associated with a given graph, irrespective of their association with maximal cliques:

$$ p(x| \theta) = \frac{1}{Z(\theta)} \exp(\sum_{i \in I} \theta_i f_i(x_{C_i}))$$

We see that the featural representation is nothing but the exponential family, and the "features" are the sufficient statistics.

注意，这种特征表示方法可以视为(离散变量?)无向图模型的一种通用表示，因为只要取 features 为 indicator functions that pick out the cells in the maximal cliques，即可以可以之前的无向图模型的分解表示.

<h3>19.1.1 Graph first or features first?</h3>

<blockquote>
  The relationship between exponential family models and graphical models is a very close one.If we use an exponential representation for the contribution of each individual feature, then the product of potential functions leads to an exponential family representation for the joint distribution associated with the graphical model. Alternatively, we can also represent an arbitrary exponential family model as a graphical model by connecting nodes that appear together as arguments to the features.
</blockquote>

<h2>19.2 Maximum entropy and maximum likelihood</h2>

下面的推导都是基于离散变量的情况，但是结果可以推广到连续变量的情况.

<h3>19.2.1 Maximum likelihood</h3>

$$
\begin{aligned}
l(\theta; D) &amp;= \sum_x m(x) \log p(x|\theta) &#92;
&amp;= \sum_x (\sum_i \theta_i f_i(x) - \log Z(\theta)) &#92;
&amp;= \sum_x m(x) \sum_i \theta_i f_i(x) - N \log Z(\theta)
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial l}{\partial \theta_i} &amp;= \sum_x m(x) f_i(x) - N \frac{\partial}{\partial \theta_i} \log Z(\theta) &#92;
&amp;= \sum_x m(x) f_i(x) - N \sum_x p(x | \theta) f_i(x)
\end{aligned}
$$

This yields the following characterization of maximum likelihood estimates:

$$ \sum_x p(x | \theta) f_i(x) = \sum_x \tilde{p}(x) f_i(x)$$

where $\tilde{p}(x) = \frac{m(x)}{N}$ is the empirical distribution. We see that the marginals of the sufficient statistics (the "features") must be equal to the empirical marginals.

注意：如果取 features 为 indicator functions that pick out the cells in the maximal cliques, 从上面的marginal of sufficient statistics 相等的公式，即可以得到 clique marginal 相等的公式 (即为 之前 Chapter 09 中，推导 无向图模型 MLE 参数估计的结果).

<h3>19.2.2 Maximum entropy</h3>

<strong>general form 的最大熵问题</strong>

$$
\begin{aligned}
\min&amp; \quad D(p || h) = \sum_x p(x) \log \frac{p(x)}{h(x)} &#92;
st.&amp; \quad \sum_x p(x)f_i(x) = \alpha_i &#92;
&amp; \quad \sum_x p(x) = 1
\end{aligned}
$$

注意:

<ol>
<li>When $h(x)$ is the uniform distribution we obtain the maximum entropy problem.</p></li>
<li><p>generanl form 当中无法保证能找到 $p(x)$ 满足 $\sum_x p(x)f_i(x) = \alpha_i$.</p></li>
</ol>

<p>使用 拉格朗日乘子法 求解上述优化问题：

$$ L = \sum_x p(x) \log \frac{p(x)}{h(x)} - \sum_i \theta_i (\sum_x p(x)f_i(x) - \alpha_i) - \mu (\sum_x p(x) - 1) $$

$$ \frac{\partial L}{\partial p(x)} = 1 + \log p(x) - \log h(x) - \sum_i \theta_i f_i(x) - \mu $$

$$ p(x) = \frac{1}{Z(\theta)} h(x) \exp(\sum_i \theta_i f_i(x)) $$

We have derived the exponential family distribution as an expression of the maximum entropy principle. The features $f_i(x)$ are the sufficient statistics, and the Lagrange multipliers $\theta_i$ are the canonical parameters.

<strong>特定形式的最大熵问题</strong>

$$
\begin{aligned}
\min&amp; \quad D(p || h) = \sum_x p(x) \log \frac{p(x)}{h(x)} &#92;
st.&amp; \quad \sum_x p(x)f_i(x) = \tilde{p}(x) f_i(x) &#92;
&amp; \quad \sum_x p(x) = 1
\end{aligned}
$$

<ol>
<li>可以看到可行解是一定存在的

empirical distribution 就是一个满足约束条件的可行解</p></li>
<li><p>Moreover, that solution must be unique.

The KL divergence is strictly convex in the variables $p(x)$. The problem of minimizing a strictly convex function with respect to linear constraints has at most a single solution.</p></li>
</ol>

<h3>19.2.3 Duality</h3>

<blockquote>
  <p>In one case (maximum likelihood) we assume the exponential family distribution and show that the model expectations must equal the empirical expectations. In the other case (maximum entropy) we assume that the model expectations must equal the empirical expectations and show that we must use an exponential family distribution.
  
  The relationship turns out to be one of duality: the maximum likelihood problem and the maximum entropy problem are Lagrangian duals.
</blockquote>

实际上, 对于上述提到的最大熵问题，求解其对偶问题，容易得到最大似然表达式.

<h4>Geometric interpretation</h4>

构造下面两种概率分布的集合：

<ul>
<li>the subset of all exponential family distributions based on a given set of features $f_i(x)$ and a given base measure h(x):</li>
</ul>

$$ \mathcal{E} = &#123; p(x): p(x) = \frac{1}{Z(\theta)} h(x) \exp(\sum_i \theta_i f_i(x)) &#125; $$

<ul>
<li>subset of all distributions that meet the moment constraints:
$$ \mathcal{M} = &#123; p(x): \sum_x p(x)f_i(x) = \sum_x \tilde{p}(x) f_i(x) &#125; $$
in general the distributions in $\mathcal{M}$ are not exponential family distributions</li>
</ul>

our results in the previous section have shown that there is a single distribution that is in $\mathcal{M}$ and is also in $\mathcal{E}$. We denote this distribution, the maximum entropy or maximum likelihood distribution, as $p(x | \theta_M )$, or $p_M$ for short.

<strong>Pythagorean theorem</strong>

there is an interesting "orthogonality" relationship between these subsets. Let $p$ be any other point in $\mathcal{E}$ (an exponential family distribution with parameters $\theta$), and let $q$ be any other point in $\mathcal{M}$. We now prove the following fact:

$$ D(q || p) = D(q || p_M ) + D(pM || p) $$

which can be viewed as a generalized "Pythagorean theorem".

可以有如下图的几何解释：

<h1>参考资料</h1>

<ul>
<li><a href="">Jordan book: PGM Chapter 19. Features, maximum entropy, and duality</a></li>
</ul>