---
ID: 537
post_title: >
  用深度学习做关系抽取文献整理-pipeline类方法
author: Chin
post_excerpt: ""
layout: post
permalink: 'http://www.tvect.cc/2018/07/13/%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%81%9a%e5%85%b3%e7%b3%bb%e6%8a%bd%e5%8f%96%e6%96%87%e7%8c%ae%e6%95%b4%e7%90%86-pipeline%e7%b1%bb%e6%96%b9%e6%b3%95/'
published: true
post_date: 2018-07-13 10:44:11
---
[toc]

<h1>概述</h1>

这里主要会整理一些用深度学习做关系抽取的方法,下面是一些pipeline类的方法.

<h1>基本方法</h1>

<h2><a href="https://arxiv.org/pdf/1504.06580.pdf">CR-CNN: Classifying Relations by Ranking with CNN</a></h2>

<ul>
<li>使用margin based的ranking-loss做损失函数, 取代了 softmax 分类器损失函数.    结果比 CNN + softmax classifier 要好.</li>
<li>省略了人造关系类别 Other 的表示.
precision and recall都有所提升</li>
</ul>

<strong>在SemEval-2010 Task 8 dataset上评测结果</strong>：

<blockquote>
  outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features
</blockquote>

<h3>模型结构图</h3>

<img src="http://www.tvect.cc/wp-content/uploads/2018/07/cr-cnn-structure.png" alt="" />

<h3>模型细节</h3>

<ul>
<li><strong>input layer</strong>：Word Embeddings + Word Position Embeddings</li>
</ul>

Word Embeddings: $$r^{w} = W^{wrd} v^{w}$$

Word Position Embeddings: 使用 Position Features, 考虑当前词相对两个实体的相对距离, 每个相对距离都会映射为一个随机初始化的向量. $$wpe^{w} = [wp_{1}, wp_{2}]$$.

input 层: 由 word embeddings 和 word position embeddings 拼接得到, $$emb_{x} = {[r^{w1}, wpe^{w1}], [r^{w2}, wpe^{w2}], ...}$$.

<ul>
<li><strong>sentence representation</strong></li>
</ul>

在前面一步的 $$emb_{x}$$ 上做卷积, 得到 size 为 [seq_length, dc] 的输出, 在每一个维度上求最大值，得到句子的表示向量$$r_{x}$$, 维度为 dc.

上面的求最大值的操作也可以视为 使用 size 为 [seq_length, 1] 的 max pooling 操作.

<ul>
<li><strong>Class embeddings and Scoring</strong></li>
</ul>

经过前面一步已经得到了句子的表示 $$r_{x}$$. 这里引入一个 class 的 embedding matrix $$W^{classes} \\in R^{dc * nc}$$, 其中 $$dc$$ 等于前面的句子表示的维度大小, $$nc$$ 等于关系类别的个数.

通过内积计算评分: $$s_{\\theta}(x, c) = r_{x} [W^{classes}]_{c}$$

<ul>
<li><strong>Training Procedure</strong></li>
</ul>

训练输入为 一个sentence $$x$$， 一个正确的关系类别标签 $$y^{+} \\in C$$, 一个错误的类别标签 $$c^{-} \\in C$$. 计算 sentence 与 这两个标签相对应的得分, 构造损失函数如下:

$$
L = log(1 + exp(\\gamma (m^{+} - s_{\\theta}(x, y^{+})))) + log(1 + exp(\\gamma (m^{-} + s_{\\theta}(x, c^{-}))))
$$

经过训练之后，希望的是正确 label 的得分大于 $$m^{+}$$, 错误 label 的得分小于 $$-m^{-}$$.

另外，在负类别 $$c^{-}$$ 的选择上，文章中选择了所有负类别中，得分最高的那一个类别. 即：$$c^{-} = arg \\max \\limits_{c \\neq y^{+}} s_{\\theta}(x, c)$$.

当涉及到的类别数量很庞大的时候，一种替代的方案是考虑固定数量的负类别，从这些负类别中选取得分最高的.

<ul>
<li><strong>Special Treatment of Artificial Classes</strong></li>
</ul>

一般的方法中会有一个 Other 类，表示不属于任意事先指定的关系类别. 文章中认为这个类别比较noisy, 因为它把很多没有什么共同性的关系放一起了。

文章中省略了Other类，相应的对训练和测试时候的Other类做了特殊处理。
在训练的时候, 遇到 Other 类别时，直接设置loss的第一项为0.
在预测时, 若最大Score为负的，表明为Other类，否则取最大Score对应的类别.

<strong>问题</strong>
怎么区分不同direction的关系，即R(e1,e2)和R(e2,e1)??? 根据这两种情况下构造的 position embedding 不同???

<h2><a href="http://www.aclweb.org/anthology/P/P16/P16-2034.pdf">Attention-Based BiLSTM for Relation Classification</a></h2>

word embedding + position indicator作为输入，使用bilstm + attention得到sentence的表示，用softmax cross entropy做损失函数

<strong>在SemEval-2010 Task 8 dataset上评测结果</strong>：

<blockquote>
  achieve an F 1-score of 84.0%, higher than most of the existing methods in the literature.
</blockquote>

<h3>模型结构图</h3>

<img src="http://www.tvect.cc/wp-content/uploads/2018/07/att-bilstm.png" alt="" />

<ul>
<li><strong>输入层</strong></li>
</ul>

使用 word embedding.

注意的是，这里使用了 <strong>Position Indicators</strong>， 而不是常用的 Position Features. 将&lt;s1&#62;, &lt;\s1>, &lt;s2&#62;, &lt;\s2> 视为四个单独的word, 而不是像前面一样为每个词添加距离特征向量.

<ul>
<li><strong>LSTM层 和 Attention层</strong></li>
</ul>

输入层得到的向量表示经过LSTM层处理，得到一系列向量，再使用Attention得到句子级别的表示.

具体的 Attention 的做法，类似文章 《Hierarchical Attention Networks for Document Classification》，另外引入的一个待训练的参数向量 w，可以把 w 想象为 a fixed query: "what is the informative word".

$$
\\begin{aligned}
M &= tanh(H) \\\\
\\alpha &= softmax(w^{T}M) \\\\
r &= H \\alpha^{T}
\\end{aligned}
$$

<ul>
<li><strong>输出层</strong></li>
</ul>

计算 Softmax Cross entropy.

<h2><a href="http://www.thunlp.org/~lzy/publications/acl2016_cnnatt.pdf">Relation Classification via Multi-Level Attention CNNs</a></h2>

word embedding + position embedding 作为初始的 input
求每一个entity与每个word的attention得分，再由word的两个attention得分得到新的word 表示
对前面的输出R, 使用att pooling取代max pooling:引入一个weighting matrix U,求出一个 word<em>label 的 att matrix Ap.最终的输出向量为wo=max(R</em>Ap).
使用的损失函数为: margin-based pairwise loss

<h3><strong>模型结构图</strong></h3>

<img src="http://www.tvect.cc/wp-content/uploads/2018/07/mutli-att-cnn.png" alt="" />

<h1>参考资料</h1>

<ul>
<li>Pipeline 方法

<ul>
<li><p><a href="https://arxiv.org/pdf/1504.06580.pdf">Classifying Relations by Ranking with CNN</a></p></li>
<li><p><a href="http://www.aclweb.org/anthology/P/P16/P16-2034.pdf">Attention-Based BiLSTM for Relation Classification</a></p></li>
<li><p><a href="http://www.thunlp.org/~lzy/publications/acl2016_cnnatt.pdf">Relation Classification via Multi-Level Attention CNNs</a></p></li>
</ul></li>
</ul>