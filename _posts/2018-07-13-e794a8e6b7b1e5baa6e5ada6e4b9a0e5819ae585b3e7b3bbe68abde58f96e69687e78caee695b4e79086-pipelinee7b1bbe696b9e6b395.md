---
ID: 537
post_title: >
  用深度学习做关系抽取文献整理-pipeline类方法
author: Chin
post_excerpt: ""
layout: post
permalink: http://blog.tvect.cc/archives/537
published: true
post_date: 2018-07-13 10:44:11
---
[toc]

这里主要会整理一些用深度学习做关系抽取的方法,下面是一些pipeline类的方法.

<!--more-->

<hr />

<h1>基本方法</h1>

<h2><a href="https://arxiv.org/pdf/1504.06580.pdf">CR-CNN: Classifying Relations by Ranking with CNN</a></h2>

<ul>
<li>使用ranking loss function做损失函数, 取代了 softmax 分类器损失函数.    结果比 CNN + softmax classifier 要好.</li>
<li>省略了人造关系类别 Other 的表示.
precision and recall都有所提升</li>
</ul>

<strong>在SemEval-2010 Task 8 dataset上评测结果</strong>：

<blockquote>
  outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features
</blockquote>

<h3>模型结构图</h3>

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/cr-cnn-structure.png" alt="" />

<h3>模型细节</h3>

<ul>
<li><strong>input layer</strong>：Word Embeddings + Word Position Embeddings</li>
</ul>

Word Embeddings: $r^{w} = W^{wrd} v^{w}$

Word Position Embeddings: 使用 Position Features, 考虑当前词相对两个实体的相对距离, 每个相对距离都会映射为一个随机初始化的向量. $wpe^{w} = [wp_{1}, wp_{2}]$.

input 层: 由 word embeddings 和 word position embeddings 拼接得到, $emb_{x} = {[r^{w1}, wpe^{w1}], [r^{w2}, wpe^{w2}], ...}$.

<ul>
<li><strong>sentence representation</strong></li>
</ul>

在前面一步的 $emb_{x}$ 上做卷积, 得到 size 为 [seq_length, dc] 的输出, 在每一个维度上求最大值，得到句子的表示向量 $r_{x}$, 维度为 dc.

上面的求最大值的操作也可以视为 使用 size 为 [seq_length, 1] 的 max pooling 操作.

<ul>
<li><strong>Class embeddings and Scoring</strong></li>
</ul>

经过前面一步已经得到了句子的表示 $r_{x}$. 这里引入一个 class 的 embedding matrix $W^{classes} \in R^{dc * nc}$, 其中 $dc$ 等于前面的句子表示的维度大小, $nc$ 等于关系类别的个数.

通过内积计算评分: $s_{\theta}(x, c) = r_{x} [W^{classes}]_{c}$

<ul>
<li><strong>Training Procedure</strong></li>
</ul>

训练输入为 一个sentence $x$， 一个正确的关系类别标签 $y^{+} \in C$, 一个错误的类别标签 $c^{-} \in C$. 计算 sentence 与 这两个标签相对应的得分, 构造损失函数如下:

$$
L = log(1 + exp(\gamma (m^{+} - s_{\theta}(x, y^{+})))) + log(1 + exp(\gamma (m^{-} + s_{\theta}(x, c^{-}))))
$$

经过训练之后，希望的是正确 label 的得分大于 $m^{+}$, 错误 label 的得分小于 $-m^{-}$.

另外，在负类别 $c^{-}$ 的选择上，文章中选择了所有负类别中，得分最高的那一个类别. 即：$c^{-} = \arg \max \limits_{c \neq y^{+}} s_{\theta}(x, c)$.

当涉及到的类别数量很庞大的时候，一种替代的方案是考虑固定数量的负类别，从这些负类别中选取得分最高的.

<ul>
<li><strong>Special Treatment of Artificial Classes</strong></li>
</ul>

一般的方法中会有一个 Other 类，表示不属于任意事先指定的关系类别. 文章中认为这个类别比较noisy, 因为它把很多没有什么共同性的关系放一起了。

文章中省略了Other类，相应的对训练和测试时候的Other类做了特殊处理。
在训练的时候, 遇到 Other 类别时，直接设置loss的第一项为0.
在预测时, 若最大Score为负的，表明为Other类，否则取最大Score对应的类别.

<strong>问题</strong>
怎么区分不同direction的关系，即R(e1,e2)和R(e2,e1)??? 根据这两种情况下构造的 position embedding 不同???

<hr />

<h2><a href="http://www.aclweb.org/anthology/P/P16/P16-2034.pdf">Attention-Based BiLSTM for Relation Classification</a></h2>

word embedding + position indicator作为输入，使用bilstm + attention得到sentence的表示，用softmax cross entropy做损失函数

<strong>在SemEval-2010 Task 8 dataset上评测结果</strong>：

<blockquote>
  achieve an F 1-score of 84.0%, higher than most of the existing methods in the literature.
</blockquote>

<h3>模型结构图</h3>

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/att-bilstm.png" alt="" />

<ul>
<li><strong>输入层</strong></li>
</ul>

使用 word embedding.

注意的是，这里使用了 <strong>Position Indicators</strong>， 而不是常用的 Position Features. 将&lt;s1&#62;, &lt;\s1>, &lt;s2&#62;, &lt;\s2> 视为四个单独的word, 而不是像前面一样为每个词添加距离特征向量.

<ul>
<li><strong>LSTM层 和 Attention层</strong></li>
</ul>

输入层得到的向量表示经过LSTM层处理，得到一系列向量，再使用Attention得到句子级别的表示.

具体的 Attention 的做法，类似文章 《Hierarchical Attention Networks for Document Classification》，另外引入的一个待训练的参数向量 w，可以把 w 想象为 a fixed query: "what is the informative word".

$$
\begin{aligned}
M &amp;= tanh(H) &#92;
\alpha &amp;= softmax(w^{T}M) &#92;
r &amp;= H \alpha^{T}
\end{aligned}
$$

<ul>
<li><strong>输出层</strong></li>
</ul>

计算 Softmax Cross entropy.

<hr />

<h2><a href="http://www.thunlp.org/~lzy/publications/acl2016_cnnatt.pdf">Relation Classification via Multi-Level Attention CNNs</a></h2>

<strong>基本思路</strong>

word embedding + position embedding 作为初始的 input

求每一个entity与每个word的attention得分，再由word的两个attention得分得到新的word 表示

对前面的输出R, 做卷积，再使用att pooling取代max pooling:引入一个weighting matrix U,求出一个 word<em>label 的 att matrix Ap.最终的输出向量为 wo = max(R</em>Ap).

使用的损失函数为: margin-based pairwise loss

<strong>在SemEval-2010 Task 8 dataset上评测结果</strong>：

<blockquote>
  obtain the new state-of-the-art results for relation classification with an F1 score of 88.0% on the SemEval 2010 Task 8 dataset, outperforming methods relying on significantly richer prior knowledge
</blockquote>

<h3><strong>模型结构图</strong></h3>

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/mutli-att-cnn.png" alt="" />

<h3>模型细节</h3>

<ul>
<li><strong>Input Representation</strong></li>
</ul>

同前面的 Word Embeddings + Word Position Embeddings, 并使用大小为 k 的上下文滑动窗口, 得到位置 i 处的向量表示 $z_i$.

<ul>
<li><strong>Input Attention Mechanism</strong></li>
</ul>

在前面的 position-based encoding 的基础上, 做 attention, 捕获 word 和 entity 之间的影响.

构造 Contextual Relevance Matrices $A^{j}$, 其中 $A^{j}<em>{i, i} = f(e</em>{j}, w_{i})$, 刻画了 word 和 entity 之间的联系和相关性, 评分函数 $f$ 可以是内积计算.

$$
\alpha_{i}^{j} = \frac{exp(A^{j}<em>{i,i})}{\sum</em>{i'=1}^{n}exp(A^{j}_{i',i'})}
$$

基于上面求出的 $\alpha_{i}^{1}, \alpha_{i}^{2}$, 可以尝试一下几种变形来刻画相关性影响.

<ol>
<li>input attention variant-0: $r_{i} = z_{i} * \frac{\alpha_{i}^{1} + \alpha_{i}^{2}}{2} $</p></li>
<li><p>input attention variant-1: $ r_{i} = [z_{i}\alpha_{i}^{1}, z_{i}\alpha_{i}^{2}]$</p></li>
<li><p>input attention variant-2: $r_{i} = z_{i} * \frac{\alpha_{i}^{1} - \alpha_{i}^{2}}{2}$</p></li>
</ol>

<p>基于上面的 $r_{i}$, 这一层最终输出为一个矩阵: $ R = [r_{1}, r_{2}, ..., r_{n}]$, 其中 n 为句子的长度.

<ul>
<li><strong>Convolutional Max-Pooling with Secondary Attention</strong></li>
</ul>

首先基于前面得到的输出 $R$, 做卷积, 得到另外一个矩阵 $R^{*} \in R^{dc * length}$.

接下来,执行 attention-based pooling 操作:

$$
\begin{aligned}
G &amp;= R^{\ast T} U W^{L} &#92;
A^p_{i,j} &amp;= \frac {exp(G_{i, j})} {\sum_{i'=1}^{n} exp(G_{i', j})} &#92;
W^{O}<em>{i} &amp;= \max \limits</em>{j} (R^\ast A^p)_{i,j}
\end{aligned}
$$

其中 $U$ 是一个要学习的权重矩阵, $W^{L}$ 是 relation class embedding matrix.
整个 attention-based pooling的流程是先计算一个 correlation modeling matrix $G$, 对 $G$ 做 softmax, 得到 attention pooling matrix $A^{p}$, 将 $R^{*}$ 和 $A^{p}$ 相乘, 可以高亮句子里面的某些重要的成分,紧接着再使用max pooling,选出其中最显著的部分.

<ul>
<li><strong>Training Procedure</strong></li>
</ul>

使用了一个新颖的距离度量函数: $\delta_{\theta}(S, y) = || \frac{w^{O}}{|W^{O}|} - W^{L}_{y}|$.

定义了一个magin-based pairwise loss function: $L = [\delta_{\theta}(S, y) + (1 - \delta_{\theta}(S, y^{-}))] + \beta ||\theta||^{2}$

其中的负标签 $y^{-}$ 是选自于所有不正确标签中得分最高的一个，即 $y^{-} = \arg \max \limits_{y' \neq y} \delta(S, y')$

<hr />

<h1>参考资料</h1>

<ul>
<li><p><a href="https://blog.csdn.net/yimixgg/article/details/80509825">博客：关系抽取总结</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1504.06580.pdf">Classifying Relations by Ranking with CNN</a></p></li>
<li><p><a href="http://www.aclweb.org/anthology/P/P16/P16-2034.pdf">Attention-Based BiLSTM for Relation Classification</a></p></li>
<li><p><a href="http://www.thunlp.org/~lzy/publications/acl2016_cnnatt.pdf">Relation Classification via Multi-Level Attention CNNs</a></p></li>
</ul>