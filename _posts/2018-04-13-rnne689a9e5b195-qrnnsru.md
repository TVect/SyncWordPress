---
ID: 25
post_title: 'RNN扩展-QRNN&#038;SRU'
author: chin340823
post_excerpt: ""
layout: post
permalink: https://blog.tvect.cn/?p=25
published: true
post_date: 2018-04-13 20:35:11
---
[toc]

<!--more-->

<hr />

<h1>概述</h1>

传统的RNN，包括LSTMs，很难处理长序列的任务，因为特征或者是状态的计算很难并行化。

CNN也可以用于序列任务上。相比于RNN，CNN使用了 time-invariant filter functions，可以沿着整个序列做并行化，能够更好的扩展到长序列上。但是通常的 max- 和 average-pooling 方法很难利用句子中词的顺序信息。

文章提出了QRNN，克服了上面这些模型的缺点。一方面，可以像 CNN 一样做一些并行化操作。另一方面，可以像 RNN 一样，使得模型的输出依赖于句子中词的顺序。

文章对 document-level sentiment classification，language modeling，character-level machine translation 等任务做了一些 QRNN 的定制化，结果超过了 LSTM 的 baseline，同时训练时间显著减少。

<h1>模型</h1>

<h2>convolution subcomponent</h2>

<img src="http://blog.tvect.cc/wp-content/uploads/2018/04/qrnn-01.png" alt="" />

这样的convolution操作可以捕获 n-gram 的特征。

<h2>pooling subcomponent</h2>

文章旨在需要一个 <strong>a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector。</strong>

基于上述想法，文中提出了三种 pooling 操作。
<img src="http://blog.tvect.cc/wp-content/uploads/2018/04/qrnn-02-300x168.png" alt="" />

pooling component 有时序依赖性，但是计算简单，并且可以特征维度方向并行化，evaluating them over even long sequences requires a negligible amount of computation time.

<h1>一些变体</h1>

考虑到具体的nlp任务，文章提出了一些 stacked QRNN 的扩展。

<h2>Regularization</h2>

LSTM 需要做正则化，但是普通的 dropout 应用到这样的循环连接上会 lack of efficacy。所以促使了一些 recurrent dropout schemes 的发展，包括variational inference–based dropout，zoneout。

文中采用了Zoneout的方式，即 keep the previous pooling state for a stochastic subset of channels。

<h2>Densely-Connected Layers</h2>

在每两个 layer 都加上连接，而不仅仅是在相邻的两层之间有连接。

This can improve gradient flow and convergence properties, especially in deeper networks, although it requires a parameter count that is quadratic in the number of layers.

<h2>Encoder–Decoder Models</h2>

为了展示 QRNN 的通用性，文中把模型结构扩展到了 seq2seq 的任务上，使用QRNN作为encoder，使用修改的 QRNN 和 attention 机制 作为decoder。

decoder layer 的每一个 convolution layer 的输出在输入到 pooling layer 之前都会加上相应的encoder hidden state。

最后，使用了 decoder layer 的最后一个 convolution layer 的输出计算了attention，进一步计算 encoder state 的 attention 加权和，用于最后的 output。

<img src="http://blog.tvect.cc/wp-content/uploads/2018/04/qrnn-seq2seq.png" alt="" />

<h1>Simple Recurrent Units</h1>

<img src="http://blog.tvect.cc/wp-content/uploads/2018/04/sru-01.png" alt="" />

完整的公式如下：

<img src="http://blog.tvect.cc/wp-content/uploads/2018/04/sru-02-300x148.png" alt="" />

从上面的公式可以看到，SRU 某种程度上可以看做是 QRNN 在 k=1 时候的特殊情况

<h1>参考资料</h1>

<a href="https://arxiv.org/abs/1611.01576" title="Quai-Recurrent Neural Networks">Quai-Recurrent Neural Networks</a>

<a href="https://arxiv.org/abs/1709.02755" title="Training RNNs as Fast as CNNs">Training RNNs as Fast as CNNs</a>