---
ID: 41
post_title: EM Algorithm 整理
author: chin340823
post_excerpt: ""
layout: post
permalink: https://blog.tvect.cn/?p=41
published: true
post_date: 2018-06-25 21:56:41
---
[toc]

<!--more-->

<hr />

<h1>The General EM Algorithm</h1>

<h2>算法概述</h2>

给定观测变量 $$X$$, 和隐变量 $$Z$$ 上的联合分布 $$p(X, Z|&#92;theta)$$, $$&#92;theta$$是参数. 目标是要关于$$&#92;theta$$极大化似然函数$$p(X|&#92;theta)$$

<ol>
<li>为参数 $$&#92;theta^{old}$$ 赋一个初始值.</li>
<li><strong>E step</strong>
计算 $$p(Z|X, &#92;theta^{old})$$.</li>
<li><strong>M step</strong>
计算 $$&#92;theta^{new}$$: $$ &#92;theta^{new} = arg &#92;max &#92;limits_{&#92;theta} Q(&#92;theta, &#92;theta^{old})$$
其中 $$Q(&#92;theta, &#92;theta^{old}) = &#92;sum &#92;limits_{Z} p(Z|X, &#92;theta^{old}) &#92;, ln&#92;,p(X, Z|&#92;theta)$$</li>
<li>检查对数似然函数或者参数值的收敛性。
如果不满足收敛准则，那么令 $$ &#92;theta^{new} &#92;leftarrow &#92;theta^{old}$$.然后，回到第2步，继续.</li>
</ol>

<h2>算法有效性/收敛性证明</h2>

<h3>证明1</h3>

要证明EM算法的有效性/收敛性, 只需要证明每一次迭代可以保证 $$log&#92;,p(X|&#92;theta^{i+1}) = L(X|&#92;theta^{i+1}) &#92;ge L(X|&#92;theta^{i})$$

<strong>证明如下：</strong>

$$L(X|&#92;theta) = log&#92;,p(X|&#92;theta) = log &#92;frac {p(X, Z|&#92;theta)}{p(Z|X, &#92;theta)} = log &#92;, p(X, Z|&#92;theta) - log &#92;, p(Z|X, &#92;theta)$$

等式两边同时关于$$p(Z|X, &#92;theta^{old})$$做积分有：

$$
&#92;begin{aligned}
log &#92;, p(X|&#92;theta) &amp; = &#92;int p(Z|X, &#92;theta^{old})log&#92;,p(X|&#92;theta) &#92;,dZ&#92;&#92;
&amp; = &#92;int p(Z|X, &#92;theta^{old}) log&#92;,p(X, Z|&#92;theta) &#92;, dZ - p(Z|X, &#92;theta^{old}) log&#92;,p(Z|X, &#92;theta) &#92;, dZ &#92;&#92;
&amp; = Q(&#92;theta, &#92;theta^{old}) - &#92;int p(Z|X, &#92;theta^{old}) log&#92;,p(Z|X, &#92;theta) &#92;, dZ
&#92;end{aligned}
$$

下面比较 $$ L(&#92;theta^{new})$$ 和 $$L(&#92;theta^{old})$$, 有:
$$
L(&#92;theta^{new}) - L(&#92;theta^{old}) = Q(&#92;theta^{new}, &#92;theta^{old}) - Q(&#92;theta^{old}, &#92;theta^{old}) + &#92;int p(Z|X,&#92;theta^{old}) log&#92;frac{p(Z|X, &#92;theta^{old})}{p(Z|X, &#92;theta^{new})} &#92;,dZ
$$

根据 $$&#92;theta^{new}$$的定义, 可以知道 $$Q(&#92;theta^{new}, &#92;theta^{old}) - Q(&#92;theta^{old}, &#92;theta^{old}) &#92;ge 0$$

根据 KL 距离的非负性, 可以知道 $$&#92;int p(Z|X,&#92;theta^{old}) log&#92;frac{p(Z|X, &#92;theta^{old})}{p(Z|X, &#92;theta^{new})} &#92;,dZ = KL(p(Z|X, &#92;theta^{old}), p(Z|X, &#92;theta^{new})) &#92;ge 0$$

综上,有 $$L(&#92;theta^{new}) - L(&#92;theta^{old}) &#92;ge 0$$, 收敛性得证.

<h3>证明2</h3>

与前一种证明类似，可以看到:
$$L(X|&#92;theta) = log&#92;,p(X|&#92;theta) = log &#92;frac {p(X, Z|&#92;theta)}{p(Z|X, &#92;theta)} = log &#92;, &#92;frac {p(X, Z|&#92;theta)}{q(Z)} + log &#92;, &#92;frac{q(Z)}{p(Z|X, &#92;theta)}$$

其中 $$q(Z)$$ 是引入的一个关于 $$Z$$ 的分布. 两边同时关于 $$q(Z)$$ 求积分, 得到:

$$
&#92;begin{aligned}
log &#92;, p(X|&#92;theta) &amp; = &#92;int q(Z) &#92;, log&#92;,p(X|&#92;theta) &#92;,dZ&#92;&#92;
&amp; = &#92;int q(Z) log&#92;,&#92;frac {p(X, Z|&#92;theta)}{q(Z)} + q(Z) log&#92;, &#92;frac {q(Z)} {p(Z|X, &#92;theta)} &#92;, dZ &#92;&#92;
&amp; = &#92;int q(Z) log&#92;,&#92;frac {p(X, Z|&#92;theta)}{q(Z)} &#92;, d(Z) + KL(q(Z), p(Z|X, &#92;theta)) &#92;&#92;
&amp; = F(q, &#92;theta) + KL(q(Z), p(Z|X, &#92;theta))
&#92;end{aligned}
$$

EM 算法可以视为 通过优化 $$L(X|&#92;theta)$$ 的下界 $$F(q, &#92;theta)$$, 来达到优化 $$L(X|&#92;theta)$$ 的目的.

<ul>
<li>在 <strong>E-step</strong>: 固定 $$F(q, &#92;theta)$$ 中的 $$&#92;theta$$, 关于 $$q(Z)$$ 做优化.
因为 $$L(X|&#92;theta)$$ 是 $$F(q, &#92;theta)$$ 的上界, 且与 $$q(Z)$$ 无关, 容易看到, 在 $$q(Z) = p(Z|X, &#92;theta)$$时, $$F(q, &#92;theta)$$ 会取得最大值，即为 $$L(X|&#92;theta)$$.</p></li>
<li><p>在 <strong>M-step</strong>: 固定 $$F(q, &#92;theta)$$ 中的 $$&#92;q(z)$$, 关于 $$&#92;theta$$ 做优化.
$$ &#92;theta = arg &#92;max &#92;limits_{&#92;theta} &#92;int q(Z) log&#92;,&#92;frac {p(X, Z|&#92;theta)}{q(Z)} &#92;, d(Z) = arg &#92;max &#92;limits_{&#92;theta} &#92;int p(Z|X, &#92;theta^{old}) log&#92;, p(X, Z|&#92;theta) &#92;, d(Z)$$</p></li>
</ul>

<p>整个过程可以用图示表示如下：
<img src="http://blog.tvect.cc/wp-content/uploads/2018/06/em-mm.png" alt="" />

<h1>应用</h1>

<h2>GMM</h2>

在 GMM 中, $$ p(X|&#92;theta) = &#92;sum_{l=1}^{k}&#92;alpha_{l}N(X|u_{l}, &#92;Sigma_{l}) $$, 其中 $$&#92;sum_{l=1}^{k}&#92;alpha_{l}=1, &#92;quad &#92;theta = &#92;{&#92;alpha_{1}, ..., &#92;alpha_{k}, u_{1}, ..., u_{k}, &#92;Sigma_{1}, ..., &#92;Sigma_{k}&#92;}$$.

为了做极大似然估计，引入隐变量 $$Z = &#92;{z_{1}, ..., z_{n}&#92;}$$, 其中$$z_{i}$$ 表示的是与观测量 $$x_{i}$$ 相对应的混合成分, $$p(z_{i}) = &#92;alpha_{z_{i}}$$

引入隐变量之后相关的概率计算公式如下：
$$
&#92;begin{aligned}
P(x_{i} | z_{i}) &amp;= N(x_{i} | u_{z_{i}}, &#92;Sigma_{z_{i}}) &#92;&#92;
P(X, Z) &amp;= &#92;prod &#92;limits_{i=1}^{n}p(x_{i}, z_{i}) = &#92;prod &#92;limits_{i=1}^{n}p(x_{i}|z_{i}) p(z_{i}) = &#92;prod &#92;limits_{i=1}^{n}&#92;alpha_{z_{i}}N(x_{i}|u_{z_{i}}, &#92;Sigma_{z_{i}}) &#92;&#92;
P(Z|X, &#92;theta) &amp;= &#92;prod &#92;limits_{i=1}^{n}p(z_{i}|x_{i}) = &#92;prod &#92;limits_{i=1}^{n} &#92;frac{&#92;alpha_{z_{i}}N(x_{i}|u_{z_{i}}, &#92;Sigma_{z_{i}})}{&#92;sum_{l=1}^{k}&#92;alpha_{z_{i}}N(x_{i}|u_{z_{i}}, &#92;Sigma_{z_{i}}) }
&#92;end{aligned}
$$

使用EM算法：

<ol>
<li><strong>E-step:</strong>
由前面的公式计算 $$p(Z|X, &#92;theta^{old})$$.</li>
<li><strong>M-step:</strong>
$$
&#92;begin{aligned}
Q(&#92;theta, &#92;theta^{old}) &amp;= &#92;sum &#92;limits_{Z} p(Z|X, &#92;theta^{old}) &#92;, ln&#92;,p(X, Z|&#92;theta) &#92;&#92;
&amp;= &#92;sum &#92;limits_{Z} (&#92;prod &#92;limits_{i=1}^{n}p(z_{i}|x_{i}, &#92;theta^{old})(&#92;sum &#92;limits_{i=1}^{n}ln&#92;,p(z_{i}, x_{i} | &#92;theta))) &#92;&#92;
&amp;= &#92;sum &#92;limits_{Z} &#92;sum &#92;limits_{j=1}^{n} (&#92;prod &#92;limits_{i &#92;neq j}p(z_{i}|x_{i}, &#92;theta^{old})) p(z_{j} | x_{j}, &#92;theta^{old}) ln &#92;, p(x_{j}, z_{j} |&#92;theta)&#92;&#92;
&amp;= &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{Z} (&#92;prod &#92;limits_{i &#92;neq j}p(z_{i}|x_{i}, &#92;theta^{old})) p(z_{j} | x_{j}, &#92;theta^{old}) ln &#92;, p(x_{j}, z_{j} |&#92;theta)&#92;&#92;
&amp;= &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{z_{j}=1}^{k} p(z_{j} | x_{j}, &#92;theta^{old}) ln &#92;, p(x_{j}, z_{j} |&#92;theta) &#92;&#92;
&amp;= &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{l=1}^{k} p(l | x_{j}, &#92;theta^{old}) ln &#92;, &#92;alpha_{l} N(x_{i} | u_{l}, &#92;Sigma_{l}) &#92;&#92;
&amp;= &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{l=1}^{k} p(l | x_{j}, &#92;theta^{old}) (ln &#92;, &#92;alpha_{l} - &#92;frac{D}{2} ln&#92;,2&#92;pi - &#92;frac{1}{2}ln&#92;,|&#92;Sigma| -&#92;frac{1}{2} (x_{j}-u_{l})^{T}&#92;Sigma_{l}^{-1}(x_{j}-u_{l}))
&#92;end{aligned}
$$</li>
</ol>

<strong>下面分别关于参数 $$&#92;alpha, u, &#92;Sigma$$ 做优化</strong>有：

<ul>
<li><strong>关于 $$&#92;alpha$$ 做优化</strong></li>
</ul>

优化时涉及到的相关项为: $$A = &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{l=1}^{k} p(l | x_{j}, &#92;theta^{old}) ln &#92;, &#92;alpha_{l}$$.

同时考虑到约束条件$$&#92;sum_{l=1}^{k}&#92;alpha_{l}=1$$, 使用 Lagrange Multiplier, 有：
$$
&#92;begin{aligned}
LM(&#92;alpha, &#92;lambda) &amp;= &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{l=1}^{k} p(l | x_{j}, &#92;theta^{old}) ln &#92;, &#92;alpha_{l} - &#92;lambda (&#92;sum_{l=1}^{k}&#92;alpha_{l} - 1) &#92;&#92;

&#92;frac {&#92;partial LM(&#92;alpha, &#92;lambda)} {&#92;partial &#92;alpha_{l}} &amp;= &#92;frac {1}{&#92;alpha_{l}} &#92;sum &#92;limits_{j=1}^{n} p(l | x_{j}, &#92;theta^{old}) -&#92;lambda &#92;&#92;

&#92;Rightarrow &amp;&#92;quad &#92;alpha_{l} = &#92;frac {&#92;sum_{j=1}^{n} p(l | x_{j}, &#92;theta^{old})}{n}
&#92;end{aligned}
$$

<ul>
<li><strong>关于 $$u$$ 做优化</strong></li>
</ul>

优化时涉及到的相关项为: $$B = -&#92;frac{1}{2} &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{l=1}^{k} p(l | x_{j}, &#92;theta^{old}) (x_{j}-u_{l})^{T}&#92;Sigma_{l}^{-1}(x_{j}-u_{l})$$.

关于 $$u$$ 求偏导数有：
$$
&#92;begin{aligned}
&#92;frac {&#92;partial B} {&#92;partial u_{l}} &amp;= - &#92;frac{1}{2} &#92;sum &#92;limits_{j=1}^{n} p(l | x_{j}, &#92;theta^{old}) &#92;Sigma_{l}^{-1}(x_{i}-u_{l}) (-2) &#92;&#92;

&#92;Rightarrow &amp;&#92;quad u_{l} = &#92;frac {&#92;sum_{j=1}^{n} p(l | x_{j}, &#92;theta^{old}) x_{j}} {&#92;sum_{j=1}^{n} p(l | x_{j}, &#92;theta^{old})}
&#92;end{aligned}
$$

<ul>
<li><strong>关于 $$&#92;Sigma$$ 做优化</strong></li>
</ul>

优化时涉及到的相关项为: $$C = &#92;sum &#92;limits_{j=1}^{n} &#92;sum &#92;limits_{l=1}^{k} p(l | x_{j}, &#92;theta^{old}) (&#92;frac{1}{2}ln&#92;,|&#92;Lambda| -&#92;frac{1}{2} (x_{j}-u_{l})^{T} &#92;Lambda (x_{j}-u_{l})))$$. 其中 $$&#92;Lambda_{l} = &#92;Sigma_{l}^{-1}$$

关于 $$&#92;Lambda_{l}$$ 求偏导数有：
$$
&#92;begin{aligned}
&#92;frac {&#92;partial C} {&#92;partial &#92;Lambda} &amp;=  &#92;frac{1}{2} &#92;sum_{j=1}^{n} p(l|x_{j}, &#92;theta^{old}) (&#92;Lambda_{l}^{-1} - (x_{j}-u_{l})(x_{j}-u_{l})^{T})&#92;&#92;
&#92;&#92;
&#92;Rightarrow &amp;&#92;quad &#92;Sigma_{l} = &#92;Lambda_{l}^{-1} = &#92;frac {&#92;sum_{j=1}^{n} p(l|x_{j}, &#92;theta^{old}) (x_{j}-u_{l})(x_{j}-u_{l})^{T})}{&#92;sum_{j=1}^{n} p(l|x_{j}, &#92;theta^{old})}
&#92;end{aligned}
$$

上面的求导过程中用到了一些矩阵的导数公式, 可以参考 PRML 中 Appendix C. Properties of Matrices.

<h2>K-Means</h2>

K-Means是一种硬聚类的手段，把每个点唯一的关联到一个聚簇. 可以通过下面的手段从 GMM 的 EM 算法中导出 K-Means 算法.

首先，假定 GMM 中每个混合成分的共享协方差矩阵 $$&#92;epsilon I$$, 此时 $$p(x|u_{l}, &#92;Sigma_{l}) = &#92;frac{1}{&#92;sqrt{2 &#92;pi &#92;epsilon}} exp &#92;{-&#92;frac{1}{2&#92;epsilon} ||x-u_{l}|| &#92;}$$

将 $$&#92;epsilon$$ 视为一个固定的常数，而不是一个待估计的参数。

在上述假定下，应用 EM 算法有：

$$
p(l | x_{i}, &#92;theta) = &#92;frac{&#92;alpha_{l} exp(-||x_{i}-u_{l}||^{2}/2&#92;epsilon)}{&#92;sum_{l=1}^{k}&#92;alpha_{l} exp(-||x_{i}-u_{l}||^{2}/2&#92;epsilon) }
$$

考虑取极限 $$&#92;epsilon &#92;rightarrow 0 $$, 在 $$p(l|x_{i}, &#92;theta)$$ 的等式右侧，分子分母同除以 $$||x_{i} - u_{l}||$$最小的项，可以知道： 当$$ l = arg &#92;min ||x_{i} - u_{l}||$$时， $$p(l|x_{i}, &#92;theta) = 1$$， 对其它$$l$$, 取值为0.
在这种取极限的情况，得到了硬聚类。容易知道，此时 $$u$$ 的更新公式即为标准的K-Means下的更新公式.

<h2>PLSA</h2>

采用的思路为: $$p(x) = p(d_{i}, w_{j}) = p(d_{i})p(w_{j}|d_{i}) = p(d_{i}) &#92;sum_{l=1}^{k}p(w_{j} | l)p(l|d_{i})$$.

观察到的数据为 $$(d_{i}, w_{j})$$, 要估计的参数为 $$ &#92;theta = &#92;{p(w_{j} | l), p(l|d_{i}) &#92;}$$.

同样因为在做loglikelihood的时候, log在求和符号的外侧，求导会很麻烦， 所以这里采用了 EM 算法来进行参数估计。

引入隐藏变量 $$z_{l}$$， 表示所属的 topic 分布.

<ol>
<li><strong>E-step:</strong></li>
</ol>

$$
&#92;begin{aligned}
p(z_{l} | d_{i}, w_{j}, &#92;theta^{old}) &amp;= &#92;frac {p(z_{l}, d_{i}, w_{j} | &#92;theta^{old})} {p(d_{i}, w_{j} | &#92;theta^{old})} &#92;&#92;
&#92;&#92;
&amp;= &#92;frac {p(d_{i}) p(w_{j}|z_{l}) p(z_{l}|d_{i})} {p(d_{i}) &#92;sum_{l=1}^{k}p(w_{j} | z_{l})p(z_{l}|d_{i})} &#92;&#92;
&#92;&#92;
&amp;= &#92;frac {p(w_{j}|z_{l}) p(z_{l}|d_{i})} {&#92;sum_{l=1}^{k}p(w_{j} | z_{l})p(z_{l}|d_{i})}
&#92;end{aligned}
$$

<ol start="2">
<li><strong>M-step:</strong></li>
</ol>

$$
&#92;begin{aligned}
Q(&#92;theta, &#92;theta^{old}) &amp;= &#92;sum &#92;limits_{i, j} n(d_{i}, w_{j}) &#92;sum &#92;limits_{l} p(z_{l} | d_{i}, w_{j}, &#92;theta^{old}) ln&#92;, p(d_{i}, w_{j}, z_{l} | &#92;theta) &#92;&#92;
&amp;= &#92;sum &#92;limits_{i, j} n(d_{i}, w_{j}) &#92;sum &#92;limits_{l} p(z_{l} | d_{i}, w_{j}, &#92;theta^{old}) (ln&#92;,p(d_{i} | &#92;theta) + ln&#92;,p(w_{j}|z_{l}, &#92;theta) + ln&#92;,p(z_{l}|d_{i})) &#92;&#92;
&#92;end{aligned}
$$

<ul>
<li><strong>优化$$p(w_{j}|z_{l}, &#92;theta)$$</strong></li>
</ul>

优化时涉及到的相关项为: $$A = &#92;sum &#92;limits_{i, j} n(d_{i}, w_{j}) &#92;sum &#92;limits_{l} p(z_{l} | d_{i}, w_{j}, &#92;theta^{old}) ln&#92;,p(w_{j}|z_{l}, &#92;theta)$$.

同时考虑到约束条件$$&#92;sum_{j}p(w_{j}|z_{l}, &#92;theta)=1$$, 使用 Lagrange Multiplier, 有：

$$
&#92;begin{aligned}
LM(p(w_{j}|z_{l}, &#92;theta), &#92;lambda) &amp;= &#92;sum &#92;limits_{i, j} n(d_{i}, w_{j}) &#92;sum &#92;limits_{l} p(z_{l} | d_{i}, w_{j}, &#92;theta^{old}) ln&#92;,p(w_{j}|z_{l}, &#92;theta) - &#92;sum_{l} &#92;lambda_{l} (&#92;sum_{j}p(w_{j}|z_{l}, &#92;theta)-1) &#92;&#92;
&#92;&#92;
&#92;Rightarrow &amp;&#92;quad p(w_{j}|z_{l}, &#92;theta) = &#92;frac{&#92;sum &#92;limits_{i} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, &#92;theta^{old})} {&#92;sum &#92;limits_{i} &#92;sum &#92;limits_{j} n(d_{i}, w_{j}) p(z_{l}|w_{j}, d_{i}, &#92;theta^{old})}

&#92;end{aligned}
$$

<ul>
<li><strong>优化$$p(z_{l}|d_{i}, &#92;theta)$$</strong></li>
</ul>

优化时涉及到的相关项为: $$A = &#92;sum &#92;limits_{i, j} n(d_{i}, w_{j}) &#92;sum &#92;limits_{l} p(z_{l} | d_{i}, w_{j}, &#92;theta^{old}) ln&#92;,p(z_{l}|d_{i}, &#92;theta)$$.

同时考虑到约束条件$$&#92;sum_{l}p(z_{l}|d_{i}, &#92;theta)=1$$, 使用 Lagrange Multiplier, 有：

$$
&#92;begin{aligned}
LM(p(z_{l}|d_{i}, &#92;theta), &#92;lambda) &amp;= &#92;sum &#92;limits_{i, j} n(d_{i}, w_{j}) &#92;sum &#92;limits_{l} p(z_{l} | d_{i}, w_{j}, &#92;theta^{old}) ln&#92;,p(z_{l}|d_{i}, &#92;theta) - &#92;sum_{i} &#92;lambda_{i} (&#92;sum_{l}p(z_{l}|d_{i}, &#92;theta)-1) &#92;&#92;
&#92;&#92;
&#92;Rightarrow
&#92;&#92;
p(z_{l}|d_{i}, &#92;theta) &amp;= &#92;frac{&#92;sum &#92;limits_{j} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, &#92;theta^{old})} {&#92;sum &#92;limits_{l} &#92;sum &#92;limits_{j} n(d_{i}, w_{j}) p(z_{l}|w_{j}, d_{i}, &#92;theta^{old})} &#92;&#92;

&amp;=&#92;frac{&#92;sum &#92;limits_{j} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, &#92;theta^{old})} {&#92;sum &#92;limits_{j} n(d_{i}, w_{j})} &#92;&#92;

&amp;= &#92;frac{&#92;sum_{j} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, &#92;theta^{old})} {n(d_{i})}
&#92;end{aligned}
$$

<hr />

<strong>...完结撒花...</strong>
:smile: :smile: :smile: :smile: :smile:

<h1>参考资料</h1>

<ul>
<li>《PRML》 Chapter 09：Mixture Models and EM</li>
<li>《PRML》 Appendix C：Properties of Matrices</li>
</ul>