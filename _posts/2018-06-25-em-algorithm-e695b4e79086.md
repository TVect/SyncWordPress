---
ID: 41
post_title: EM Algorithm 整理
author: chin340823
post_excerpt: ""
layout: post
permalink: https://blog.tvect.cn/archives/41
published: true
post_date: 2018-06-25 21:56:41
---
[toc]

<!--more-->

<h1>The General EM Algorithm</h1>

<h2>算法概述</h2>

给定观测变量 $X$, 和隐变量 $Z$ 上的联合分布 $p(X, Z|\theta)$, $\theta$ 是参数. 目标是要关于 $\theta$ 极大化似然函数 $p(X|\theta)$

<ol>
<li>为参数 $\theta^{old}$ 赋一个初始值.</p></li>
<li><p><strong>E step</strong>
计算 $p(Z|X, \theta^{old})$.</p></li>
<li><p><strong>M step</strong>
计算 $\theta^{new}$: $ \theta^{new} = arg \max \limits_{\theta} Q(\theta, \theta^{old})$
其中 $Q(\theta, \theta^{old}) = \sum \limits_{Z} p(Z|X, \theta^{old}) \ ln \ p(X, Z|\theta)$</p></li>
<li><p>检查对数似然函数或者参数值的收敛性。
如果不满足收敛准则，那么令 $ \theta^{new} \leftarrow \theta^{old}$. 然后，回到第2步，继续.</p></li>
</ol>

<h2>算法有效性/收敛性证明</h2>

<h3>证明1</h3>

<p>要证明EM算法的有效性/收敛性, 只需要证明每一次迭代可以保证 $log \ p(X|\theta^{i+1}) = L(X|\theta^{i+1}) \ge L(X|\theta^{i})$

<strong>证明如下：</strong>

$$L(X|\theta) = log \ p(X|\theta) = log \frac {p(X, Z|\theta)}{p(Z|X, \theta)} = log \ p(X, Z|\theta) - log \ p(Z|X, \theta)$$

等式两边同时关于 $p(Z|X, \theta^{old})$ 做积分有：

$$
\begin{aligned}
log \, p(X|\theta) &amp; = \int p(Z|X, \theta^{old})log \ p(X|\theta) \ dZ&#92;
&amp; = \int p(Z|X, \theta^{old}) log \ p(X, Z|\theta) \ dZ - p(Z|X, \theta^{old}) log \ p(Z|X, \theta) \ dZ &#92;
&amp; = Q(\theta, \theta^{old}) - \int p(Z|X, \theta^{old}) log \ p(Z|X, \theta) \ dZ
\end{aligned}
$$

下面比较 $ L(\theta^{new})$ 和 $L(\theta^{old})$, 有:

$$
L(\theta^{new}) - L(\theta^{old}) = Q(\theta^{new}, \theta^{old}) - Q(\theta^{old}, \theta^{old}) + \int p(Z|X,\theta^{old}) log\frac{p(Z|X, \theta^{old})}{p(Z|X, \theta^{new})} \ dZ
$$

根据 $\theta^{new}$ 的定义, 可以知道 $Q(\theta^{new}, \theta^{old}) - Q(\theta^{old}, \theta^{old}) \ge 0$

根据 KL 距离的非负性, 可以知道 $\int p(Z|X,\theta^{old}) log\frac{p(Z|X, \theta^{old})}{p(Z|X, \theta^{new})} \ dZ = KL(p(Z|X, \theta^{old}), p(Z|X, \theta^{new})) \ge 0$

综上,有 $L(\theta^{new}) - L(\theta^{old}) \ge 0$, 收敛性得证.

<h3>证明2</h3>

与前一种证明类似，可以看到:

$$L(X|\theta) = log \ p(X|\theta) = log \frac {p(X, Z|\theta)}{p(Z|X, \theta)} = log \ \frac {p(X, Z|\theta)}{q(Z)} + log \ \frac{q(Z)}{p(Z|X, \theta)}$$

其中 $q(Z)$ 是引入的一个关于 $Z$ 的分布. 两边同时关于 $q(Z)$ 求积分, 得到:

$$
\begin{aligned}
log \ p(X|\theta) &amp; = \int q(Z) \ log \ p(X|\theta) \ dZ&#92;
&amp; = \int q(Z) log \ \frac {p(X, Z|\theta)}{q(Z)} + q(Z) log \ \frac {q(Z)} {p(Z|X, \theta)} \ dZ &#92;
&amp; = \int q(Z) log \ \frac {p(X, Z|\theta)}{q(Z)} \ d(Z) + KL(q(Z), p(Z|X, \theta)) &#92;
&amp; = F(q, \theta) + KL(q(Z), p(Z|X, \theta))
\end{aligned}
$$

EM 算法可以视为 通过优化 $L(X|\theta)$ 的下界 $F(q, \theta)$, 来达到优化 $L(X|\theta)$ 的目的.

<ul>
<li>在 <strong>E-step</strong>: 固定 $F(q, \theta)$ 中的 $\theta$, 关于 $q(Z)$ 做优化.
因为 $L(X|\theta)$ 是 $F(q, \theta)$ 的上界, 且与 $q(Z)$ 无关, 容易看到, 在 $q(Z) = p(Z|X, \theta)$时, $F(q, \theta)$ 会取得最大值，即为 $L(X|\theta)$.</p></li>
<li><p>在 <strong>M-step</strong>: 固定 $F(q, \theta)$ 中的 $q(z)$, 关于 $\theta$ 做优化.
$ \theta = arg \max \limits_{\theta} \int q(Z) log\,\frac {p(X, Z|\theta)}{q(Z)} \, d(Z) = arg \max \limits_{\theta} \int p(Z|X, \theta^{old}) log\, p(X, Z|\theta) \, d(Z)$</p></li>
</ul>

<p>整个过程可以用图示表示如下：
<img src="http://blog.tvect.cc/wp-content/uploads/2018/06/em-mm.png" alt="" />

<h1>应用</h1>

<h2>GMM</h2>

在 GMM 中, $ p(X|\theta) = \sum_{l=1}^{k}\alpha_{l}N(X|u_{l}, \Sigma_{l}) $, 其中 $\sum_{l=1}^{k}\alpha_{l}=1, \quad \theta = &#123;\alpha_{1}, ..., \alpha_{k}, u_{1}, ..., u_{k}, \Sigma_{1}, ..., \Sigma_{k}&#125;$.

为了做极大似然估计，引入隐变量 $Z = &#123;z_{1}, ..., z_{n}&#125;$, 其中$z_{i}$ 表示的是与观测量 $x_{i}$ 相对应的混合成分, $p(z_{i}) = \alpha_{z_{i}}$

引入隐变量之后相关的概率计算公式如下：
$$
\begin{aligned}
P(x_{i} | z_{i}) &amp;= N(x_{i} | u_{z_{i}}, \Sigma_{z_{i}}) &#92;
P(X, Z) &amp;= \prod \limits_{i=1}^{n}p(x_{i}, z_{i}) = \prod \limits_{i=1}^{n}p(x_{i}|z_{i}) p(z_{i}) = \prod \limits_{i=1}^{n}\alpha_{z_{i}}N(x_{i}|u_{z_{i}}, \Sigma_{z_{i}}) &#92;
P(Z|X, \theta) &amp;= \prod \limits_{i=1}^{n}p(z_{i}|x_{i}) = \prod \limits_{i=1}^{n} \frac{\alpha_{z_{i}}N(x_{i}|u_{z_{i}}, \Sigma_{z_{i}})}{\sum_{l=1}^{k}\alpha_{z_{i}}N(x_{i}|u_{z_{i}}, \Sigma_{z_{i}}) }
\end{aligned}
$$

使用EM算法：

<ol>
<li><strong>E-step:</strong>
由前面的公式计算 $p(Z|X, \theta^{old})$.</p></li>
<li><p><strong>M-step:</strong>
$$
\begin{aligned}
Q(\theta, \theta^{old}) &amp;= \sum \limits_{Z} p(Z|X, \theta^{old}) \, ln\,p(X, Z|\theta) &#92;
&amp;= \sum \limits_{Z} (\prod \limits_{i=1}^{n}p(z_{i}|x_{i}, \theta^{old})(\sum \limits_{i=1}^{n}ln\,p(z_{i}, x_{i} | \theta))) &#92;
&amp;= \sum \limits_{Z} \sum \limits_{j=1}^{n} (\prod \limits_{i \neq j}p(z_{i}|x_{i}, \theta^{old})) p(z_{j} | x_{j}, \theta^{old}) ln \, p(x_{j}, z_{j} |\theta)&#92;
&amp;= \sum \limits_{j=1}^{n} \sum \limits_{Z} (\prod \limits_{i \neq j}p(z_{i}|x_{i}, \theta^{old})) p(z_{j} | x_{j}, \theta^{old}) ln \, p(x_{j}, z_{j} |\theta)&#92;
&amp;= \sum \limits_{j=1}^{n} \sum \limits_{z_{j}=1}^{k} p(z_{j} | x_{j}, \theta^{old}) ln \, p(x_{j}, z_{j} |\theta) &#92;
&amp;= \sum \limits_{j=1}^{n} \sum \limits_{l=1}^{k} p(l | x_{j}, \theta^{old}) ln \, \alpha_{l} N(x_{i} | u_{l}, \Sigma_{l}) &#92;
&amp;= \sum \limits_{j=1}^{n} \sum \limits_{l=1}^{k} p(l | x_{j}, \theta^{old}) (ln \, \alpha_{l} - \frac{D}{2} ln\,2\pi - \frac{1}{2}ln\,|\Sigma| -\frac{1}{2} (x_{j}-u_{l})^{T}\Sigma_{l}^{-1}(x_{j}-u_{l}))
\end{aligned}
$$</p></li>
</ol>

<p><strong>下面分别关于参数 $\alpha, u, \Sigma$ 做优化</strong>有：

<ul>
<li><strong>关于 $\alpha$ 做优化</strong></li>
</ul>

优化时涉及到的相关项为: $A = \sum \limits_{j=1}^{n} \sum \limits_{l=1}^{k} p(l | x_{j}, \theta^{old}) ln \ \alpha_{l}$.

同时考虑到约束条件$\sum_{l=1}^{k}\alpha_{l}=1$, 使用 Lagrange Multiplier, 有：
$$
\begin{aligned}
LM(\alpha, \lambda) &amp;= \sum \limits_{j=1}^{n} \sum \limits_{l=1}^{k} p(l | x_{j}, \theta^{old}) ln \, \alpha_{l} - \lambda (\sum_{l=1}^{k}\alpha_{l} - 1) &#92;

\frac {\partial LM(\alpha, \lambda)} {\partial \alpha_{l}} &amp;= \frac {1}{\alpha_{l}} \sum \limits_{j=1}^{n} p(l | x_{j}, \theta^{old}) -\lambda &#92;

\Rightarrow &amp;\quad \alpha_{l} = \frac {\sum_{j=1}^{n} p(l | x_{j}, \theta^{old})}{n}
\end{aligned}
$$

<ul>
<li><strong>关于 $u$ 做优化</strong></li>
</ul>

优化时涉及到的相关项为: $B = -\frac{1}{2} \sum \limits_{j=1}^{n} \sum \limits_{l=1}^{k} p(l | x_{j}, \theta^{old}) (x_{j}-u_{l})^{T}\Sigma_{l}^{-1}(x_{j}-u_{l})$.

关于 $u$ 求偏导数有：
$$
\begin{aligned}
\frac {\partial B} {\partial u_{l}} &amp;= - \frac{1}{2} \sum \limits_{j=1}^{n} p(l | x_{j}, \theta^{old}) \Sigma_{l}^{-1}(x_{i}-u_{l}) (-2) &#92;

\Rightarrow &amp;\quad u_{l} = \frac {\sum_{j=1}^{n} p(l | x_{j}, \theta^{old}) x_{j}} {\sum_{j=1}^{n} p(l | x_{j}, \theta^{old})}
\end{aligned}
$$

<ul>
<li><strong>关于 $\Sigma$ 做优化</strong></li>
</ul>

优化时涉及到的相关项为: $C = \sum \limits_{j=1}^{n} \sum \limits_{l=1}^{k} p(l | x_{j}, \theta^{old}) (\frac{1}{2}ln\,|\Lambda| -\frac{1}{2} (x_{j}-u_{l})^{T} \Lambda (x_{j}-u_{l})))$. 其中 $\Lambda_{l} = \Sigma_{l}^{-1}$

关于 $\Lambda_{l}$ 求偏导数有：
$$
\begin{aligned}
\frac {\partial C} {\partial \Lambda} &amp;=  \frac{1}{2} \sum_{j=1}^{n} p(l|x_{j}, \theta^{old}) (\Lambda_{l}^{-1} - (x_{j}-u_{l})(x_{j}-u_{l})^{T})&#92;
&#92;
\Rightarrow &amp;\quad \Sigma_{l} = \Lambda_{l}^{-1} = \frac {\sum_{j=1}^{n} p(l|x_{j}, \theta^{old}) (x_{j}-u_{l})(x_{j}-u_{l})^{T})}{\sum_{j=1}^{n} p(l|x_{j}, \theta^{old})}
\end{aligned}
$$

上面的求导过程中用到了一些矩阵的导数公式, 可以参考 PRML 中 Appendix C. Properties of Matrices.

<h2>K-Means</h2>

K-Means是一种硬聚类的手段，把每个点唯一的关联到一个聚簇. 可以通过下面的手段从 GMM 的 EM 算法中导出 K-Means 算法.

首先，假定 GMM 中每个混合成分的共享协方差矩阵 $\epsilon I$, 此时 $p(x|u_{l}, \Sigma_{l}) = \frac{1}{\sqrt{2 \pi \epsilon}} exp &#123;-\frac{1}{2\epsilon} ||x-u_{l}|| &#125;$

将 $\epsilon$ 视为一个固定的常数，而不是一个待估计的参数。

在上述假定下，应用 EM 算法有：

$$
p(l | x_{i}, \theta) = \frac{\alpha_{l} exp(-||x_{i}-u_{l}||^{2}/2\epsilon)}{\sum_{l=1}^{k}\alpha_{l} exp(-||x_{i}-u_{l}||^{2}/2\epsilon) }
$$

考虑取极限 $\epsilon \rightarrow 0 $, 在 $p(l|x_{i}, \theta)$ 的等式右侧，分子分母同除以 $||x_{i} - u_{l}||$最小的项，
可以知道： 当$ l = arg \min ||x_{i} - u_{l}||$时， $p(l|x_{i}, \theta) = 1$， 对其它$l$, 取值为0.
在这种取极限的情况，得到了硬聚类。容易知道，此时 $u$ 的更新公式即为标准的K-Means下的更新公式.

<h2>PLSA</h2>

采用的思路为: $p(x) = p(d_{i}, w_{j}) = p(d_{i})p(w_{j}|d_{i}) = p(d_{i}) \sum_{l=1}^{k}p(w_{j} | l)p(l|d_{i})$.

观察到的数据为 $(d_{i}, w_{j})$, 要估计的参数为 $ \theta = &#123;p(w_{j} | l), p(l|d_{i}) &#125;$.

同样因为在做loglikelihood的时候, log在求和符号的外侧，求导会很麻烦， 所以这里采用了 EM 算法来进行参数估计。

引入隐藏变量 $z_{l}$， 表示所属的 topic 分布.

<ol>
<li><strong>E-step:</strong></li>
</ol>

$$
\begin{aligned}
p(z_{l} | d_{i}, w_{j}, \theta^{old}) &amp;= \frac {p(z_{l}, d_{i}, w_{j} | \theta^{old})} {p(d_{i}, w_{j} | \theta^{old})} &#92;
&#92;
&amp;= \frac {p(d_{i}) p(w_{j}|z_{l}) p(z_{l}|d_{i})} {p(d_{i}) \sum_{l=1}^{k}p(w_{j} | z_{l})p(z_{l}|d_{i})} &#92;
&#92;
&amp;= \frac {p(w_{j}|z_{l}) p(z_{l}|d_{i})} {\sum_{l=1}^{k}p(w_{j} | z_{l})p(z_{l}|d_{i})}
\end{aligned}
$$

<ol start="2">
<li><strong>M-step:</strong></li>
</ol>

$$
\begin{aligned}
Q(\theta, \theta^{old}) &amp;= \sum \limits_{i, j} n(d_{i}, w_{j}) \sum \limits_{l} p(z_{l} | d_{i}, w_{j}, \theta^{old}) ln\, p(d_{i}, w_{j}, z_{l} | \theta) &#92;
&amp;= \sum \limits_{i, j} n(d_{i}, w_{j}) \sum \limits_{l} p(z_{l} | d_{i}, w_{j}, \theta^{old}) (ln\,p(d_{i} | \theta) + ln\,p(w_{j}|z_{l}, \theta) + ln\,p(z_{l}|d_{i})) &#92;
\end{aligned}
$$

<ul>
<li><strong>优化 $p(w_{j}|z_{l}, \theta)$</strong>
优化时涉及到的相关项为: $A = \sum \limits_{i, j} n(d_{i}, w_{j}) \sum \limits_{l} p(z_{l} | d_{i}, w_{j}, \theta^{old}) ln\,p(w_{j}|z_{l}, \theta)$.
同时考虑到约束条件$\sum_{j}p(w_{j}|z_{l}, \theta)=1$, 使用 Lagrange Multiplier, 有：</li>
</ul>

$$
\begin{aligned}
LM(p(w_{j}|z_{l}, \theta), \lambda) &amp;= \sum \limits_{i, j} n(d_{i}, w_{j}) \sum \limits_{l} p(z_{l} | d_{i}, w_{j}, \theta^{old}) ln\,p(w_{j}|z_{l}, \theta) - \sum_{l} \lambda_{l} (\sum_{j}p(w_{j}|z_{l}, \theta)-1) &#92;
&#92;
\Rightarrow &amp;\quad p(w_{j}|z_{l}, \theta) = \frac{\sum \limits_{i} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, \theta^{old})} {\sum \limits_{i} \sum \limits_{j} n(d_{i}, w_{j}) p(z_{l}|w_{j}, d_{i}, \theta^{old})}

\end{aligned}
$$

<ul>
<li><strong>优化 $p(z_{l}|d_{i}, \theta)$</strong>
优化时涉及到的相关项为: $A = \sum \limits_{i, j} n(d_{i}, w_{j}) \sum \limits_{l} p(z_{l} | d_{i}, w_{j}, \theta^{old}) ln\,p(z_{l}|d_{i}, \theta)$.
同时考虑到约束条件 $\sum_{l}p(z_{l}|d_{i}, \theta)=1$, 使用 Lagrange Multiplier, 有：</li>
</ul>

$$
\begin{aligned}
LM(p(z_{l}|d_{i}, \theta), \lambda) &amp;= \sum \limits_{i, j} n(d_{i}, w_{j}) \sum \limits_{l} p(z_{l} | d_{i}, w_{j}, \theta^{old}) ln\,p(z_{l}|d_{i}, \theta) - \sum_{i} \lambda_{i} (\sum_{l}p(z_{l}|d_{i}, \theta)-1) &#92;
&#92;
\Rightarrow
&#92;
p(z_{l}|d_{i}, \theta) &amp;= \frac{\sum \limits_{j} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, \theta^{old})} {\sum \limits_{l} \sum \limits_{j} n(d_{i}, w_{j}) p(z_{l}|w_{j}, d_{i}, \theta^{old})} &#92;

&amp;=\frac{\sum \limits_{j} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, \theta^{old})} {\sum \limits_{j} n(d_{i}, w_{j})} &#92;

&amp;= \frac{\sum_{j} n(d_{i}, w{j}) p(z_{l}|w_{j}, d_{i}, \theta^{old})} {n(d_{i})}
\end{aligned}
$$

<h1>相关代码实现</h1>

参考 <a href="https://github.com/TVect/VectorLab/tree/master/ml/mixture_models">mixture models 代码</a>.

其中演示了使用 EM 求解 GMM 中参数, GMM 中的 Singularity 问题, 以及 Bayesian GMM 等.

<hr />

...完结撒花...
:smile: :smile: :smile: :smile: :smile:

<h1>参考资料</h1>

-《PRML》 Chapter 09：Mixture Models and EM
-《PRML》 Appendix C：Properties of Matrices