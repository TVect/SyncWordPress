---
ID: 531
post_title: 语义分割阅读笔记
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/531
published: true
post_date: 2019-07-31 09:09:26
---
之前在做蛋白质口袋发现的时候，接触到文章 <a href="https://arxiv.org/abs/1904.06517">Detection of protein-ligand binding sites with 3Dsegmentation</a>, 这篇文章使用语义分割的方法来做口袋发现. 借此机会，学习了一下语义分割的一些方法. 现将近期看的一些语义分割的内容整理如下.

<h1>概述</h1>

CNN 内在的 translation invariance, 适用于 high-level tasks. 但会损害 low-level tasks, 比如在语义分割任务中, 我们想要的是精确的位置信息而不是抽象化之后的空间信息.

<h1>模型简述</h1>

<h2>FCN</h2>

<strong>文献</strong>: <a href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a>

the first work to train FCNs end-to-end (1) for pixelwise prediction and (2) from supervised pre-training.

这篇文章中使用了跳跃结构来结合了深层的粗略的语义信息和浅层的精细的表征信息.

<blockquote>
  Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a local-to-global pyramid. We define a novel “skip” architecture to combine deep, coarse, semantic information and shallow, fine, appearance information
</blockquote>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/fcn.png" alt="" />

未使用跳跃结构的模型为FCN-32s，它直接将最后一层池化的特征图进行反卷积得到分割图像．原始图像经过FCN模型中的5次池化，图像大小压缩为原来的1/32，因此FCN-32s模型中的最后一层直接对它进行了因子 f 为32的上采样操作，由于上采样倍数过大，输出图像完全无法体现出原始图像的细节，所以结果非常差．

在FCN-16s和FCN-8s中使用了跳跃结构如上图所示，FCN-16s模型将第四次池化后的特征图与第五次池化后的特征图结合，既保留了图像的高阶特征，又不会损失过多的图像细节，得到的分割结果明显好于FCN-32s．FCN-8s模型与FCN-16s模型类似．

以FCN-16s模型为例，具体的融合过程为：首先对最后一个池化后的特征图进行因子为2的上采样操作，转化为2X2的图像，然后再与第四次池化后的2X2的特征图相叠加，最后对叠加后的结果进行因子为16的上采样操作．

<h2>UNet</h2>

<strong>文献</strong>: <a href="https://arxiv.org/abs/1505.04597">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/unet.png" alt="" />

<strong>结构说明</strong>:
1. The architecture consists of a <strong>contracting path</strong> to capture context and a symmetric <strong>expanding path</strong> that enables precise localization.

<ol start="2">
<li>左边的 contracting path 中用的是 valid convolution, 是为了让每次计算的时候有完整的上下文信息.</p></li>
<li><p>右边的 expanding path 中做上采样之后, 会和左侧 contracting path 中相对应的 feature map 做裁剪后再 concat.</p></li>
</ol>

<p><strong>overlap-tile strategy</strong>:

如果我们要预测黄色框内区域（即对黄色的内的细胞进行分割，获取它们的边缘），需要将蓝色框内部分作为输入，如果黄色区域在输入图像的边缘的话，那么缺失的数据使用镜像进行补充。如下图左边图像所示，输入图像周围一圈都进行了镜像补充。

这种做法可以让我们以分块图像作为输入而不是整个图像作为输入. 这种策略在处理大的图像的时候会很有用, 因为会有 GPU memory 的限制.

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/overlap-tile.png" alt="" />

<h2>SegNet</h2>

<strong>文献</strong>： <a href="https://arxiv.org/abs/1511.00561">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a>

<strong>代码</strong>： https://github.com/ykamikawa/tf-keras-SegNet.git

<blockquote>
  While several layers of max-pooling and sub-sampling can achieve more translation invariance for robust classification correspondingly there is a loss of spatial resolution of the feature maps.
  Therefore, it is necessary to capture and store boundary information in the encoder feature maps before sub-sampling is performed.
</blockquote>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/segnet.png" alt="" />

SegNet 和 U-net 结构很类似, 主要区别在于 decoder 阶段. SegNet 利用了相应的 encoder 层池化操作的 index 来做 unsampling, 而 U-net 则是将相应的 encoder 层做融合之后执行反卷积.

<strong>结构说明</strong>:

<ul>
<li>encoder 部分:</li>
</ul>

采用的是 VGG16 的前 13 层卷积网络，每个编码器层都对应一个解码器层，最终解码器的输出被送入softmax 分类器以独立的为每个像素产生类概率.

这里的卷积都是 same 卷积, 保持输入的尺寸, max pooling 操作会使得尺寸减少, 但这里在做 max pooling 的时候会记下相应的 max index, 以便在 decoder 的 upsampling 操作中使用.

<ul>
<li>decoder 部分:</li>
</ul>

如下图所示, Unpooling 时使用相应的 encoder 层保存下来的 index 信息, 直接将数据放回对应位置，后面再接 Conv 训练学习.

tensorflow 中使用 tf.scatter_nd 实现 <code>根据 max pooling indices 做 upsampling</code>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/unpooing_with_index.png" alt="" />

<h2>DeepLab v1</h2>

<strong>文献</strong>：
<a href="https://arxiv.org/abs/1606.00915">Semantic Image Segmantation with Deep Convolutional Nets and Fully Connected CRF</a>
<a href="http://vision.cs.utexas.edu/381V-fall2016/slides/kelle_paper.pdf">DeepLab slides</a>

<strong>代码</strong>： https://github.com/CoinCheung/Deeplab-Large-FOV/blob/master/lib/model.py

引入了带孔卷积处理DCNN中信号分辨率降低的问题, 引入 CRF 处理 spatial 不敏感的问题, 以获得捕获精细细节的能力.

<strong>两个问题及求解</strong>:

<ol>
<li>signal downsampling  --> 带孔卷积</li>
</ol>

使用 max_pooing + downsampling (strides) 会导致信号分辨率降低. 这里使用带孔卷积取代一部分 max_pooing + downsampling (strides) 操作.

使用了 atrous conv, 就一定程度上不用 pooling + downsampling (strides)了. atrous conv 中的孔可以取代一部分 pooling + downsampling (strides) 增加视野的功能.

<ol start="2">
<li>spatial 不敏感  --> CRF</li>
</ol>

前面提到的一些方法使用 skip-layers 之类的方法处理这个问题, 这里使用了 CRF.

<strong>模型说明</strong>：

<ul>
<li>模型基本结构：</li>
</ul>

基于 VGG-16, 忽略最后两个 max-pooling 层中下采样操作(即 stride 设置为 1), 并将其后的卷积改为 atrous conv,  最后得到的输出分辨率为 [224/8, 224/8].
训练时候, ground truth 会降采样到 [224/8, 224/8].
预测时候, 对 [224/8, 224/8] 做双线性插值得到 [224, 224] 的结果.

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/vgg-16-1024x282.png" alt="" />

<ul>
<li>Fully-Connected CRF:</li>
</ul>

DCNN 输出的 score map 已经很平滑了, 使用 short-range CRF 只能让结果更平滑, 无法得到 detailed local structure.
所以这里使用了 Fully-Connected CRF.

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/fc-crf.png" alt="" />

<h2>DeepLab v2</h2>

<strong>文献</strong>：<a href="https://arxiv.org/abs/1606.00915">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a>

在 DeepLab v1 的基础上引入了  atrous spatial pyramid pooling (ASPP) 结构, 以便能分割不同大小尺寸的对象. 另外主干网络从预训练的 VGG 变成了 ResNet.

ASPP层就是为了融合不同级别的语义信息：选择不同扩张率的带孔卷积去处理 Feature Map，由于感受野不同，得到的信息的Level也就不同，ASPP层把这些不同层级的feature map concat到一起，进行信息融合。

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/aspp.png" alt="" />

<h2>DeepLab V3</h2>

<strong>文献</strong>：<a href="https://arxiv.org/abs/1706.05587">Rethinking Atrous Convolution for Semantic Image Segmentation</a>

<h2>DeepLab V3+</h2>

<h1>其他参考资料</h1>

<ul>
<li><a href="https://github.com/mrgloom/awesome-semantic-segmentation">github: awesome-semantic-segmentation</a></li>
</ul>