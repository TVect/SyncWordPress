---
ID: 698
post_title: 一周小结-20200112-Part 3
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/698
published: true
post_date: 2020-01-12 21:40:48
---
本周的阅读笔记的第二部分，来源于 Jordan Book：An Introduction to PGMs，Chapter07. Linear classification

本周主要介绍了 Linear classification 问题中的 generative model 和 discriminative model.

<!--more-->

<h1>Linear regression and linear classification</h1>

<h1>Generative models</h1>

<img src="https://www.tvect.cn/wp-content/uploads/2020/01/generative_models.png" alt="" />

<h2>Gaussian class-conditional densities</h2>

首先考虑 binary classification, features 之间是条件独立的情形.

定义 prior 服从一个 Bernoulli 分布：$ p(y | \pi) = \pi^y (1-\pi)^{(1-y)}$

conditional density 为 高斯分布：

$$
\begin{aligned}
p(x_j | Y=0, \theta_j) &amp;= \frac{1}{(2\pi \sigma_j^2)^{1/2}} \exp(-\frac{1}{2\sigma_j^2}(x_j - \mu_{0j})^2) &#92;
p(x_j | Y=1, \theta_j) &amp;= \frac{1}{(2\pi \sigma_j^2)^{1/2}} \exp(-\frac{1}{2\sigma_j^2}(x_j - \mu_{1j})^2)
\end{aligned}
$$

<strong>Posterior probability</strong>

for Gaussian class-conditional densities(covariance matrix is the same in the two classes), the posterior probability takes the form:

$$ p(Y = 1 | x; \theta) = \frac{1}{1 + e^{- \theta x}} $$

where the parameter vector $\theta$ is a function of the means $u_k$, the covariance matrix $\Sigma$, and the prior probability $\pi$

<strong>Maximum likelihood estimates</strong>

<strong>Multiway classification</strong>

The linearity again arises from the Gaussian assumption for the class-conditional densities, together with the assumption of a constant covariance matrix.

$$ p(Y^k = 1 | x, \theta) = \frac{\exp(\beta_k^T x)}{\sum_l \exp(\beta_l^T x)} $$

<h2>The naive Bayes classifier</h2>

前面考虑的是连续的features, 现在考虑 features 是离散的情况, 即 class conditional distribution for every component is multinomial distribution.

此时, $p(x_1, ..., x_m | Y^i=1, \eta) = \prod_j \prod_k \eta_{ijk}^{x_j^k}$.

其中, $ \eta_{ijk} = p(x_j^k = 1 | Y^i=1, \eta)$

可以求得后验概率同样形如: $ p(Y^k = 1 | x, \theta) = \frac{\exp(\beta_k^T x)}{\sum_l \exp(\beta_l^T x)} $

<h2>The exponential family</h2>

前面已经看到，在 class-conditional density 是 multinomial 和 Gaussian 的情况下，后验概率分布形如 sigmoid / softmax, 而且等值线是是特征空间上的超平面（Gaussian的情况下不同的label对应的class-conditional density 需要有相等的 covariance）.

实际上，这种性质对于 exponential family distributions 都成立.

exponential family distributions 形如：$ p(x|\eta) = h(x) \exp(\eta^Tx - a(\eta))$

<h1>Discriminative models</h1>

前面看到一大类的class-conditional density 都导出了 logistic-linear or softmax-linear 形式的 posterior probability, 所以我们或许甚至可以不需要指定 class-conditional density 的阶段.

<h2>Logistic Regression</h2>

$$ p(y|x) = \mu(x)^y (1-\mu(x))^{1-y} $$

$$ \mu(x) = \frac{1}{1 + \exp(-\eta(x))} = \frac{1}{1 + \exp(-\theta^T x)}$$

log likelihood 表达式以及导数如下:

$$ l(\theta | D) = \sum_n (y_n \log \mu_n + (1-y_n) \log (1-\mu_n)) $$

$$ \bigtriangledown_{\theta} l = \sum_n (y_n - u_n) x_n $$

<h3>求解方法</h3>

<strong>An on-line estimation algorithm</strong>: $ \theta^{t+1} = \theta^t + \rho (y_n - \mu_n^t) x_n$

<strong>The iteratively reweighted least squares (IRLS) algorithm</strong>:

考虑 batch methods, 与 linear regression 类似，这里不适宜直接使用 steepest descent.

Newton-Raphson algorithm: $ \theta^{t+1} = \theta^{t} - H^{-1} \bigtriangledown_{\theta} J$

下面将 Newton-Raphson algorithm 分别用到 linear regression &amp; logistic regression 中.

<ol>
<li>linear regression</li>
</ol>

$$ J = \frac{1}{2} (y - X \theta)^T (y - X \theta) $$

$$ \bigtriangledown_{\theta} J = - X^T (y - X\theta) $$

$$ H = -X^T X$$

$$ \theta^{t+1} = \theta^{t} + (X^T X)^{-1} X^T (y - X\theta^t) = (X^T X)^{-1} X^T y$$

<ol start="2">
<li>logistic regression</li>
</ol>

将 Newton-Raphson algorithm 运用到 logistic regression, 可以求得更新公式为:

$$ \theta^{t+1} = (X^T W^{t} X)^{-1} X^T W^{t} z^{t} $$

其中,

$$ W = diag(\mu_1(1-\mu_1), \mu_2(1-\mu_2), ..., \mu_N (1 - \mu_N))$$

$$z^{(t)} = \eta + [W^{(t)}]^{-1} (y-u^{{(t)}})$$

相当于求解 weighted least squares 问题：$ J = \frac{1}{2} (z^{t} - X \theta)^T W^t (z^{t} - X \theta) $

related to weighted least squares in Chapter 06.

另外文中还提到了另外一个 <strong>关于 上述IRLS 的一个启发式视角</strong>.

<h2>Multiway classification</h2>

<h2>Probit regression</h2>

$$ p(Y = 1 | x; \theta) = \Phi(\theta^T x) $$

其中, $\Phi$ 为标准正态分布的累计分布函数.

<h2>The noisy-OR model</h2>

<img src="https://www.tvect.cn/wp-content/uploads/2020/01/noise-or.png" alt="" />

$$ p(Y = 0 | x; \xi) = \exp (\sum_{i=1}^m x_i \log \xi_i)  $$

<h2>Other exponential models</h2>

考虑 Poisson 分布: $p(z | \lambda) = \frac{\lambda^z e^{-\lambda}}{z!}$

$$ p(Y=1) = 1 - p(Z=0) = 1 - e^{-\lambda} = 1 - \exp(-e^{\theta^T x})$$

<h1>Summary</h1>

<strong>generative vs. discriminative models</strong>

<blockquote>
  Generative and discriminative models have complementary strengths and weaknesses. The generative approach allows knowledge about class-conditional densities to be exploited. If this knowledge is indeed reflective of the true data-generation process, then the generative approach can be more efficient than a corresponding discriminative model, in the sense that it will tend to require fewer data points. On the other hand, discriminative approaches tend to be more robust than generative approaches, making use of weaker assumptions regarding class-conditional densities. Note also that the discriminative framework presents a straightforward "upgrade path" toward the development of nonlinear classifiers --- we can retain the logistic and softmax functions, but replace the linear combination $\eta = \theta^T x$ with a nonlinear function
</blockquote>

<h1>参考资料</h1>

<ul>
<li><a href="">Jordan: An Introduction to PGMs Chapter 7</a></li>
</ul>