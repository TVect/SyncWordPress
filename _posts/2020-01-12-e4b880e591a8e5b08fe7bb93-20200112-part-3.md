---
ID: 698
post_title: 一周小结-20200112-Part 3
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/698
published: true
post_date: 2020-01-12 21:35:48
---
本周的阅读笔记的第二部分，来源于 Jordan Book：An Introduction to PGMs，Chapter07. Linear classification

本周主要介绍了 Linear classification 问题中的 generative model 和 discriminative model.

<!--more-->

<h1>Linear regression and linear classification</h1>

<h1>Generative models</h1>

<h2>Gaussian class-conditional densities</h2>

首先考虑 binary classification.

定义 prior 服从一个 Bernoulli 分布：$ p(y | \pi) = \pi^y (1-\pi)^{(1-y)}$

conditional density 为 高斯分布：

$$
\begin{aligned}
p()
\end{aligned}
$$

<strong>Posterior probability</strong>

for Gaussian class-conditional densities(covariance matrix is the same in the two classes), the posterior probability takes the form:

<code>$ p(Y = 1 | x; \theta) = \frac{1}{1 + e^{- \theta x}} $</code>

where the parameter vector <code>$\theta$</code> is a function of the means <code>$u_k$</code>, the covariance matrix <code>$\Sigma$</code>, and the prior probability <code>$\pi$</code>

<strong>Maximum likelihood estimates</strong>

<strong>Multiway classification</strong>

The linearity again arises from the Gaussian assumption for the class-conditional densities, together with the assumption of a constant covariance matrix.

<h2>The naive Bayes classifier</h2>

<h2>The exponential family</h2>

前面已经看到，在 class-conditional density 是 multinomial 和 Gaussian 的情况下，后验概率分布形如 sigmoid / softmax, 而且等值线是是特征空间上的超平面（Gaussian的情况下不同的label对应的class-conditional density 需要有相等的 covariance）.

实际上，这种性质对于 exponential family distributions 都成立.

<code>$ p(x|\eta) = h(x) \exp(\eta^Tx - a(\eta))$</code>

<h1>Discriminative models</h1>

前面看到一大类的class-conditional density 都导出了 logistic-linear or softmax-linear 形式的 posterior probability, 所以我们或许甚至可以不需要指定 class-conditional density 的阶段.

<h2>Logistic Regression</h2>

<strong>An on-line estimation algorithm</strong>

<strong>The iteratively reweighted least squares (IRLS) algorithm</strong>:

考虑 batch methods, 与 linear regression 类似，这里不适宜直接使用 steepest descent.

<code>$ \theta^{t+1} = \theta^{t} - H^{-1} \bigtriangledown_{\theta} J$</code>

<ol>
<li>linear regression</p></li>
<li><p>logistirc regression</p></li>
</ol>

<p><code>$ \theta^{t+1} = (X^T W^{t} X)^{-1} X^T W^{t} z^{t}$</code>

相当于求解 weighted least squares 问题：<code>$ J = \frac{1}{2} (z^{t} - X \theta)^T W^t (z^{t} - X \theta) $</code>

related to weighted least squares in Chapter 06.

<strong>关于 上述IRLS 的一个启发式视角</strong>

<h2>Multiway classification</h2>

<h2>Probit regression</h2>

<h2>The noisy-OR model</h2>

<h2>Other exponential models</h2>

<h1>Summary</h1>

<strong>generative vs. discriminative models</strong>

<blockquote>
  Generative and discriminative models have complementary strengths and weaknesses. The generative approach allows knowledge about class-conditional densities to be exploited. If this knowledge is indeed reflective of the true data-generation process, then the generative approach can be more efficient than a corresponding discriminative model, in the sense that it will tend to require fewer data points. On the other hand, discriminative approaches tend to be more robust than generative approaches, making use of weaker assumptions regarding class-conditional densities. Note also that the discriminative framework presents a straightforward "upgrade path" toward the development of nonlinear classifiers --- we can retain the logistic and softmax functions, but replace the linear combination <code>$\eta = \theta^T x$</code> with a nonlinear function
</blockquote>

<h1>参考资料</h1>

<ul>
<li><a href="">Jordan: An Introduction to PGMs Chapter 7</a></li>
</ul>