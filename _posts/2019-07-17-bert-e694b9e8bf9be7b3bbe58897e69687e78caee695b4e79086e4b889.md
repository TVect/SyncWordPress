---
ID: 501
post_title: BERT 改进系列文献整理(三)
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/501
published: true
post_date: 2019-07-17 22:06:29
---
从 BERT 提出之后, 后续出现了很多篇文章基于 BERT 做了各式各样的改进. 这里会尝试对自己看到了关于BERT改进的文献做一些整理。

<h1>XLNet (Autoregressive LM + Autoencoder LM)</h1>

autoregressive (AR) language modeling 和 autoencoding (AE) 是无监督表示学习的两类方法.

<ul>
<li>autoregressive (AR) language modeling</li>
</ul>

<blockquote>
  AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model.
</blockquote>

代表性的方法有 GPT, ELMo. 自回归语言模型的缺点在于只能利用单向的上下文信息(前向或者后向).

<ul>
<li>autoencoding (AE)</li>
</ul>

<blockquote>
  AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input.
</blockquote>

代表性的方法如 Bert, 其可以利用双向的上下文信息. 但是 Bert 在预训练的时候引入了一个 <code>[MASK]</code> 符号, 这个符号在 fine-tuning 阶段不会出现, 这导致了 pretrain-finetune 的不一致. 另外在 Bert 中隐式的假设了待预测的 tokens 在给定其他 unmasked tokens 的情况下是条件独立的.

XLNet 尝试结合了这两类方法的优点, 同时避免了两类方法各自的缺点.

<h2>Objective: Permutation Language Modeling</h2>

XLNet 试图最大化一个句子在其所有可能的置换对应的分解次序下的 expected log likelihood. 相比之下, 之前提到的自回归语言模型只考虑了前向或者后向的分解次序. 即, 优化的目标函数为:

$$\max_{\theta} \mathbb{E}<em>{z \sim \mathcal{Z}_T} [\sum</em>{t=1}^T \log p_{\theta} (x_{z_t} | x_{z &lt; t})] $$

其中, $T$ 为序列的长度, $\mathcal{Z}<em>T$ 表示长度为 $T$ 的索引序列 $[1, 2,..., T]$ 的所有可能的排列. $z_t$ 和 $z</em>{\lt t}$ 表示在某个排列 $z \in \mathcal{Z}_T$ 中的 第 $t$ 个元素和前 $t-1$ 个元素.

注意, 这里的不同排列可以通过在 Transformer 中使用合适的 attention mask 来实现 (无需对输入句子做重新排列)

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/xlnet-permutation-1024x820.png" width=800 align=center>

<h2>Architecture: Two-Stream Self-Attention for Target-Aware Representations</h2>

在预测当前 token 的时候, 只能看到当前的位置信息(且应该利用当前位置信息, 否则在重排列无法区分不同位置), 不能看到当前 token. 而在预测其他 token 的时候, 需要用到 token 整个信息. 所以这里设计了 Two-Stream Self-Attention.

<ul>
<li>The content representation $h_{\theta}(x_{z≤t})$, or abbreviated as $h_{z_t}$, which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and xzt itself.</p></li>
<li><p>The query representation $g_{\theta}(x_{z \lt t}, z_t)$, or abbreviated as $g_{z_t}$, which only has access to the contextual information $x_{z \lt t}$ and the position $z_t$, but not the content $x_{z_t}$.</p></li>
</ul>

<p>更新公式如下:

$$
\begin{aligned}
g_{z_t}^{(m)} &amp;\leftarrow Attention(Q=g_{z_t}^{(m-1)}, KV=h_{z \lt t}^{(m-1)}; \theta) \quad \text{(query stream)} &#92;
h_{z_t}^{(m)} &amp;\leftarrow Attention(Q=h_{z_t}^{(m-1)}, KV=h_{z \le t}^{(m-1)}; \theta) \quad \text{(content stream)}
\end{aligned}
$$

其中, $m = 1, . . . , M$ 表示不同的 self-attention 层. content stream 的第一层初始化为相应的 word embedding：$h_i^{(0)} = e(x_i)$. query stream 的第一层初始化为一个待训练的向量: $g_i^{(0)} = w$

最后某个位置的 token 的预测概率为:

$$p_{\theta}(X_{z_t} = x | x_{z \lt t}) = \frac{exp(e(x)^T g_{\theta}(x_{z \lt t}, z_t))}{\sum_{x^{\prime}} exp(e(x^{\prime})^T g_{\theta}(x_{z \lt t}, z_t))}$$

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/xlnet-2streams-1024x589.png" width=800 align=center>

<strong>Partial Prediction</strong>

文章中提到, 在初步的实验中收敛的很慢, 为了降低训练的难度, 文中将 $z$ 切分为两段,  non-target subsequence $z_{\le c}$ 和 target subsequence $z_{\gt c}$, 目标函数只考虑 target subsequence conditioned on the non-target subsequence, 即:

$$ \max_{\theta} \mathbb{E}<em>{z \sim \mathcal{Z}_T}[\log p</em>{\theta}(x_{z \gt c} | x_{z\le c})] = \mathbb{E}<em>{z \sim \mathcal{Z}_T} [\sum</em>{t=c+1}^T \log p_{\theta} (x_{z_t} | x_{z &lt; t})]$$

<h2>Incorporating Ideas from Transformer-XL</h2>

借鉴使用了 Transformer-XL 中两个重要的技术:

<ul>
<li>the relative positional encoding scheme</p></li>
<li><p>the segment recurrence mechanism</p></li>
</ul>

<h2>Modeling Multiple Segments</h2>

<blockquote>
  <p>XLNet-Large does not use the objective of next sentence prediction as it does not show consistent improvement in our ablation study.
</blockquote>

<strong>Relative Segment Encodings</strong>
...

<hr />

<h1>补充资料: Transformer-XL</h1>

Transformer 结构有学习长距离依赖的潜力, 但是在实际工作中 Transformer 会作用在固定长度的文本片段中, 导致 Transformer 无法学到超过固定文本长度的依赖性.

这篇文章为处理上述问题, 尝试了如下改进：

<ul>
<li>提出片<strong>段级递归机制(segment-level recurrence mechanism)</strong>，引入一个记忆(memory)模块（类似于cache或cell），循环用来建模片段之间的联系。
使得长距离依赖的建模成为可能；
使得片段之间产生交互，解决上下文碎片化问题。</p></li>
<li><p>提出<strong>相对位置编码机制(relative position embedding scheme)</strong>，代替绝对位置编码。
在memory的循环计算过程中，避免时序混淆，位置编码可重用。</p></li>
</ul>

<h2>Vanilla Transformer Language Models</h2>

<p><strong>Training</strong>

训练的时候, 把语料库分成一小段一小段的能够处理的 Segments, 在每个 Segment 内部训练 Transformer.

缺点在于: (1)无法考虑长度大于 segment length 的依赖关系, (2)普通的将语料库切分为固定长度文本的做法中可能会带来 context fragmentation 的问题.

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/vanilla-transformer-train.gif" alt="" />

<strong>Evaluation</strong>

做评估的时候, 输入也是一小段 Segment, 然后只对这个 Segment 的最后一个位置做预测, 做完预测之后, 向右平移一个单位, 继续输入一小段 Segment 做预测.

这样子做预测的时候, 可以利用到前面的 segment length 个 token 的信息来做预测, 缺点是预测过程很昂贵.

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/vanilla-transformer-evaluation.gif" alt="" />

<h2>Segment-Level Recurrence with State Reuse</h2>

训练过程当中, 前一个 Segment 计算得到的隐状态序列会被缓存并固定下来, 在处理下一个 Segment 的时候会利用到这个 Segment 保存下来的隐状态.

<strong>训练过程展示如下</strong>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/segment-level-recurrence.gif" alt="" />

<strong>评估过程展示如下</strong>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/transformer-xl-evaluation-300x150.gif" alt="" />

<h2>Relative Positional Encodings</h2>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/relative-position-1024x166.png" width=600 align=center>

最后 Transformer-XL 的总体计算流程如下:

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/transformer-xl.png" width=400 align=center>

<h1>参考资料</h1>

<ul>
<li><p><a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p></li>
<li><p><a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p></li>
</ul>