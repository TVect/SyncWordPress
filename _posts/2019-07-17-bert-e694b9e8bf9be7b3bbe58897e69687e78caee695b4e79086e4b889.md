---
ID: 501
post_title: BERT 改进系列文献整理(三)
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/501
published: true
post_date: 2019-07-17 22:06:29
---
从 BERT 提出之后, 后续出现了很多篇文章基于 BERT 做了各式各样的改进. 这里会尝试对自己看到了关于BERT改进的文献做一些整理。

<h1>XLNet (Autoregressive LM + Autoencoder LM)</h1>

autoregressive (AR) language modeling 和 autoencoding (AE) 是无监督表示学习的两类方法.

<ul>
<li>autoregressive (AR) language modeling</li>
</ul>

<blockquote>
  AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model.
</blockquote>

代表性的方法有 GPT, ELMo. 自回归语言模型的缺点在于只能利用单向的上下文信息(前向或者后向).

<ul>
<li>autoencoding (AE)</li>
</ul>

<blockquote>
  AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input.
</blockquote>

代表性的方法如 Bert, 其可以利用双向的上下文信息. 但是 Bert 在预训练的时候引入了一个 <code>[MASK]</code> 符号, 这个符号在 fine-tuning 阶段不会出现, 这导致了 pretrain-finetune 的不一致. 另外在 Bert 中隐式的假设了待预测的 tokens 在给定其他 unmasked tokens 的情况下是条件独立的.

XLNet 尝试结合了这两类方法的优点, 同时避免了两类方法各自的缺点.

<h2>Objective: Permutation Language Modeling</h2>

与之前自回归语言模型中

XLNet 试图最大化一个句子在其所有可能的置换对应的分解次序下的 expected log likelihood. 相比之下, 之前提到的自回归语言模型只考虑了前向或者后向的分解次序. 即, 优化的目标函数为:

$$\max_{\theta} \mathbb{E}<em>{z \sim \mathcal{Z}_T} [\sum</em>{t=1}^T \log p_{\theta} (x_{z_t} | x_{z &lt; t})] $$

其中, $T$ 为序列的长度, $\mathcal{Z}<em>T$ 表示长度为 $T$ 的索引序列 $[1, 2,..., T]$ 的所有可能的排列. $z_t$ 和 $z</em>{&lt;t}$ 表示在某个排列 $z \in \mathcal{Z}_T$ 中的 第 $t$ 个元素和前 $t-1$ 个元素.

注意, 这里的不同排列可以通过在 Transformer 中使用合适的 attention mask 来实现 (无需对输入句子做重新排列)

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/xlnet-permutation-1024x820.png" width=800 align=center>

<h2>Architecture: Two-Stream Self-Attention for Target-Aware Representations</h2>

在预测当前 token 的时候, 只能看到当前的位置信息(且应该利用当前位置信息, 否则在重排列无法区分不同位置), 不能看到当前 token. 而在预测其他 token 的时候, 需要用到 token 整个信息. 所以这里设计了 Two-Stream Self-Attention.

<ul>
<li>The content representation $h_{\theta}(x_{z≤t})$, or abbreviated as $h_{z_t}$, which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and xzt itself.</p></li>
<li><p>The query representation $g_{\theta}(x_{z \lt t}, z_t)$, or abbreviated as $g_{z_t}$, which only has access to the contextual information $x_{z \lt t}$ and the position $z_t$, but not the content $x_{z_t}$.</p></li>
</ul>

<p>$$
\begin{aligned}
g_{z_t}^{(m)} &amp;\leftarrow Attention(Q=g_{z_t}^{(m-1)}, KV=h_{z \lt t}^{(m-1)}; \theta) \quad \text{(query stream)} &#92;
h_{z_t}^{(m)} &amp;\leftarrow Attention(Q=h_{z_t}^{(m-1)}, KV=h_{z \le t}^{(m-1)}; \theta) \quad \text{(content stream)}
\end{aligned}
$$

其中, $m = 1, . . . , M$ 表示不同的 self-attention 层. content stream 的第一层初始化为相应的 word embedding：$h_i^{(0)} = e(x_i)$. query stream 的第一层初始化为一个待训练的向量: $g_i^{(0)} = w$

<h1>参考资料</h1>

<ul>
<li><p><a href="">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p></li>
<li><p><a href="">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p></li>
</ul>