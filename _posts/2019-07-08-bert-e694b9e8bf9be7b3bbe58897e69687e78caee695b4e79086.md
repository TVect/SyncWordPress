---
ID: 470
post_title: BERT 改进系列文献整理(一)
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/470
published: true
post_date: 2019-07-08 22:02:52
---
从 BERT 提出之后, 后续出现了很多篇文章基于 BERT 做了各式各样的改进. 这里会尝试对自己看到了关于BERT改进的文献做一些整理。

这篇博文当中收录了一些在 BERT 中融入 Knowledge 的文章.

<h1>ERNIE baidu</h1>

使用 knowledge masking strategies 来将短语和实体的知识整合到语言表示中, 从而可以得到更好的语义和语法的信息.

具体的 knowledge masking strategies 包括以下两种:

<ul>
<li>phrase-level masking</li>
</ul>

<blockquote>
  In phrase-level mask stage, we also use basic language units as training input, unlike random basic units mask, this time we randomly select a few phrases in the sentence, mask and predict all the basic units in the same phrase.
</blockquote>

<ul>
<li>entity-level masking</li>
</ul>

<blockquote>
  As in the phrase masking stage, we first analyze the named entities in a sentence, and then mask and predict all slots in the entities.
</blockquote>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/ab86e85fd817c46a7f32a4bd6e68870b.png" width="800" align=center>

<h1>ERNIE tsinghua</h1>

之前的 Bert 中没有融入 knowledge information, 而 Knowledge Information 对某些(细粒度的)任务来说是很必要的.

比如: <code>Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004</code>. 如果不知道 <code>Blowin’ in the Wind</code> 是一首歌, <code>Chronicles: Volume One</code> 是一本书, 就很难得出 <code>Bob Dylan</code> 的 songwriter 和 writer 实体类型, 也很难抽取出 composer 和 author 的关系类型.

这篇文章中试图结合文本语料和知识库, 训练一个enhanced language representation model (ERNIE).

<h2>Model Architecture</h2>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/b0c975608796b74d4378500f6926dccf.png" width="800" align=center>

记 token sequence 为 ${w_1, ..., w_n}$,  entity sequence 为 ${e_1, ..., e_m}$.

<ul>
<li>the underlying textual encoder (T-Encoder)
用于对 ${w_1, ..., w_n}$ 编码, 具体操作与 Bert 中 encoder 方式一致. 记为: ${w_1, . . . , w_n} = TEncoder ({w_1, . . . , w_n})$</p></li>
<li><p>the upper knowledgeable encoder (K-Encoder)
${w^o_1, . . . , w^o_n}, {e^o_1, ..., e^o_m} = KEncoder({w_1, . . . , w_n}, {e_1, ..., e_m})$
具体的, K-Encoder 包含了堆叠的 aggregators, 每个 aggregator 又由 encoder layer 和 information fusion layer 构成：</p></li>
</ul>

<p>另外, 文章中提到, entity embedding 由 Wikidata 做 TransE 得到, 且在训练中保持不变. 句子当中的 entity annotations 用工具 <code>TAGME</code> 得到, 这个工具可以用于找到 text 中的 entity mentions, 并将其对应到 Wikipedia.

<strong>encoder layer</strong>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/051afbade2246053060cc71bb6d87835.png" width="400" align=center>

<strong>information fusion layer</strong>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/df2abef693b64500f81791b637739f30.png" width="800" align=center>

<h2>Pre-training for Injecting Knowledge</h2>

这里提出了一个新的预训练任务, 即随机地mask掉一些 token-entity 对齐关系, 要求模型可以预测token对应的entity.

token 对应的 entity 的预测分布概率为: $p(e_j | w_i) = \frac{exp(linear(w^o_i)e_j)}{\sum_{k=1}^{m} exp(linear(w_i^o)e_k)}$.

这个预训练任务具体操作如下:

<ol>
<li>In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity</li>
<li>In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments</li>
<li>In the rest of the time, we keep token entity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding</li>
</ol>

<h2>Fine-tuning for Specific Tasks</h2>

<ol>
<li>对于 Common NLP tasks, fine-tuning 操作和 Bert 一样.</p></li>
<li><p>对于 knowledge-driven tasks:

<ul>
<li>relation classification
引入了 tokens <code>[HD]</code>和<code>[TL]</code>, 用于标识 head entity 和 tail entity.</li>
<li>entity typing
引入了 tokens <code>[ENT]</code>, 用于标识 entity.</li>
</ul></li>
</ol>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/4bcf654fe05ef4e70409d525330c110b.png" width="800" align=center>

<h1>参考资料</h1>

<ul>
<li><p>ERNIE Baidu <a href="https://arxiv.org/abs/1904.09223">ERNIE: Enhanced Representation through Knowledge Integration</a></p></li>
<li><p>ERNIE Tsinghua <a href="https://arxiv.org/abs/1905.07129">ERNIE: Enhanced Language Representation with Informative Entities</a></p></li>
</ul>