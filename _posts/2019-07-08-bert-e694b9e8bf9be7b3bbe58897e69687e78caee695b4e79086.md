---
ID: 470
post_title: BERT 改进系列文献整理
author: chin340823
post_excerpt: ""
layout: post
permalink: https://www.tvect.cn/archives/470
published: true
post_date: 2019-07-08 22:02:52
---
从 BERT 提出之后, 后续出现了很多篇文章基于 BERT 做了各式各样的改进. 这里会尝试对自己看到了关于BERT改进的文献做一些整理。

<h1>在 BERT 中融入 Knowledge</h1>

<h2>ERNIE baidu</h2>

使用 knowledge masking strategies 来将短语和实体的知识整合到语言表示中, 从而可以得到更好的语义和语法的信息.

具体的 knowledge masking strategies 包括以下两种:

<ul>
<li>phrase-level masking</li>
</ul>

<blockquote>
  In phrase-level mask stage, we also use basic language units as training input, unlike random basic units mask, this time we randomly select a few phrases in the sentence, mask and predict all the basic units in the same phrase.
</blockquote>

<ul>
<li>entity-level masking</li>
</ul>

<blockquote>
  As in the phrase masking stage, we first analyze the named entities in a sentence, and then mask and predict all slots in the entities.
</blockquote>

<img src="https://www.tvect.cn/wp-content/uploads/2019/07/ab86e85fd817c46a7f32a4bd6e68870b.png" alt="" />

<h2>ERNIE tsinghua</h2>

<h1>MT-DNN (multi-task + pre_train)</h1>

<h1>XLNet (Autoregressive LM + Autoencoder LM)</h1>

<h1>参考资料</h1>

<ul>
<li><a href="https://arxiv.org/abs/1904.09223">ERNIE: Enhanced Representation through Knowledge Integration</a></li>
</ul>