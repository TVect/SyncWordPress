---
ID: 440
post_title: sgd及其变形整理
author: Chin
post_excerpt: ""
layout: post
permalink: 'http://blog.tvect.cc/2018/07/25/sgd%e5%8f%8a%e5%85%b6%e5%8f%98%e5%bd%a2%e6%95%b4%e7%90%86/'
published: true
post_date: 2018-07-25 19:52:32
---
[toc]

这里主要是 文章 An overview of gradient descent optimization algorithms 的笔记。

实际上，自己对文章中提到的一些想法还不是理解的很清楚，但还是当做读书笔记做一点记录吧。

<h1>概述</h1>

<h1>Gradient Descent 的一些变体</h1>

常见的 Gradient Descent 的变体有以下三种, 在实际使用中，会根据数据量大小在参数更新的准确性和更新速度之间做 trade-off.

<ol>
<li>Batch gradient descent: 每次在整个数据集上计算梯度，进行更新

这种做法速度比较慢，而且在数据无法完全加载到内存中的情况下会比较麻烦。

<blockquote>
  Batch gradient descent is guaranteed to converge to the global
  minimum for convex error surfaces and to a local minimum for non-convex surfaces.
</blockquote></li>
<li>Stochastic gradient descent: 每次取一个样本，计算梯度做更新

因此 SGD 会更快，也比较适合于在线学习。另外，SGD 频繁地做方差很大的参数更新，会导致目标函数震荡比较剧烈。

SGD 的震荡性可能能让目标函数达到潜在的更好的 local minima.

<blockquote>
  On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting ????
</blockquote></li>
<li>Mini-batch gradient descent: 每次取一个 mini-batch 的样本，计算梯度做更新

相比 SGD，减少了参数更新的方差，可以导致更稳定的收敛。另外，借助优化过的矩阵操作，mini-batch gradient descent 会很有效率。</p></li>
</ol>

<h1>Gradient Descent 中一些Challenges</h1>

<ul>
<li><p><strong>学习率的选择</strong>

过多或者过小都会有相应的问题.</p></li>
<li><p><strong>学习率的调整</strong>

如何在训练的过程中调整到合适学习率，比如像模拟退火算法一样.</p></li>
<li><p><strong>为不同的参数赋予不同的学习率</strong>

有时候数据比较稀疏，可能会希望对出现频次少的特征做更大尺度的更新。比如训练 Word2Vec.</p></li>
<li><p><strong>如何处理局部极值和鞍点的问题</strong>

在神经网络中，目标函数一般是非凸的，经常会遇到局部极值和鞍点。</p></li>
</ul>

<h1>Gradient Descent 优化算法</h1>

<p>下面的列举了deep learning中常见的一些旨在处理上述challenges的优化算法。

<strong>大体上或者是使用了冲量，或者是了采用自适应学习率，或者是两者结合。</strong>

<h2>Momentum</h2>

SGD 在遇到沟壑时容易陷入震荡。为此，可以为其引入动量，加速 SGD 在正确方向的下降并抑制震荡。

具体的优化算法如下：
$$
\\begin{aligned}
v_{t} &= \\gamma v_{t-1} + \\eta \\bigtriangledown_{\\theta}J(\\theta) \\\\
\\theta &= \\theta - v_{t}
\\end{aligned}
$$
其中，$$\\gamma$$ 通常取 0.9 左右。

可以看到，在添加冲量之后，参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关。这使得参数中那些梯度方向变化不大的维度可以加速更新，并减少梯度方向变化较大的维度上的更新幅度。由此产生了加速收敛和减小震荡的效果。

对比效果如下：

<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/momentum.png" alt="" />

<h2>NAG (Nesterov accelerated gradient)</h2>

与前面的 Momentum 相比，NAG 赋予了momentum term 一种预见性。先使用 momentum term $$\\gamma v_{t-1}$$ 尝试着移动参数 $$\\theta$$, 得到的 $$\\theta - \\gamma v_{t-1}$$ 是参数的下一个位置的近似。计算 $$\\theta - \\gamma v_{t-1}$$ 位置的梯度，而不是当前 $$\\theta$$ 位置的梯度。

具体的优化算法如下：
$$
\\begin{aligned}
v_{t} &= \\gamma v_{t-1} + \\eta \\bigtriangledown_{\\theta}J(\\theta - \\gamma v_{t-1}) \\\\
\\theta &= \\theta - v_{t}
\\end{aligned}
$$
同样的，$$\\gamma$$ 通常取 0.9 左右。

图示如下：
<img src="http://blog.tvect.cc/wp-content/uploads/2018/07/nag-300x95.png" alt="" />

这样的一种预见性更新机制，可以防止参数更新过快，提升了更新中的响应性，进而大大提升 RNN 在很多任务上的性能。

<h2>Adagrad</h2>

Adagrad 尝试着为不同的参数赋予不同的学习率，它会对 infrequent parameters 做 larger updates，对 frequent parameters 做 smaller updates. 所以，Adagrad 经常会用于sparse data上，比如做 GloVe 训练。

Adagrad 会对参数 $$\\theta_{i}$$, 在不同时间步 $$t$$, 赋予不同的学习率。
具体优化算法如下：

$$
\\begin{aligned}
g_{t, i} &= \\bigtriangledown_{\\theta}J(\\theta_{t, i}) \\\\
\\theta_{t+1, i} &= \\theta_{t, i} - \\frac {\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} g_{t, i}
\\end{aligned}
$$

其中，$$G_{t}$$ 是一个对角阵，对角线元素 $$G_{t, ii}$$ 为到时间步 t 为止，$$\\theta_{i}$$ 的梯度平方和。

将上述更新公式写成矩阵形式为：

$$
\\theta_{t+1} = \\theta_{t} - \\frac {\\eta}{\\sqrt{G_{t} + \\epsilon}} g_{t}
$$

直观上，Adagrad 尝试给二阶导数大的参数(或者是更新次数多的参数)更小的学习率。因为二阶导数大可能会比二阶导数小的变量离极小点更近。<del>这里用一阶导数的平方和来近似表达二阶导数的大小关系</del>

Adagrad 的一个主要优势在于不再需要手动调整学习率了，大部分实现中直接使用默认值$$\\eta = 0.01$$. Adagrad 的一个主要弱点在于分母上的梯度平方和累加会导致最后的学习率特别小。

<h2>Adadelta</h2>

Adadelta 尝试处理 Adagrad 中梯度平方和累加导致最后的学习率特别小的问题。

相比于 Adagrad，Adadelta 限制了梯度平方和累加在一个fixed size 的窗口。因为，需要存储前 w 个梯度，比较麻烦，又进一步使用了二阶动量的指数移动平均来做。

<ul>
<li><strong>第一个版本的 Adadelta 算法如下</strong>：</li>
</ul>

$$
\\begin{aligned}
E[g^2]_{t} &= \\gamma E[g^2]_{t-1} + (1-\\gamma) g_{t}^2 \\\\
\\\\
RMS[g]_{t} &= \\sqrt{E[g^2]_{t}+\\epsilon} \\\\
\\\\
\\Delta \\theta_{t} &= - \\frac {\\eta}{RMS[g]_{t}} \\\\
\\\\
\\theta_{t+1} &= \\theta_{t} + \\Delta \\theta_{t}
\\end{aligned}
$$

其中， Hinton 建议 $$\\gamma$$ 设置为 0.9, $$\\eta$$ 设置为 0.001

这种形式的 Adagrad 和 RMSProp 是一样的。

<ul>
<li><strong>第二个版本的 Adadelta 算法如下</strong>：</li>
</ul>

$$
\\begin{aligned}
E[\\Delta \\theta^{2}]_{t} &= \\gamma E[\\Delta \\theta^{2}]_{t-1} + (1-\\gamma) \\Delta \\theta^{2} \\\\
\\\\
RMS[\\Delta \\theta]_{t} &= \\sqrt{E[\\Delta \\theta^{2}]_{t}+\\epsilon} \\\\
\\\\
\\Delta \\theta_{t} &= - \\frac {RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}} \\\\
\\\\
\\theta_{t+1} &= \\theta_{t} + \\Delta \\theta_{t}
\\end{aligned}
$$

这个版本的算法中已经不需要指定学习率了。具体的设计想法在 <a href="http://www.cnblogs.com/neopenx/p/4768388.html">博客：自适应学习率调整：AdaDelta</a> 中有介绍。

<h2>RMSprop</h2>

和上面的 Adadelta 的第一个版本相同。历史上，RMSprop 和 Adadelta 是同时独立的提出的。

<h2>Adam (Adaptive Moment Estimation)</h2>

Adam 可以认为是 RMSprop 和 Momentum 的结合。既有类似于 RMSprop 中对二阶动量使用指数移动平均，也有类似于 Momentum 中对一阶动量使用的用指数移动平均。

具体算法如下：

$$
\\begin{aligned}
m_{t} &= \\beta_{1} m_{t-1} + (1-\\beta_{1})g_{t} \\\\
\\\\
v_{t} &= \\beta_{1} v_{t-1} + (1-\\beta_{2})g_{t}^{2} \\\\
\\\\
\\hat{m_{t}} &= \\frac{m_{t}}{1-\\beta_{1}^{t}} \\\\
\\\\
\\hat{v_{t}} &= \\frac{v_{t}}{1-\\beta_{2}^{t}} \\\\
\\\\
\\theta_{t+1} &= \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v_{t}}} + \\epsilon} \\hat{m_{t}}
\\end{aligned}
$$

其中， 作者建议 $$\\beta_{1}$$ 的默认值为 0.9, $$\\beta_{2}$$ 的默认值为 0.999, $$\\epsilon$$ 的默认值为 1e−8. 上面的 $$\\hat{m_{t}}, \\hat{v_{t}} $$ 为对一阶和二阶动量做的偏置校正.

<h2>AdaMax</h2>

Adam 在 $$v_{t}$$ 计算公式中使用了 l2-norm. 更广义的情况可以使用 lp-norm. 在 AdaMax 中使用了 无穷范数.

具体算法如下：

$$
\\begin{aligned}
v_{t} &= \\beta_{2}^{\\infty} v_{t-1} + (1-\\beta_{2}^{\\infty}) |g_{t}|^{\\infty} = \\max (\\beta_{2}v_{t-1}, |g_{t}|) \\\\
\\\\
\\theta_{t+1} &= \\theta_{t} - \\frac {\\eta}{v_{t}} \\hat{m_{t}}
\\end{aligned}
$$

这里的 $$v_{t}$$ 并没有再像 Adam 中一样做偏执修正。同样的建议 $$\\beta_{1}$$ 的默认值为 0.9, $$\\beta_{2}$$ 的默认值为 0.999, $$\\epsilon$$ 的默认值为 1e−8。

<h2>Nadam</h2>

<h1>Parallelizing and distributing SGD</h1>

<h1>Gradient Descent 中的一些小技巧</h1>

<h1>参考资料</h1>

<ul>
<li><p><a href="http://www.deeplearningbook.org">Deep Learning - by Ian Goodfellow</a>. Chapter 8.3</p></li>
<li><p><a href="https://arxiv.org/pdf/1609.04747.pdf">An overview of gradient descent optimization algorithms</a></p></li>
<li><p><a href="http://www.cnblogs.com/neopenx/p/4768388.html">博客：自适应学习率调整：AdaDelta</a></p></li>
<li><p><a href="https://zhuanlan.zhihu.com/p/32626442">知乎专栏：从 SGD 到 Adam</a></p></li>
<li><p><a href="https://blog.csdn.net/u012759136/article/details/52302426">深度学习最全优化方法总结比较</a></p></li>
</ul>